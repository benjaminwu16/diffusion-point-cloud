{"cells":[{"cell_type":"code","source":["# Load the Drive helper and mount\n","from google.colab import drive\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pi3k-qevLhSR","executionInfo":{"status":"ok","timestamp":1713279644954,"user_tz":240,"elapsed":34586,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"1b9dc251-91ff-44c1-9f18-a293a692981c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# !git clone https://github.com/luost26/diffusion-point-cloud.git\n","# %cd diffusion-point-cloud/"],"metadata":{"id":"Vf02AVr8LpI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/diffusion-point-cloud"],"metadata":{"id":"v0jx5_Gg_-bf","executionInfo":{"status":"ok","timestamp":1713279644954,"user_tz":240,"elapsed":2,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5719ba90-c8e1-40a2-ad5a-2da4e52585d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/diffusion-point-cloud\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwxPQ40XFHkG","executionInfo":{"status":"ok","timestamp":1713279726369,"user_tz":240,"elapsed":81416,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"6f2a846e-f804-475a-8193-155d9ee40da3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"]}],"source":["\"\"\"\n","PyTorch\tâ‰¥ 1.6.0\n","h5py\tnot specified (we used 4.61.1)\n","tqdm\tnot specified\n","tensorboard\tnot specified (we used 2.5.0)\n","numpy\tnot specified (we used 1.20.2)\n","scipy\tnot specified (we used 1.6.2)\n","scikit-learn\n","\"\"\"\n","\n","!pip3 install torch torchvision\n","!pip3 install h5py tqdm tensorboard numpy scipy scikit-learn plotly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fm3rsf9XGJY_"},"outputs":[],"source":["# !python3 test_gen.py --ckpt ./pretrained/GEN_airplane.pt --categories airplane\n","\n","# Test an auto-encoder\n","# !python3 test_ae.py --ckpt ./pretrained/AE_all.pt --categories all\n"]},{"cell_type":"code","source":["# !python3 test_ae.py --ckpt ./pretrained/AE_chair.pt --categories chair"],"metadata":{"id":"FMNy5cjrZMZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generating Out-Of-Distribution Datasets"],"metadata":{"id":"0dmqoTXklOwZ"}},{"cell_type":"markdown","source":["### 1. Imports"],"metadata":{"id":"fdWkRaN3bFPW"}},{"cell_type":"code","source":["import os\n","import time\n","import argparse\n","import plotly.graph_objects as go\n","import torch\n","from tqdm.auto import tqdm\n","\n","from utils.dataset import *\n","from utils.misc import *\n","from utils.data import *\n","from models.autoencoder import *\n","from evaluation import EMD_CD"],"metadata":{"id":"bEvJAbEja5Pv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda'\n","\n","# Checkpoint\n","ckpt = torch.load('./pretrained/AE_chair.pt')\n","seed_all(2020)\n","\n","# Load datasets\n","train_dset = ShapeNetCore(\n","    path='./data/shapenet.hdf5',\n","    cates=['chair'],\n","    split='train',\n","    scale_mode='shape_unit'\n",")\n","train_loader = DataLoader(train_dset, batch_size=256, num_workers=0)"],"metadata":{"id":"cZZdRQUAbOm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Utility Functions"],"metadata":{"id":"eO-uVFVbfQ_E"}},{"cell_type":"code","source":["def plot_point_cloud(np_data, color='blue'):\n","  fig = go.Figure(\n","    data=[\n","        go.Scatter3d(\n","            x=np_data[:,0], y=np_data[:,1], z=np_data[:,2],\n","            mode='markers',\n","            marker=dict(size=1, color=color)\n","        )\n","    ],\n","    layout=dict(\n","        scene=dict(\n","            xaxis=dict(visible=False),\n","            yaxis=dict(visible=False),\n","            zaxis=dict(visible=False)\n","        )\n","    )\n","  )\n","  fig.show()\n"],"metadata":{"id":"D2OHaqLofT5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_ood_points(boundary_points, p=10, variance=1e-3):\n","    \"\"\"\n","    Params:\n","      boundary_points\n","        - (M x 256), where M is the number of boundary points\n","      variance\n","        - variance of Gaussian distribution around boundary point\n","\n","    Returns:\n","      latent out-of-distribution points ((MP) x 256)\n","    \"\"\"\n","    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n","        loc=torch.zeros(256), covariance_matrix=variance*torch.eye(256)\n","    )\n","    num_samples = boundary_points.size(0)*p\n","    noise = dist.sample([num_samples]).to(device)\n","    repeated_boundary_points = boundary_points.repeat_interleave(p, dim=0)\n","    return repeated_boundary_points + noise"],"metadata":{"id":"YIC78ziCNv1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pairwise_cosine_similarity(batch_vectors):\n","  return torch.nn.functional.cosine_similarity(\n","      batch_vectors.unsqueeze(1), batch_vectors.unsqueeze(0), dim=-1\n","  )\n","# Example usage\n","batch_size = 3\n","vector_dim = 3\n","batch_vectors = torch.randn(batch_size, vector_dim)  # Example batch of vectors\n","print(batch_vectors)\n","similarity_matrix = pairwise_cosine_similarity(batch_vectors)\n","print(similarity_matrix)\n","\n","print(torch.nn.functional.cosine_similarity(batch_vectors[0:1, :], batch_vectors[1:2, :], dim=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKylgD2H4RjC","executionInfo":{"status":"ok","timestamp":1712751246612,"user_tz":240,"elapsed":6,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"a873f59f-a8cc-4041-f1eb-8b5abbc63b89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.2372, -0.9604,  1.5415],\n","        [-0.4079,  0.8806,  0.0529],\n","        [ 0.0751,  0.4777, -0.6759]])\n","tensor([[ 1.0000, -0.5940, -0.7708],\n","        [-0.5940,  1.0000,  0.4385],\n","        [-0.7708,  0.4385,  1.0000]])\n","tensor([-0.5940])\n"]}]},{"cell_type":"code","source":["def find_latent_ood_points(points, k=100, m=50):\n","    \"\"\"\n","    Params:\n","      points\n","        - (N x 256), where N is the number of points\n","      k\n","        - the k in k-NN distance\n","      m\n","        - the number of boundary points\n","\n","    Returns:\n","      A tuple with two items\n","         1. latent ood points\n","         2. indices in `points` of the corresponding boundary\n","       point for each ood point\n","    \"\"\"\n","    def get_kth_dists(pts1, pts2):\n","      # what we want: dists = get_dist(pts1, pts2)\n","      #print(pts1.shape)\n","      #print(pts2.shape)\n","      diffs = pts1.unsqueeze(1)-pts2.unsqueeze(0)\n","      print(diffs.shape)\n","      dists = torch.sqrt(torch.sum(diffs ** 2, dim=2))\n","      sorted_dists, _ = torch.sort(dists, dim=1)\n","      return torch.sort(sorted_dists[:, k], dim=0, descending=True, stable=True)\n","\n","    def min_row_variance(tensor, dim=1):\n","      row_means = torch.mean(tensor, dim=dim, keepdim=True)\n","      squared_diffs = (tensor - row_means)**2\n","      row_variances = torch.mean(squared_diffs, dim=dim)\n","      return torch.min(row_variances).item()\n","\n","    # 1. find boundary points\n","    kth_dists, indices = get_kth_dists(points, points)\n","    boundary_points = points[indices[:m]]\n","\n","    # 2. sample ood points from each boundary point according to a\n","    # Gaussian distribution centered around the boundary point.\n","    p = 10\n","    ood_points = sample_ood_points(boundary_points, p=p, variance=1e-6)\n","    ood_kth_dists, ood_indices = get_kth_dists(ood_points, points)\n","    index_map = indices[:m].repeat_interleave(p, dim=0)\n","\n","    # filter out generated ood points whose kth nearest neighbor is not\n","    # as far away as the largest kth nearest neighbor distance in the original\n","    # set of points.\n","    threshold = kth_dists[0].item()\n","    mask = torch.gt(ood_kth_dists, threshold)\n","    filtered_indices = ood_indices[mask.squeeze()]\n","    return ood_points[filtered_indices], index_map[filtered_indices]"],"metadata":{"id":"mMYgETby6e4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Model"],"metadata":{"id":"id4D_9EmcLok"}},{"cell_type":"code","source":["model = AutoEncoder(ckpt['args']).to(device)\n","model.load_state_dict(ckpt['state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3kiwoaTcPvE","executionInfo":{"status":"ok","timestamp":1712758633519,"user_tz":240,"elapsed":4,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"f7678c65-9bc3-45ef-87c4-ef077dc0f546"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["all_oods = []\n","refs_for_oods = []\n","ood_ref_indices = []\n","cates_for_oods = []\n","\n","for i, batch in enumerate(tqdm(train_loader)):\n","    print(f\"Processing batch {i}...\")\n","    ref = batch['pointcloud'].to(device)\n","    shift = batch['shift'].to(device)\n","    scale = batch['scale'].to(device)\n","    cate = batch['cate']\n","    model.eval()\n","    with torch.no_grad():\n","      code = model.encode(ref)\n","      latent_ood_points, ref_indices = find_latent_ood_points(code.detach())\n","      ood_points = model.decode(latent_ood_points, ref.size(1)).detach()\n","\n","      ref = ref[ref_indices] * scale[ref_indices] + shift[ref_indices]\n","      ood_points = ood_points * scale[ref_indices] + shift[ref_indices]\n","      refs_for_oods.append(ref.squeeze().detach())\n","      all_oods.append(ood_points.detach())\n","      ood_ref_indices.append(ref_indices)\n","      filtered_cate = [cate[i] for i in ref_indices]\n","    cates_for_oods.extend(filtered_cate)\n","\n","refs_for_oods = torch.cat(refs_for_oods, dim=0)\n","all_oods = torch.cat(all_oods, dim=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["aecc63c50ea6439da22b01a4a67d6ed6","780ca1d039ff445b9c48f82c6eda36fd","62fcc30b297e4831b31b71bc04188931","64d85ca2ad6847e0a127ce4ff0d4c668","a3854fcfb9814b298a222bbbba41dcc2","627c1d3756214b178a75cf4e724bca19","599a55d788554aeca7dba0e40bb98d40","13c296911d1c443e976b758fb179f430","616e99b84e534659a850c1893e1fdea7","be8082c1872d4832944e0d9452f22fba","a3c1d8b3e138460ca3a990d02832df5c"]},"id":"A-6MdPBwcdJ6","executionInfo":{"status":"error","timestamp":1712758640231,"user_tz":240,"elapsed":6490,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"5bc72217-118a-4f6c-9cbe-6aecdf12edbc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/22 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecc63c50ea6439da22b01a4a67d6ed6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Processing batch 0...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 7.0274e-02, -7.7085e-02, -9.3813e-02,  ...,  2.1851e-03,\n","          -6.2737e-03,  6.2459e-02],\n","         [ 1.1257e-01, -5.4547e-02, -1.7927e-01,  ...,  3.3924e-02,\n","           8.0913e-02,  1.0600e-01],\n","         ...,\n","         [ 2.8915e-02, -1.0883e-01, -8.4751e-02,  ...,  2.8087e-02,\n","          -1.2669e-02,  4.8519e-02],\n","         [ 4.4112e-03, -8.5510e-02, -1.4021e-01,  ...,  2.3483e-02,\n","          -5.3905e-02,  2.9860e-02],\n","         [ 3.6779e-02, -1.0084e-01, -9.6373e-02,  ...,  3.3838e-02,\n","          -1.1829e-01,  4.5088e-02]],\n","\n","        [[-7.0274e-02,  7.7085e-02,  9.3813e-02,  ..., -2.1851e-03,\n","           6.2737e-03, -6.2459e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 4.2296e-02,  2.2538e-02, -8.5454e-02,  ...,  3.1739e-02,\n","           8.7187e-02,  4.3543e-02],\n","         ...,\n","         [-4.1359e-02, -3.1744e-02,  9.0627e-03,  ...,  2.5902e-02,\n","          -6.3951e-03, -1.3939e-02],\n","         [-6.5863e-02, -8.4245e-03, -4.6397e-02,  ...,  2.1298e-02,\n","          -4.7631e-02, -3.2599e-02],\n","         [-3.3495e-02, -2.3757e-02, -2.5600e-03,  ...,  3.1652e-02,\n","          -1.1202e-01, -1.7370e-02]],\n","\n","        [[-1.1257e-01,  5.4547e-02,  1.7927e-01,  ..., -3.3924e-02,\n","          -8.0913e-02, -1.0600e-01],\n","         [-4.2296e-02, -2.2538e-02,  8.5454e-02,  ..., -3.1739e-02,\n","          -8.7187e-02, -4.3543e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         ...,\n","         [-8.3656e-02, -5.4282e-02,  9.4517e-02,  ..., -5.8367e-03,\n","          -9.3582e-02, -5.7482e-02],\n","         [-1.0816e-01, -3.0963e-02,  3.9058e-02,  ..., -1.0441e-02,\n","          -1.3482e-01, -7.6142e-02],\n","         [-7.5791e-02, -4.6295e-02,  8.2894e-02,  ..., -8.6278e-05,\n","          -1.9921e-01, -6.0913e-02]],\n","\n","        ...,\n","\n","        [[-2.8915e-02,  1.0883e-01,  8.4751e-02,  ..., -2.8087e-02,\n","           1.2669e-02, -4.8519e-02],\n","         [ 4.1359e-02,  3.1744e-02, -9.0627e-03,  ..., -2.5902e-02,\n","           6.3951e-03,  1.3939e-02],\n","         [ 8.3656e-02,  5.4282e-02, -9.4517e-02,  ...,  5.8367e-03,\n","           9.3582e-02,  5.7482e-02],\n","         ...,\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [-2.4504e-02,  2.3320e-02, -5.5459e-02,  ..., -4.6038e-03,\n","          -4.1236e-02, -1.8660e-02],\n","         [ 7.8645e-03,  7.9874e-03, -1.1623e-02,  ...,  5.7504e-03,\n","          -1.0562e-01, -3.4308e-03]],\n","\n","        [[-4.4112e-03,  8.5510e-02,  1.4021e-01,  ..., -2.3483e-02,\n","           5.3905e-02, -2.9860e-02],\n","         [ 6.5863e-02,  8.4245e-03,  4.6397e-02,  ..., -2.1298e-02,\n","           4.7631e-02,  3.2599e-02],\n","         [ 1.0816e-01,  3.0963e-02, -3.9058e-02,  ...,  1.0441e-02,\n","           1.3482e-01,  7.6142e-02],\n","         ...,\n","         [ 2.4504e-02, -2.3320e-02,  5.5459e-02,  ...,  4.6038e-03,\n","           4.1236e-02,  1.8660e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 3.2368e-02, -1.5332e-02,  4.3837e-02,  ...,  1.0354e-02,\n","          -6.4388e-02,  1.5229e-02]],\n","\n","        [[-3.6779e-02,  1.0084e-01,  9.6373e-02,  ..., -3.3838e-02,\n","           1.1829e-01, -4.5088e-02],\n","         [ 3.3495e-02,  2.3757e-02,  2.5600e-03,  ..., -3.1652e-02,\n","           1.1202e-01,  1.7370e-02],\n","         [ 7.5791e-02,  4.6295e-02, -8.2894e-02,  ...,  8.6278e-05,\n","           1.9921e-01,  6.0913e-02],\n","         ...,\n","         [-7.8645e-03, -7.9874e-03,  1.1623e-02,  ..., -5.7504e-03,\n","           1.0562e-01,  3.4308e-03],\n","         [-3.2368e-02,  1.5332e-02, -4.3837e-02,  ..., -1.0354e-02,\n","           6.4388e-02, -1.5229e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00]]], device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[-4.6578e-02, -5.4729e-02, -3.6052e-02,  ..., -8.5914e-03,\n","           2.8689e-02,  5.1505e-02],\n","         [ 2.3696e-02, -1.3181e-01, -1.2987e-01,  ..., -6.4063e-03,\n","           2.2415e-02,  1.1396e-01],\n","         [ 6.5993e-02, -1.0928e-01, -2.1532e-01,  ...,  2.5332e-02,\n","           1.0960e-01,  1.5751e-01],\n","         ...,\n","         [-1.7663e-02, -1.6356e-01, -1.2080e-01,  ...,  1.9496e-02,\n","           1.6020e-02,  1.0002e-01],\n","         [-4.2167e-02, -1.4024e-01, -1.7626e-01,  ...,  1.4892e-02,\n","          -2.5216e-02,  8.1365e-02],\n","         [-9.7983e-03, -1.5557e-01, -1.3243e-01,  ...,  2.5246e-02,\n","          -8.9604e-02,  9.6594e-02]],\n","\n","        [[-4.5694e-02, -5.5308e-02, -3.5416e-02,  ..., -1.0591e-02,\n","           3.0201e-02,  5.0120e-02],\n","         [ 2.4580e-02, -1.3239e-01, -1.2923e-01,  ..., -8.4060e-03,\n","           2.3927e-02,  1.1258e-01],\n","         [ 6.6877e-02, -1.0986e-01, -2.1468e-01,  ...,  2.3333e-02,\n","           1.1111e-01,  1.5612e-01],\n","         ...,\n","         [-1.6779e-02, -1.6414e-01, -1.2017e-01,  ...,  1.7496e-02,\n","           1.7532e-02,  9.8639e-02],\n","         [-4.1283e-02, -1.4082e-01, -1.7563e-01,  ...,  1.2892e-02,\n","          -2.3704e-02,  7.9980e-02],\n","         [-8.9145e-03, -1.5615e-01, -1.3179e-01,  ...,  2.3246e-02,\n","          -8.8092e-02,  9.5209e-02]],\n","\n","        [[-4.5007e-02, -5.3526e-02, -3.6569e-02,  ..., -9.6956e-03,\n","           3.0143e-02,  5.0400e-02],\n","         [ 2.5267e-02, -1.3061e-01, -1.3038e-01,  ..., -7.5105e-03,\n","           2.3869e-02,  1.1286e-01],\n","         [ 6.7564e-02, -1.0807e-01, -2.1584e-01,  ...,  2.4228e-02,\n","           1.1106e-01,  1.5640e-01],\n","         ...,\n","         [-1.6092e-02, -1.6236e-01, -1.2132e-01,  ...,  1.8392e-02,\n","           1.7474e-02,  9.8920e-02],\n","         [-4.0596e-02, -1.3904e-01, -1.7678e-01,  ...,  1.3788e-02,\n","          -2.3762e-02,  8.0260e-02],\n","         [-8.2275e-03, -1.5437e-01, -1.3294e-01,  ...,  2.4142e-02,\n","          -8.8150e-02,  9.5489e-02]],\n","\n","        ...,\n","\n","        [[-3.1194e-01,  6.4414e-02,  2.1614e-01,  ..., -2.4204e-02,\n","           9.1817e-02, -1.0166e-02],\n","         [-2.4167e-01, -1.2672e-02,  1.2233e-01,  ..., -2.2018e-02,\n","           8.5543e-02,  5.2293e-02],\n","         [-1.9937e-01,  9.8662e-03,  3.6874e-02,  ...,  9.7204e-03,\n","           1.7273e-01,  9.5836e-02],\n","         ...,\n","         [-2.8302e-01, -4.4416e-02,  1.3139e-01,  ...,  3.8836e-03,\n","           7.9148e-02,  3.8353e-02],\n","         [-3.0753e-01, -2.1096e-02,  7.5931e-02,  ..., -7.2016e-04,\n","           3.7912e-02,  1.9694e-02],\n","         [-2.7516e-01, -3.6429e-02,  1.1977e-01,  ...,  9.6341e-03,\n","          -2.6476e-02,  3.4922e-02]],\n","\n","        [[-3.1034e-01,  6.4667e-02,  2.1408e-01,  ..., -2.3487e-02,\n","           9.1574e-02, -9.1618e-03],\n","         [-2.4007e-01, -1.2419e-02,  1.2027e-01,  ..., -2.1302e-02,\n","           8.5300e-02,  5.3297e-02],\n","         [-1.9777e-01,  1.0119e-02,  3.4817e-02,  ...,  1.0437e-02,\n","           1.7249e-01,  9.6840e-02],\n","         ...,\n","         [-2.8143e-01, -4.4163e-02,  1.2933e-01,  ...,  4.6000e-03,\n","           7.8905e-02,  3.9357e-02],\n","         [-3.0593e-01, -2.0843e-02,  7.3874e-02,  ..., -3.7611e-06,\n","           3.7669e-02,  2.0698e-02],\n","         [-2.7356e-01, -3.6175e-02,  1.1771e-01,  ...,  1.0350e-02,\n","          -2.6719e-02,  3.5927e-02]],\n","\n","        [[-3.1058e-01,  6.3210e-02,  2.1810e-01,  ..., -2.4586e-02,\n","           9.1183e-02, -8.8015e-03],\n","         [-2.4031e-01, -1.3876e-02,  1.2429e-01,  ..., -2.2401e-02,\n","           8.4910e-02,  5.3657e-02],\n","         [-1.9801e-01,  8.6622e-03,  3.8837e-02,  ...,  9.3374e-03,\n","           1.7210e-01,  9.7200e-02],\n","         ...,\n","         [-2.8167e-01, -4.5620e-02,  1.3335e-01,  ...,  3.5007e-03,\n","           7.8515e-02,  3.9718e-02],\n","         [-3.0617e-01, -2.2300e-02,  7.7894e-02,  ..., -1.1031e-03,\n","           3.7279e-02,  2.1058e-02],\n","         [-2.7381e-01, -3.7633e-02,  1.2173e-01,  ...,  9.2511e-03,\n","          -2.7109e-02,  3.6287e-02]]], device='cuda:0')\n","Processing batch 1...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [-7.9674e-03,  2.3314e-02, -6.3815e-03,  ..., -3.8158e-03,\n","           8.0897e-02,  1.7102e-04],\n","         [-9.8345e-03,  3.6828e-02, -2.6418e-02,  ...,  8.0458e-03,\n","          -3.8868e-02, -5.5587e-03],\n","         ...,\n","         [-1.3899e-02, -2.1515e-02,  5.2332e-03,  ..., -3.4908e-02,\n","           1.2785e-02,  3.9784e-02],\n","         [ 7.5085e-03,  1.5080e-02,  9.2153e-02,  ...,  5.2527e-02,\n","           6.6920e-02,  1.8390e-02],\n","         [ 1.1404e-01,  1.1655e-01,  7.6591e-02,  ..., -1.6765e-02,\n","           1.3751e-01,  8.3381e-04]],\n","\n","        [[ 7.9674e-03, -2.3314e-02,  6.3815e-03,  ...,  3.8158e-03,\n","          -8.0897e-02, -1.7102e-04],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [-1.8671e-03,  1.3514e-02, -2.0036e-02,  ...,  1.1862e-02,\n","          -1.1977e-01, -5.7298e-03],\n","         ...,\n","         [-5.9317e-03, -4.4829e-02,  1.1615e-02,  ..., -3.1092e-02,\n","          -6.8112e-02,  3.9613e-02],\n","         [ 1.5476e-02, -8.2340e-03,  9.8534e-02,  ...,  5.6343e-02,\n","          -1.3977e-02,  1.8219e-02],\n","         [ 1.2201e-01,  9.3232e-02,  8.2972e-02,  ..., -1.2949e-02,\n","           5.6612e-02,  6.6278e-04]],\n","\n","        [[ 9.8345e-03, -3.6828e-02,  2.6418e-02,  ..., -8.0458e-03,\n","           3.8868e-02,  5.5587e-03],\n","         [ 1.8671e-03, -1.3514e-02,  2.0036e-02,  ..., -1.1862e-02,\n","           1.1977e-01,  5.7298e-03],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         ...,\n","         [-4.0645e-03, -5.8343e-02,  3.1651e-02,  ..., -4.2954e-02,\n","           5.1654e-02,  4.5343e-02],\n","         [ 1.7343e-02, -2.1748e-02,  1.1857e-01,  ...,  4.4481e-02,\n","           1.0579e-01,  2.3949e-02],\n","         [ 1.2388e-01,  7.9718e-02,  1.0301e-01,  ..., -2.4811e-02,\n","           1.7638e-01,  6.3926e-03]],\n","\n","        ...,\n","\n","        [[ 1.3899e-02,  2.1515e-02, -5.2332e-03,  ...,  3.4908e-02,\n","          -1.2785e-02, -3.9784e-02],\n","         [ 5.9317e-03,  4.4829e-02, -1.1615e-02,  ...,  3.1092e-02,\n","           6.8112e-02, -3.9613e-02],\n","         [ 4.0645e-03,  5.8343e-02, -3.1651e-02,  ...,  4.2954e-02,\n","          -5.1654e-02, -4.5343e-02],\n","         ...,\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 2.1408e-02,  3.6595e-02,  8.6919e-02,  ...,  8.7435e-02,\n","           5.4135e-02, -2.1395e-02],\n","         [ 1.2794e-01,  1.3806e-01,  7.1358e-02,  ...,  1.8143e-02,\n","           1.2472e-01, -3.8951e-02]],\n","\n","        [[-7.5085e-03, -1.5080e-02, -9.2153e-02,  ..., -5.2527e-02,\n","          -6.6920e-02, -1.8390e-02],\n","         [-1.5476e-02,  8.2340e-03, -9.8534e-02,  ..., -5.6343e-02,\n","           1.3977e-02, -1.8219e-02],\n","         [-1.7343e-02,  2.1748e-02, -1.1857e-01,  ..., -4.4481e-02,\n","          -1.0579e-01, -2.3949e-02],\n","         ...,\n","         [-2.1408e-02, -3.6595e-02, -8.6919e-02,  ..., -8.7435e-02,\n","          -5.4135e-02,  2.1395e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00],\n","         [ 1.0653e-01,  1.0147e-01, -1.5562e-02,  ..., -6.9292e-02,\n","           7.0588e-02, -1.7556e-02]],\n","\n","        [[-1.1404e-01, -1.1655e-01, -7.6591e-02,  ...,  1.6765e-02,\n","          -1.3751e-01, -8.3381e-04],\n","         [-1.2201e-01, -9.3232e-02, -8.2972e-02,  ...,  1.2949e-02,\n","          -5.6612e-02, -6.6278e-04],\n","         [-1.2388e-01, -7.9718e-02, -1.0301e-01,  ...,  2.4811e-02,\n","          -1.7638e-01, -6.3926e-03],\n","         ...,\n","         [-1.2794e-01, -1.3806e-01, -7.1358e-02,  ..., -1.8143e-02,\n","          -1.2472e-01,  3.8951e-02],\n","         [-1.0653e-01, -1.0147e-01,  1.5562e-02,  ...,  6.9292e-02,\n","          -7.0588e-02,  1.7556e-02],\n","         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00]]], device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 0.1017, -0.0608, -0.1786,  ..., -0.0285, -0.1707,  0.0034],\n","         [ 0.0938, -0.0375, -0.1849,  ..., -0.0323, -0.0898,  0.0036],\n","         [ 0.0919, -0.0240, -0.2050,  ..., -0.0204, -0.2095, -0.0021],\n","         ...,\n","         [ 0.0878, -0.0824, -0.1733,  ..., -0.0634, -0.1579,  0.0432],\n","         [ 0.1092, -0.0458, -0.0864,  ...,  0.0241, -0.1037,  0.0218],\n","         [ 0.2158,  0.0557, -0.1020,  ..., -0.0452, -0.0331,  0.0043]],\n","\n","        [[ 0.1035, -0.0607, -0.1787,  ..., -0.0251, -0.1709,  0.0041],\n","         [ 0.0955, -0.0374, -0.1851,  ..., -0.0290, -0.0900,  0.0043],\n","         [ 0.0937, -0.0239, -0.2051,  ..., -0.0171, -0.2098, -0.0015],\n","         ...,\n","         [ 0.0896, -0.0822, -0.1735,  ..., -0.0600, -0.1581,  0.0439],\n","         [ 0.1110, -0.0456, -0.0865,  ...,  0.0274, -0.1040,  0.0225],\n","         [ 0.2176,  0.0559, -0.1021,  ..., -0.0419, -0.0334,  0.0049]],\n","\n","        [[ 0.1029, -0.0611, -0.1794,  ..., -0.0278, -0.1688,  0.0046],\n","         [ 0.0950, -0.0378, -0.1857,  ..., -0.0316, -0.0879,  0.0048],\n","         [ 0.0931, -0.0243, -0.2058,  ..., -0.0198, -0.2077, -0.0010],\n","         ...,\n","         [ 0.0890, -0.0826, -0.1741,  ..., -0.0627, -0.1560,  0.0444],\n","         [ 0.1104, -0.0460, -0.0872,  ...,  0.0247, -0.1019,  0.0230],\n","         [ 0.2170,  0.0555, -0.1028,  ..., -0.0446, -0.0313,  0.0054]],\n","\n","        ...,\n","\n","        [[-0.0538, -0.0994, -0.0706,  ..., -0.0236, -0.2066,  0.0388],\n","         [-0.0618, -0.0761, -0.0770,  ..., -0.0274, -0.1257,  0.0390],\n","         [-0.0636, -0.0626, -0.0970,  ..., -0.0156, -0.2455,  0.0332],\n","         ...,\n","         [-0.0677, -0.1209, -0.0654,  ..., -0.0585, -0.1939,  0.0786],\n","         [-0.0463, -0.0843,  0.0215,  ...,  0.0289, -0.1397,  0.0572],\n","         [ 0.0602,  0.0171,  0.0060,  ..., -0.0404, -0.0691,  0.0396]],\n","\n","        [[-0.0527, -0.0992, -0.0702,  ..., -0.0240, -0.2086,  0.0382],\n","         [-0.0607, -0.0759, -0.0766,  ..., -0.0278, -0.1277,  0.0383],\n","         [-0.0626, -0.0624, -0.0966,  ..., -0.0159, -0.2475,  0.0326],\n","         ...,\n","         [-0.0666, -0.1207, -0.0650,  ..., -0.0589, -0.1958,  0.0779],\n","         [-0.0452, -0.0841,  0.0219,  ...,  0.0285, -0.1417,  0.0565],\n","         [ 0.0613,  0.0173,  0.0064,  ..., -0.0408, -0.0711,  0.0390]],\n","\n","        [[-0.0540, -0.0994, -0.0706,  ..., -0.0256, -0.2082,  0.0375],\n","         [-0.0619, -0.0760, -0.0770,  ..., -0.0294, -0.1273,  0.0377],\n","         [-0.0638, -0.0625, -0.0970,  ..., -0.0176, -0.2471,  0.0320],\n","         ...,\n","         [-0.0679, -0.1209, -0.0653,  ..., -0.0605, -0.1954,  0.0773],\n","         [-0.0464, -0.0843,  0.0216,  ...,  0.0269, -0.1413,  0.0559],\n","         [ 0.0601,  0.0172,  0.0060,  ..., -0.0424, -0.0707,  0.0384]]],\n","       device='cuda:0')\n","Processing batch 2...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0538,  0.0872,  0.0169,  ..., -0.0061, -0.1632, -0.0546],\n","         [ 0.0723,  0.0637, -0.0180,  ...,  0.0098, -0.1077, -0.0084],\n","         ...,\n","         [ 0.0156,  0.1135,  0.0206,  ...,  0.0327, -0.0382, -0.0062],\n","         [ 0.0090,  0.0616,  0.0986,  ...,  0.0303, -0.2379, -0.0252],\n","         [-0.0015,  0.0935, -0.0325,  ...,  0.0009, -0.0968,  0.0481]],\n","\n","        [[-0.0538, -0.0872, -0.0169,  ...,  0.0061,  0.1632,  0.0546],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0185, -0.0235, -0.0349,  ...,  0.0159,  0.0556,  0.0462],\n","         ...,\n","         [-0.0382,  0.0263,  0.0037,  ...,  0.0388,  0.1250,  0.0485],\n","         [-0.0448, -0.0256,  0.0817,  ...,  0.0364, -0.0747,  0.0294],\n","         [-0.0553,  0.0063, -0.0494,  ...,  0.0069,  0.0665,  0.1027]],\n","\n","        [[-0.0723, -0.0637,  0.0180,  ..., -0.0098,  0.1077,  0.0084],\n","         [-0.0185,  0.0235,  0.0349,  ..., -0.0159, -0.0556, -0.0462],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [-0.0567,  0.0498,  0.0386,  ...,  0.0229,  0.0695,  0.0023],\n","         [-0.0633, -0.0021,  0.1166,  ...,  0.0205, -0.1303, -0.0168],\n","         [-0.0738,  0.0298, -0.0145,  ..., -0.0090,  0.0109,  0.0565]],\n","\n","        ...,\n","\n","        [[-0.0156, -0.1135, -0.0206,  ..., -0.0327,  0.0382,  0.0062],\n","         [ 0.0382, -0.0263, -0.0037,  ..., -0.0388, -0.1250, -0.0485],\n","         [ 0.0567, -0.0498, -0.0386,  ..., -0.0229, -0.0695, -0.0023],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0066, -0.0519,  0.0780,  ..., -0.0024, -0.1997, -0.0190],\n","         [-0.0171, -0.0200, -0.0531,  ..., -0.0319, -0.0586,  0.0542]],\n","\n","        [[-0.0090, -0.0616, -0.0986,  ..., -0.0303,  0.2379,  0.0252],\n","         [ 0.0448,  0.0256, -0.0817,  ..., -0.0364,  0.0747, -0.0294],\n","         [ 0.0633,  0.0021, -0.1166,  ..., -0.0205,  0.1303,  0.0168],\n","         ...,\n","         [ 0.0066,  0.0519, -0.0780,  ...,  0.0024,  0.1997,  0.0190],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0105,  0.0318, -0.1311,  ..., -0.0294,  0.1411,  0.0733]],\n","\n","        [[ 0.0015, -0.0935,  0.0325,  ..., -0.0009,  0.0968, -0.0481],\n","         [ 0.0553, -0.0063,  0.0494,  ..., -0.0069, -0.0665, -0.1027],\n","         [ 0.0738, -0.0298,  0.0145,  ...,  0.0090, -0.0109, -0.0565],\n","         ...,\n","         [ 0.0171,  0.0200,  0.0531,  ...,  0.0319,  0.0586, -0.0542],\n","         [ 0.0105, -0.0318,  0.1311,  ...,  0.0294, -0.1411, -0.0733],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[-0.0365, -0.0593, -0.2961,  ...,  0.0212,  0.3085,  0.0934],\n","         [ 0.0173,  0.0279, -0.2791,  ...,  0.0151,  0.1453,  0.0387],\n","         [ 0.0358,  0.0044, -0.3140,  ...,  0.0310,  0.2008,  0.0849],\n","         ...,\n","         [-0.0209,  0.0542, -0.2754,  ...,  0.0539,  0.2703,  0.0872],\n","         [-0.0275,  0.0023, -0.1974,  ...,  0.0515,  0.0706,  0.0681],\n","         [-0.0380,  0.0341, -0.3286,  ...,  0.0221,  0.2117,  0.1414]],\n","\n","        [[-0.0372, -0.0582, -0.2963,  ...,  0.0196,  0.3085,  0.0938],\n","         [ 0.0166,  0.0290, -0.2794,  ...,  0.0135,  0.1453,  0.0391],\n","         [ 0.0351,  0.0055, -0.3143,  ...,  0.0294,  0.2008,  0.0853],\n","         ...,\n","         [-0.0216,  0.0553, -0.2757,  ...,  0.0523,  0.2703,  0.0876],\n","         [-0.0282,  0.0034, -0.1977,  ...,  0.0499,  0.0706,  0.0685],\n","         [-0.0387,  0.0353, -0.3288,  ...,  0.0205,  0.2117,  0.1418]],\n","\n","        [[-0.0367, -0.0592, -0.2949,  ...,  0.0183,  0.3087,  0.0938],\n","         [ 0.0171,  0.0280, -0.2780,  ...,  0.0122,  0.1454,  0.0392],\n","         [ 0.0356,  0.0045, -0.3129,  ...,  0.0281,  0.2010,  0.0854],\n","         ...,\n","         [-0.0211,  0.0543, -0.2743,  ...,  0.0510,  0.2705,  0.0876],\n","         [-0.0277,  0.0024, -0.1963,  ...,  0.0486,  0.0707,  0.0686],\n","         [-0.0382,  0.0342, -0.3274,  ...,  0.0192,  0.2119,  0.1419]],\n","\n","        ...,\n","\n","        [[ 0.0133, -0.1267, -0.0262,  ...,  0.0174,  0.1015,  0.0754],\n","         [ 0.0671, -0.0395, -0.0093,  ...,  0.0113, -0.0618,  0.0208],\n","         [ 0.0856, -0.0630, -0.0442,  ...,  0.0272, -0.0062,  0.0670],\n","         ...,\n","         [ 0.0289, -0.0132, -0.0056,  ...,  0.0501,  0.0633,  0.0692],\n","         [ 0.0223, -0.0651,  0.0724,  ...,  0.0477, -0.1364,  0.0502],\n","         [ 0.0118, -0.0333, -0.0587,  ...,  0.0182,  0.0047,  0.1235]],\n","\n","        [[ 0.0147, -0.1247, -0.0259,  ...,  0.0175,  0.0999,  0.0732],\n","         [ 0.0685, -0.0374, -0.0089,  ...,  0.0114, -0.0634,  0.0186],\n","         [ 0.0870, -0.0609, -0.0438,  ...,  0.0273, -0.0078,  0.0648],\n","         ...,\n","         [ 0.0303, -0.0111, -0.0052,  ...,  0.0502,  0.0616,  0.0670],\n","         [ 0.0237, -0.0630,  0.0728,  ...,  0.0478, -0.1381,  0.0480],\n","         [ 0.0132, -0.0312, -0.0584,  ...,  0.0184,  0.0031,  0.1213]],\n","\n","        [[ 0.0161, -0.1270, -0.0249,  ...,  0.0197,  0.0993,  0.0755],\n","         [ 0.0699, -0.0398, -0.0080,  ...,  0.0136, -0.0639,  0.0209],\n","         [ 0.0884, -0.0633, -0.0429,  ...,  0.0295, -0.0083,  0.0671],\n","         ...,\n","         [ 0.0317, -0.0135, -0.0043,  ...,  0.0524,  0.0611,  0.0693],\n","         [ 0.0251, -0.0654,  0.0737,  ...,  0.0500, -0.1386,  0.0503],\n","         [ 0.0146, -0.0335, -0.0574,  ...,  0.0205,  0.0026,  0.1236]]],\n","       device='cuda:0')\n","Processing batch 3...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1289,  0.0105, -0.0553,  ...,  0.0230,  0.1744,  0.1284],\n","         [-0.1312, -0.0565,  0.0597,  ..., -0.0118, -0.0167,  0.0513],\n","         ...,\n","         [-0.0663, -0.0481,  0.0558,  ...,  0.0763,  0.1456, -0.0593],\n","         [-0.1487, -0.0195, -0.0186,  ...,  0.0158,  0.0934,  0.0443],\n","         [-0.1514, -0.0973,  0.0220,  ...,  0.0331,  0.0807,  0.0478]],\n","\n","        [[ 0.1289, -0.0105,  0.0553,  ..., -0.0230, -0.1744, -0.1284],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0023, -0.0670,  0.1150,  ..., -0.0348, -0.1911, -0.0771],\n","         ...,\n","         [ 0.0627, -0.0586,  0.1110,  ...,  0.0533, -0.0288, -0.1876],\n","         [-0.0198, -0.0301,  0.0367,  ..., -0.0072, -0.0810, -0.0841],\n","         [-0.0224, -0.1078,  0.0773,  ...,  0.0101, -0.0936, -0.0806]],\n","\n","        [[ 0.1312,  0.0565, -0.0597,  ...,  0.0118,  0.0167, -0.0513],\n","         [ 0.0023,  0.0670, -0.1150,  ...,  0.0348,  0.1911,  0.0771],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0649,  0.0084, -0.0039,  ...,  0.0880,  0.1623, -0.1106],\n","         [-0.0175,  0.0369, -0.0783,  ...,  0.0276,  0.1101, -0.0070],\n","         [-0.0202, -0.0408, -0.0377,  ...,  0.0449,  0.0974, -0.0036]],\n","\n","        ...,\n","\n","        [[ 0.0663,  0.0481, -0.0558,  ..., -0.0763, -0.1456,  0.0593],\n","         [-0.0627,  0.0586, -0.1110,  ..., -0.0533,  0.0288,  0.1876],\n","         [-0.0649, -0.0084,  0.0039,  ..., -0.0880, -0.1623,  0.1106],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0825,  0.0286, -0.0744,  ..., -0.0604, -0.0522,  0.1035],\n","         [-0.0851, -0.0492, -0.0338,  ..., -0.0432, -0.0649,  0.1070]],\n","\n","        [[ 0.1487,  0.0195,  0.0186,  ..., -0.0158, -0.0934, -0.0443],\n","         [ 0.0198,  0.0301, -0.0367,  ...,  0.0072,  0.0810,  0.0841],\n","         [ 0.0175, -0.0369,  0.0783,  ..., -0.0276, -0.1101,  0.0070],\n","         ...,\n","         [ 0.0825, -0.0286,  0.0744,  ...,  0.0604,  0.0522, -0.1035],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0026, -0.0778,  0.0406,  ...,  0.0173, -0.0126,  0.0035]],\n","\n","        [[ 0.1514,  0.0973, -0.0220,  ..., -0.0331, -0.0807, -0.0478],\n","         [ 0.0224,  0.1078, -0.0773,  ..., -0.0101,  0.0936,  0.0806],\n","         [ 0.0202,  0.0408,  0.0377,  ..., -0.0449, -0.0974,  0.0036],\n","         ...,\n","         [ 0.0851,  0.0492,  0.0338,  ...,  0.0432,  0.0649, -0.1070],\n","         [ 0.0026,  0.0778, -0.0406,  ..., -0.0173,  0.0126, -0.0035],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 5.9147e-02, -7.2604e-02, -7.6123e-02,  ...,  1.0832e-02,\n","          -4.5838e-02,  3.1323e-02],\n","         [-6.9782e-02, -6.2073e-02, -1.3141e-01,  ...,  3.3827e-02,\n","           1.2855e-01,  1.5971e-01],\n","         [-7.2040e-02, -1.2908e-01, -1.6425e-02,  ..., -9.3893e-04,\n","          -6.2523e-02,  8.2650e-02],\n","         ...,\n","         [-7.1268e-03, -1.2071e-01, -2.0365e-02,  ...,  8.7110e-02,\n","           9.9794e-02, -2.7929e-02],\n","         [-8.9587e-02, -9.2152e-02, -9.4747e-02,  ...,  2.6674e-02,\n","           4.7552e-02,  7.5613e-02],\n","         [-9.2217e-02, -1.6991e-01, -5.4150e-02,  ...,  4.3951e-02,\n","           3.4904e-02,  7.9083e-02]],\n","\n","        [[ 6.0425e-02, -7.1622e-02, -7.7577e-02,  ...,  1.1978e-02,\n","          -4.4718e-02,  3.0812e-02],\n","         [-6.8505e-02, -6.1091e-02, -1.3287e-01,  ...,  3.4973e-02,\n","           1.2967e-01,  1.5920e-01],\n","         [-7.0763e-02, -1.2810e-01, -1.7879e-02,  ...,  2.0678e-04,\n","          -6.1403e-02,  8.2138e-02],\n","         ...,\n","         [-5.8495e-03, -1.1973e-01, -2.1820e-02,  ...,  8.8256e-02,\n","           1.0091e-01, -2.8440e-02],\n","         [-8.8310e-02, -9.1170e-02, -9.6202e-02,  ...,  2.7820e-02,\n","           4.8672e-02,  7.5102e-02],\n","         [-9.0940e-02, -1.6892e-01, -5.5604e-02,  ...,  4.5096e-02,\n","           3.6025e-02,  7.8572e-02]],\n","\n","        [[ 5.9882e-02, -7.1333e-02, -7.5886e-02,  ...,  1.2318e-02,\n","          -4.3256e-02,  3.1276e-02],\n","         [-6.9048e-02, -6.0802e-02, -1.3118e-01,  ...,  3.5313e-02,\n","           1.3114e-01,  1.5966e-01],\n","         [-7.1306e-02, -1.2781e-01, -1.6188e-02,  ...,  5.4689e-04,\n","          -5.9941e-02,  8.2602e-02],\n","         ...,\n","         [-6.3926e-03, -1.1944e-01, -2.0128e-02,  ...,  8.8596e-02,\n","           1.0238e-01, -2.7977e-02],\n","         [-8.8853e-02, -9.0880e-02, -9.4510e-02,  ...,  2.8160e-02,\n","           5.0134e-02,  7.5566e-02],\n","         [-9.1483e-02, -1.6863e-01, -5.3913e-02,  ...,  4.5436e-02,\n","           3.7486e-02,  7.9036e-02]],\n","\n","        ...,\n","\n","        [[ 1.9350e-01, -5.1402e-02, -1.7661e-02,  ..., -8.8612e-02,\n","          -9.2542e-02,  1.2987e-01],\n","         [ 6.4571e-02, -4.0871e-02, -7.2951e-02,  ..., -6.5617e-02,\n","           8.1849e-02,  2.5826e-01],\n","         [ 6.2314e-02, -1.0788e-01,  4.2037e-02,  ..., -1.0038e-01,\n","          -1.0923e-01,  1.8120e-01],\n","         ...,\n","         [ 1.2723e-01, -9.9507e-02,  3.8097e-02,  ..., -1.2334e-02,\n","           5.3091e-02,  7.0618e-02],\n","         [ 4.4766e-02, -7.0949e-02, -3.6286e-02,  ..., -7.2770e-02,\n","           8.4825e-04,  1.7416e-01],\n","         [ 4.2137e-02, -1.4870e-01,  4.3117e-03,  ..., -5.5494e-02,\n","          -1.1799e-02,  1.7763e-01]],\n","\n","        [[ 1.9524e-01, -5.1157e-02, -1.6953e-02,  ..., -8.8710e-02,\n","          -9.3056e-02,  1.2836e-01],\n","         [ 6.6313e-02, -4.0626e-02, -7.2242e-02,  ..., -6.5715e-02,\n","           8.1335e-02,  2.5675e-01],\n","         [ 6.4056e-02, -1.0763e-01,  4.2745e-02,  ..., -1.0048e-01,\n","          -1.0974e-01,  1.7969e-01],\n","         ...,\n","         [ 1.2897e-01, -9.9261e-02,  3.8805e-02,  ..., -1.2432e-02,\n","           5.2576e-02,  6.9109e-02],\n","         [ 4.6508e-02, -7.0704e-02, -3.5577e-02,  ..., -7.2868e-02,\n","           3.3399e-04,  1.7265e-01],\n","         [ 4.3878e-02, -1.4846e-01,  5.0203e-03,  ..., -5.5592e-02,\n","          -1.2314e-02,  1.7612e-01]],\n","\n","        [[ 1.9362e-01, -5.1762e-02, -1.8212e-02,  ..., -8.9169e-02,\n","          -9.1435e-02,  1.2885e-01],\n","         [ 6.4688e-02, -4.1231e-02, -7.3502e-02,  ..., -6.6175e-02,\n","           8.2956e-02,  2.5724e-01],\n","         [ 6.2431e-02, -1.0824e-01,  4.1486e-02,  ..., -1.0094e-01,\n","          -1.0812e-01,  1.8018e-01],\n","         ...,\n","         [ 1.2734e-01, -9.9867e-02,  3.7545e-02,  ..., -1.2892e-02,\n","           5.4198e-02,  6.9601e-02],\n","         [ 4.4883e-02, -7.1309e-02, -3.6837e-02,  ..., -7.3327e-02,\n","           1.9553e-03,  1.7314e-01],\n","         [ 4.2254e-02, -1.4906e-01,  3.7606e-03,  ..., -5.6051e-02,\n","          -1.0692e-02,  1.7661e-01]]], device='cuda:0')\n","Processing batch 4...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1301,  0.0282,  0.0058,  ...,  0.0529, -0.0943,  0.0411],\n","         [ 0.0932,  0.0654,  0.0247,  ...,  0.0192,  0.1083,  0.0304],\n","         ...,\n","         [-0.1171, -0.0148,  0.0239,  ...,  0.0446,  0.1124, -0.0007],\n","         [ 0.0040,  0.0308,  0.0646,  ...,  0.0610, -0.0862,  0.0359],\n","         [ 0.0181, -0.0024,  0.0157,  ...,  0.0680,  0.0117,  0.0365]],\n","\n","        [[ 0.1301, -0.0282, -0.0058,  ..., -0.0529,  0.0943, -0.0411],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.2234,  0.0372,  0.0190,  ..., -0.0337,  0.2026, -0.0108],\n","         ...,\n","         [ 0.0130, -0.0430,  0.0181,  ..., -0.0083,  0.2067, -0.0418],\n","         [ 0.1342,  0.0025,  0.0588,  ...,  0.0081,  0.0081, -0.0053],\n","         [ 0.1483, -0.0306,  0.0100,  ...,  0.0151,  0.1060, -0.0047]],\n","\n","        [[-0.0932, -0.0654, -0.0247,  ..., -0.0192, -0.1083, -0.0304],\n","         [-0.2234, -0.0372, -0.0190,  ...,  0.0337, -0.2026,  0.0108],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [-0.2103, -0.0802, -0.0009,  ...,  0.0254,  0.0041, -0.0310],\n","         [-0.0892, -0.0346,  0.0399,  ...,  0.0419, -0.1945,  0.0055],\n","         [-0.0751, -0.0678, -0.0090,  ...,  0.0488, -0.0966,  0.0061]],\n","\n","        ...,\n","\n","        [[ 0.1171,  0.0148, -0.0239,  ..., -0.0446, -0.1124,  0.0007],\n","         [-0.0130,  0.0430, -0.0181,  ...,  0.0083, -0.2067,  0.0418],\n","         [ 0.2103,  0.0802,  0.0009,  ..., -0.0254, -0.0041,  0.0310],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1211,  0.0455,  0.0407,  ...,  0.0165, -0.1986,  0.0365],\n","         [ 0.1352,  0.0124, -0.0081,  ...,  0.0235, -0.1007,  0.0371]],\n","\n","        [[-0.0040, -0.0308, -0.0646,  ..., -0.0610,  0.0862, -0.0359],\n","         [-0.1342, -0.0025, -0.0588,  ..., -0.0081, -0.0081,  0.0053],\n","         [ 0.0892,  0.0346, -0.0399,  ..., -0.0419,  0.1945, -0.0055],\n","         ...,\n","         [-0.1211, -0.0455, -0.0407,  ..., -0.0165,  0.1986, -0.0365],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0141, -0.0332, -0.0489,  ...,  0.0070,  0.0979,  0.0006]],\n","\n","        [[-0.0181,  0.0024, -0.0157,  ..., -0.0680, -0.0117, -0.0365],\n","         [-0.1483,  0.0306, -0.0100,  ..., -0.0151, -0.1060,  0.0047],\n","         [ 0.0751,  0.0678,  0.0090,  ..., -0.0488,  0.0966, -0.0061],\n","         ...,\n","         [-0.1352, -0.0124,  0.0081,  ..., -0.0235,  0.1007, -0.0371],\n","         [-0.0141,  0.0332,  0.0489,  ..., -0.0070, -0.0979, -0.0006],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 0.0716, -0.0564, -0.1241,  ..., -0.0763, -0.0256,  0.0956],\n","         [-0.0586, -0.0282, -0.1184,  ..., -0.0234, -0.1199,  0.1367],\n","         [ 0.1648,  0.0089, -0.0994,  ..., -0.0571,  0.0827,  0.1259],\n","         ...,\n","         [-0.0455, -0.0712, -0.1003,  ..., -0.0318,  0.0868,  0.0949],\n","         [ 0.0756, -0.0257, -0.0595,  ..., -0.0153, -0.1117,  0.1314],\n","         [ 0.0897, -0.0588, -0.1084,  ..., -0.0083, -0.0138,  0.1320]],\n","\n","        [[ 0.0712, -0.0558, -0.1254,  ..., -0.0763, -0.0244,  0.0951],\n","         [-0.0589, -0.0276, -0.1196,  ..., -0.0235, -0.1187,  0.1362],\n","         [ 0.1645,  0.0096, -0.1007,  ..., -0.0572,  0.0839,  0.1254],\n","         ...,\n","         [-0.0458, -0.0706, -0.1015,  ..., -0.0318,  0.0880,  0.0944],\n","         [ 0.0753, -0.0251, -0.0608,  ..., -0.0153, -0.1106,  0.1310],\n","         [ 0.0894, -0.0582, -0.1097,  ..., -0.0083, -0.0127,  0.1316]],\n","\n","        [[ 0.0695, -0.0544, -0.1263,  ..., -0.0750, -0.0254,  0.0956],\n","         [-0.0607, -0.0262, -0.1206,  ..., -0.0221, -0.1197,  0.1368],\n","         [ 0.1627,  0.0110, -0.1016,  ..., -0.0559,  0.0829,  0.1260],\n","         ...,\n","         [-0.0476, -0.0692, -0.1025,  ..., -0.0305,  0.0870,  0.0950],\n","         [ 0.0735, -0.0236, -0.0617,  ..., -0.0140, -0.1116,  0.1315],\n","         [ 0.0876, -0.0568, -0.1106,  ..., -0.0070, -0.0137,  0.1321]],\n","\n","        ...,\n","\n","        [[ 0.0227, -0.0428, -0.0392,  ..., -0.0515, -0.0658, -0.0573],\n","         [-0.1074, -0.0146, -0.0335,  ...,  0.0014, -0.1601, -0.0161],\n","         [ 0.1160,  0.0226, -0.0145,  ..., -0.0323,  0.0425, -0.0269],\n","         ...,\n","         [-0.0944, -0.0576, -0.0154,  ..., -0.0069,  0.0466, -0.0579],\n","         [ 0.0268, -0.0120,  0.0254,  ...,  0.0095, -0.1520, -0.0214],\n","         [ 0.0409, -0.0452, -0.0235,  ...,  0.0165, -0.0541, -0.0208]],\n","\n","        [[ 0.0263, -0.0433, -0.0391,  ..., -0.0543, -0.0676, -0.0592],\n","         [-0.1038, -0.0151, -0.0333,  ..., -0.0014, -0.1619, -0.0181],\n","         [ 0.1196,  0.0221, -0.0144,  ..., -0.0351,  0.0407, -0.0289],\n","         ...,\n","         [-0.0908, -0.0581, -0.0152,  ..., -0.0098,  0.0448, -0.0599],\n","         [ 0.0304, -0.0125,  0.0255,  ...,  0.0067, -0.1538, -0.0234],\n","         [ 0.0445, -0.0457, -0.0234,  ...,  0.0137, -0.0559, -0.0228]],\n","\n","        [[ 0.0257, -0.0435, -0.0387,  ..., -0.0541, -0.0678, -0.0586],\n","         [-0.1044, -0.0153, -0.0329,  ..., -0.0012, -0.1621, -0.0174],\n","         [ 0.1190,  0.0219, -0.0140,  ..., -0.0349,  0.0405, -0.0282],\n","         ...,\n","         [-0.0914, -0.0583, -0.0148,  ..., -0.0096,  0.0446, -0.0592],\n","         [ 0.0298, -0.0127,  0.0259,  ...,  0.0069, -0.1540, -0.0227],\n","         [ 0.0439, -0.0459, -0.0230,  ...,  0.0139, -0.0561, -0.0221]]],\n","       device='cuda:0')\n","Processing batch 5...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0610,  0.0363, -0.0351,  ...,  0.0227,  0.0118,  0.0341],\n","         [ 0.0739,  0.0317, -0.0038,  ...,  0.0358, -0.1045,  0.0109],\n","         ...,\n","         [-0.0144, -0.0082, -0.0658,  ...,  0.0391, -0.1732, -0.0113],\n","         [ 0.0242,  0.0063, -0.0008,  ...,  0.0403,  0.0451,  0.0371],\n","         [ 0.0208, -0.0395,  0.0243,  ...,  0.0509, -0.2804,  0.0786]],\n","\n","        [[ 0.0610, -0.0363,  0.0351,  ..., -0.0227, -0.0118, -0.0341],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1350, -0.0047,  0.0313,  ...,  0.0131, -0.1163, -0.0232],\n","         ...,\n","         [ 0.0466, -0.0445, -0.0308,  ...,  0.0164, -0.1849, -0.0454],\n","         [ 0.0852, -0.0300,  0.0342,  ...,  0.0176,  0.0333,  0.0029],\n","         [ 0.0818, -0.0758,  0.0594,  ...,  0.0281, -0.2922,  0.0444]],\n","\n","        [[-0.0739, -0.0317,  0.0038,  ..., -0.0358,  0.1045, -0.0109],\n","         [-0.1350,  0.0047, -0.0313,  ..., -0.0131,  0.1163,  0.0232],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [-0.0884, -0.0398, -0.0621,  ...,  0.0033, -0.0687, -0.0222],\n","         [-0.0498, -0.0254,  0.0029,  ...,  0.0045,  0.1496,  0.0261],\n","         [-0.0531, -0.0712,  0.0281,  ...,  0.0150, -0.1759,  0.0676]],\n","\n","        ...,\n","\n","        [[ 0.0144,  0.0082,  0.0658,  ..., -0.0391,  0.1732,  0.0113],\n","         [-0.0466,  0.0445,  0.0308,  ..., -0.0164,  0.1849,  0.0454],\n","         [ 0.0884,  0.0398,  0.0621,  ..., -0.0033,  0.0687,  0.0222],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0386,  0.0144,  0.0650,  ...,  0.0012,  0.2182,  0.0483],\n","         [ 0.0353, -0.0314,  0.0901,  ...,  0.0117, -0.1072,  0.0898]],\n","\n","        [[-0.0242, -0.0063,  0.0008,  ..., -0.0403, -0.0451, -0.0371],\n","         [-0.0852,  0.0300, -0.0342,  ..., -0.0176, -0.0333, -0.0029],\n","         [ 0.0498,  0.0254, -0.0029,  ..., -0.0045, -0.1496, -0.0261],\n","         ...,\n","         [-0.0386, -0.0144, -0.0650,  ..., -0.0012, -0.2182, -0.0483],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0034, -0.0458,  0.0252,  ...,  0.0106, -0.3255,  0.0415]],\n","\n","        [[-0.0208,  0.0395, -0.0243,  ..., -0.0509,  0.2804, -0.0786],\n","         [-0.0818,  0.0758, -0.0594,  ..., -0.0281,  0.2922, -0.0444],\n","         [ 0.0531,  0.0712, -0.0281,  ..., -0.0150,  0.1759, -0.0676],\n","         ...,\n","         [-0.0353,  0.0314, -0.0901,  ..., -0.0117,  0.1072, -0.0898],\n","         [ 0.0034,  0.0458, -0.0252,  ..., -0.0106,  0.3255, -0.0415],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[-0.0129, -0.0120,  0.0134,  ...,  0.0128,  0.1710,  0.0117],\n","         [-0.0739,  0.0243, -0.0217,  ...,  0.0355,  0.1828,  0.0458],\n","         [ 0.0611,  0.0196,  0.0096,  ...,  0.0486,  0.0665,  0.0226],\n","         ...,\n","         [-0.0273, -0.0202, -0.0525,  ...,  0.0519, -0.0022,  0.0004],\n","         [ 0.0113, -0.0058,  0.0125,  ...,  0.0531,  0.2160,  0.0488],\n","         [ 0.0079, -0.0516,  0.0377,  ...,  0.0636, -0.1094,  0.0902]],\n","\n","        [[-0.0158, -0.0114,  0.0122,  ...,  0.0148,  0.1693,  0.0128],\n","         [-0.0768,  0.0249, -0.0229,  ...,  0.0376,  0.1811,  0.0469],\n","         [ 0.0581,  0.0202,  0.0084,  ...,  0.0506,  0.0648,  0.0238],\n","         ...,\n","         [-0.0303, -0.0196, -0.0536,  ...,  0.0540, -0.0038,  0.0016],\n","         [ 0.0084, -0.0052,  0.0114,  ...,  0.0551,  0.2144,  0.0499],\n","         [ 0.0050, -0.0510,  0.0365,  ...,  0.0657, -0.1111,  0.0914]],\n","\n","        [[-0.0153, -0.0114,  0.0119,  ...,  0.0110,  0.1683,  0.0109],\n","         [-0.0763,  0.0249, -0.0231,  ...,  0.0338,  0.1800,  0.0450],\n","         [ 0.0587,  0.0202,  0.0081,  ...,  0.0469,  0.0638,  0.0219],\n","         ...,\n","         [-0.0297, -0.0196, -0.0539,  ...,  0.0502, -0.0049, -0.0003],\n","         [ 0.0089, -0.0052,  0.0111,  ...,  0.0514,  0.2133,  0.0480],\n","         [ 0.0055, -0.0510,  0.0362,  ...,  0.0619, -0.1121,  0.0895]],\n","\n","        ...,\n","\n","        [[ 0.0193, -0.0156,  0.1033,  ..., -0.0280,  0.1590, -0.0085],\n","         [-0.0417,  0.0207,  0.0682,  ..., -0.0053,  0.1708,  0.0257],\n","         [ 0.0933,  0.0161,  0.0995,  ...,  0.0078,  0.0545,  0.0025],\n","         ...,\n","         [ 0.0049, -0.0237,  0.0374,  ...,  0.0111, -0.0142, -0.0197],\n","         [ 0.0435, -0.0093,  0.1024,  ...,  0.0123,  0.2041,  0.0286],\n","         [ 0.0402, -0.0551,  0.1276,  ...,  0.0228, -0.1214,  0.0701]],\n","\n","        [[ 0.0193, -0.0124,  0.1063,  ..., -0.0258,  0.1587, -0.0070],\n","         [-0.0418,  0.0239,  0.0713,  ..., -0.0030,  0.1705,  0.0271],\n","         [ 0.0932,  0.0193,  0.1025,  ...,  0.0101,  0.0542,  0.0039],\n","         ...,\n","         [ 0.0048, -0.0205,  0.0405,  ...,  0.0134, -0.0145, -0.0183],\n","         [ 0.0434, -0.0061,  0.1055,  ...,  0.0145,  0.2038,  0.0300],\n","         [ 0.0401, -0.0519,  0.1306,  ...,  0.0251, -0.1217,  0.0715]],\n","\n","        [[ 0.0191, -0.0129,  0.1050,  ..., -0.0260,  0.1595, -0.0085],\n","         [-0.0419,  0.0235,  0.0699,  ..., -0.0033,  0.1712,  0.0256],\n","         [ 0.0931,  0.0188,  0.1012,  ...,  0.0098,  0.0550,  0.0025],\n","         ...,\n","         [ 0.0047, -0.0210,  0.0391,  ...,  0.0131, -0.0137, -0.0197],\n","         [ 0.0433, -0.0066,  0.1041,  ...,  0.0143,  0.2045,  0.0286],\n","         [ 0.0399, -0.0524,  0.1293,  ...,  0.0248, -0.1209,  0.0701]]],\n","       device='cuda:0')\n","Processing batch 6...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1641, -0.0924,  0.0013,  ...,  0.0584, -0.0389,  0.0564],\n","         [-0.0454, -0.0437,  0.0354,  ..., -0.0075,  0.1275,  0.1259],\n","         ...,\n","         [-0.0103, -0.0174, -0.0706,  ...,  0.0425, -0.1239,  0.0670],\n","         [-0.0806,  0.0472,  0.0183,  ...,  0.1167, -0.0475,  0.0092],\n","         [-0.1172, -0.0713,  0.0757,  ..., -0.0313,  0.0243, -0.0660]],\n","\n","        [[-0.1641,  0.0924, -0.0013,  ..., -0.0584,  0.0389, -0.0564],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.2095,  0.0487,  0.0341,  ..., -0.0660,  0.1664,  0.0695],\n","         ...,\n","         [-0.1744,  0.0750, -0.0719,  ..., -0.0159, -0.0850,  0.0106],\n","         [-0.2447,  0.1396,  0.0170,  ...,  0.0583, -0.0086, -0.0472],\n","         [-0.2812,  0.0212,  0.0744,  ..., -0.0897,  0.0631, -0.1224]],\n","\n","        [[ 0.0454,  0.0437, -0.0354,  ...,  0.0075, -0.1275, -0.1259],\n","         [ 0.2095, -0.0487, -0.0341,  ...,  0.0660, -0.1664, -0.0695],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0350,  0.0263, -0.1060,  ...,  0.0500, -0.2514, -0.0589],\n","         [-0.0352,  0.0908, -0.0171,  ...,  0.1242, -0.1750, -0.1166],\n","         [-0.0718, -0.0276,  0.0403,  ..., -0.0238, -0.1032, -0.1919]],\n","\n","        ...,\n","\n","        [[ 0.0103,  0.0174,  0.0706,  ..., -0.0425,  0.1239, -0.0670],\n","         [ 0.1744, -0.0750,  0.0719,  ...,  0.0159,  0.0850, -0.0106],\n","         [-0.0350, -0.0263,  0.1060,  ..., -0.0500,  0.2514,  0.0589],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0702,  0.0645,  0.0889,  ...,  0.0742,  0.0764, -0.0578],\n","         [-0.1068, -0.0539,  0.1463,  ..., -0.0738,  0.1482, -0.1330]],\n","\n","        [[ 0.0806, -0.0472, -0.0183,  ..., -0.1167,  0.0475, -0.0092],\n","         [ 0.2447, -0.1396, -0.0170,  ..., -0.0583,  0.0086,  0.0472],\n","         [ 0.0352, -0.0908,  0.0171,  ..., -0.1242,  0.1750,  0.1166],\n","         ...,\n","         [ 0.0702, -0.0645, -0.0889,  ..., -0.0742, -0.0764,  0.0578],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0366, -0.1184,  0.0574,  ..., -0.1480,  0.0718, -0.0753]],\n","\n","        [[ 0.1172,  0.0713, -0.0757,  ...,  0.0313, -0.0243,  0.0660],\n","         [ 0.2812, -0.0212, -0.0744,  ...,  0.0897, -0.0631,  0.1224],\n","         [ 0.0718,  0.0276, -0.0403,  ...,  0.0238,  0.1032,  0.1919],\n","         ...,\n","         [ 0.1068,  0.0539, -0.1463,  ...,  0.0738, -0.1482,  0.1330],\n","         [ 0.0366,  0.1184, -0.0574,  ...,  0.1480, -0.0718,  0.0753],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 2.9034e-02,  1.5287e-02, -2.5350e-01,  ...,  4.4738e-03,\n","           2.0673e-01,  3.2792e-02],\n","         [ 1.9313e-01, -7.7125e-02, -2.5218e-01,  ...,  6.2899e-02,\n","           1.6788e-01,  8.9193e-02],\n","         [-1.6327e-02, -2.8376e-02, -2.1810e-01,  ..., -3.0711e-03,\n","           3.3424e-01,  1.5865e-01],\n","         ...,\n","         [ 1.8693e-02, -2.0993e-03, -3.2410e-01,  ...,  4.6962e-02,\n","           8.2848e-02,  9.9798e-02],\n","         [-5.1555e-02,  6.2446e-02, -2.3522e-01,  ...,  1.2116e-01,\n","           1.5927e-01,  4.2036e-02],\n","         [-8.8125e-02, -5.5970e-02, -1.7782e-01,  ..., -2.6835e-02,\n","           2.3103e-01, -3.3244e-02]],\n","\n","        [[ 2.5881e-02,  1.7396e-02, -2.5482e-01,  ...,  7.2093e-03,\n","           2.0726e-01,  3.2440e-02],\n","         [ 1.8997e-01, -7.5016e-02, -2.5350e-01,  ...,  6.5635e-02,\n","           1.6841e-01,  8.8842e-02],\n","         [-1.9480e-02, -2.6266e-02, -2.1942e-01,  ..., -3.3556e-04,\n","           3.3477e-01,  1.5830e-01],\n","         ...,\n","         [ 1.5540e-02,  1.0164e-05, -3.2542e-01,  ...,  4.9698e-02,\n","           8.3381e-02,  9.9447e-02],\n","         [-5.4708e-02,  6.4555e-02, -2.3655e-01,  ...,  1.2390e-01,\n","           1.5980e-01,  4.1684e-02],\n","         [-9.1278e-02, -5.3861e-02, -1.7914e-01,  ..., -2.4099e-02,\n","           2.3156e-01, -3.3595e-02]],\n","\n","        [[ 2.7682e-02,  1.6946e-02, -2.5281e-01,  ...,  5.5454e-03,\n","           2.0573e-01,  3.1895e-02],\n","         [ 1.9177e-01, -7.5466e-02, -2.5149e-01,  ...,  6.3971e-02,\n","           1.6688e-01,  8.8297e-02],\n","         [-1.7679e-02, -2.6716e-02, -2.1740e-01,  ..., -1.9995e-03,\n","           3.3324e-01,  1.5775e-01],\n","         ...,\n","         [ 1.7341e-02, -4.3985e-04, -3.2340e-01,  ...,  4.8034e-02,\n","           8.1848e-02,  9.8901e-02],\n","         [-5.2908e-02,  6.4105e-02, -2.3453e-01,  ...,  1.2224e-01,\n","           1.5827e-01,  4.1139e-02],\n","         [-8.9477e-02, -5.4311e-02, -1.7712e-01,  ..., -2.5763e-02,\n","           2.3003e-01, -3.4141e-02]],\n","\n","        ...,\n","\n","        [[ 1.5863e-02, -1.1185e-03, -2.2576e-02,  ...,  5.0504e-02,\n","           3.6344e-02, -4.9076e-02],\n","         [ 1.7995e-01, -9.3530e-02, -2.1253e-02,  ...,  1.0893e-01,\n","          -2.5105e-03,  7.3256e-03],\n","         [-2.9498e-02, -4.4781e-02,  1.2833e-02,  ...,  4.2959e-02,\n","           1.6385e-01,  7.6782e-02],\n","         ...,\n","         [ 5.5219e-03, -1.8504e-02, -9.3168e-02,  ...,  9.2993e-02,\n","          -8.7540e-02,  1.7930e-02],\n","         [-6.4727e-02,  4.6040e-02, -4.2964e-03,  ...,  1.6719e-01,\n","          -1.1116e-02, -3.9832e-02],\n","         [-1.0130e-01, -7.2375e-02,  5.3112e-02,  ...,  1.9196e-02,\n","           6.0637e-02, -1.1511e-01]],\n","\n","        [[ 1.4830e-02, -1.3128e-03, -2.0049e-02,  ...,  4.9118e-02,\n","           3.5893e-02, -4.7678e-02],\n","         [ 1.7892e-01, -9.3724e-02, -1.8726e-02,  ...,  1.0754e-01,\n","          -2.9618e-03,  8.7238e-03],\n","         [-3.0532e-02, -4.4975e-02,  1.5360e-02,  ...,  4.1573e-02,\n","           1.6340e-01,  7.8181e-02],\n","         ...,\n","         [ 4.4883e-03, -1.8699e-02, -9.0641e-02,  ...,  9.1607e-02,\n","          -8.7991e-02,  1.9329e-02],\n","         [-6.5760e-02,  4.5846e-02, -1.7694e-03,  ...,  1.6581e-01,\n","          -1.1567e-02, -3.8434e-02],\n","         [-1.0233e-01, -7.2569e-02,  5.5639e-02,  ...,  1.7810e-02,\n","           6.0186e-02, -1.1371e-01]],\n","\n","        [[ 1.3945e-02,  6.4089e-04, -1.9957e-02,  ...,  5.0788e-02,\n","           3.7182e-02, -4.8820e-02],\n","         [ 1.7804e-01, -9.1771e-02, -1.8634e-02,  ...,  1.0921e-01,\n","          -1.6723e-03,  7.5813e-03],\n","         [-3.1416e-02, -4.3022e-02,  1.5452e-02,  ...,  4.3244e-02,\n","           1.6469e-01,  7.7038e-02],\n","         ...,\n","         [ 3.6041e-03, -1.6745e-02, -9.0549e-02,  ...,  9.3277e-02,\n","          -8.6702e-02,  1.8186e-02],\n","         [-6.6644e-02,  4.7800e-02, -1.6769e-03,  ...,  1.6748e-01,\n","          -1.0278e-02, -3.9577e-02],\n","         [-1.0321e-01, -7.0616e-02,  5.5731e-02,  ...,  1.9480e-02,\n","           6.1475e-02, -1.1486e-01]]], device='cuda:0')\n","Processing batch 7...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0384, -0.0120,  0.0606,  ...,  0.0413, -0.1425, -0.0286],\n","         [ 0.0030, -0.0589, -0.0207,  ...,  0.0240,  0.1403, -0.0136],\n","         ...,\n","         [-0.0039, -0.0503, -0.0219,  ...,  0.0499,  0.0634,  0.0273],\n","         [-0.0540, -0.0108,  0.0242,  ...,  0.0651,  0.0881,  0.0167],\n","         [ 0.0051, -0.0042,  0.0165,  ...,  0.0579, -0.0135, -0.0704]],\n","\n","        [[ 0.0384,  0.0120, -0.0606,  ..., -0.0413,  0.1425,  0.0286],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0414, -0.0469, -0.0813,  ..., -0.0173,  0.2828,  0.0150],\n","         ...,\n","         [ 0.0345, -0.0383, -0.0825,  ...,  0.0086,  0.2058,  0.0558],\n","         [-0.0156,  0.0012, -0.0364,  ...,  0.0239,  0.2305,  0.0453],\n","         [ 0.0435,  0.0078, -0.0441,  ...,  0.0166,  0.1290, -0.0418]],\n","\n","        [[-0.0030,  0.0589,  0.0207,  ..., -0.0240, -0.1403,  0.0136],\n","         [-0.0414,  0.0469,  0.0813,  ...,  0.0173, -0.2828, -0.0150],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [-0.0069,  0.0086, -0.0012,  ...,  0.0259, -0.0770,  0.0408],\n","         [-0.0570,  0.0481,  0.0449,  ...,  0.0411, -0.0523,  0.0303],\n","         [ 0.0020,  0.0546,  0.0372,  ...,  0.0339, -0.1538, -0.0568]],\n","\n","        ...,\n","\n","        [[ 0.0039,  0.0503,  0.0219,  ..., -0.0499, -0.0634, -0.0273],\n","         [-0.0345,  0.0383,  0.0825,  ..., -0.0086, -0.2058, -0.0558],\n","         [ 0.0069, -0.0086,  0.0012,  ..., -0.0259,  0.0770, -0.0408],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0501,  0.0396,  0.0461,  ...,  0.0152,  0.0247, -0.0105],\n","         [ 0.0090,  0.0461,  0.0384,  ...,  0.0080, -0.0768, -0.0976]],\n","\n","        [[ 0.0540,  0.0108, -0.0242,  ..., -0.0651, -0.0881, -0.0167],\n","         [ 0.0156, -0.0012,  0.0364,  ..., -0.0239, -0.2305, -0.0453],\n","         [ 0.0570, -0.0481, -0.0449,  ..., -0.0411,  0.0523, -0.0303],\n","         ...,\n","         [ 0.0501, -0.0396, -0.0461,  ..., -0.0152, -0.0247,  0.0105],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0591,  0.0065, -0.0077,  ..., -0.0073, -0.1015, -0.0871]],\n","\n","        [[-0.0051,  0.0042, -0.0165,  ..., -0.0579,  0.0135,  0.0704],\n","         [-0.0435, -0.0078,  0.0441,  ..., -0.0166, -0.1290,  0.0418],\n","         [-0.0020, -0.0546, -0.0372,  ..., -0.0339,  0.1538,  0.0568],\n","         ...,\n","         [-0.0090, -0.0461, -0.0384,  ..., -0.0080,  0.0768,  0.0976],\n","         [-0.0591, -0.0065,  0.0077,  ...,  0.0073,  0.1015,  0.0871],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 0.1012,  0.1835, -0.0303,  ..., -0.0642,  0.0434,  0.0601],\n","         [ 0.0628,  0.1715,  0.0302,  ..., -0.0229, -0.0990,  0.0315],\n","         [ 0.1042,  0.1246, -0.0511,  ..., -0.0402,  0.1838,  0.0465],\n","         ...,\n","         [ 0.0973,  0.1331, -0.0522,  ..., -0.0143,  0.1068,  0.0874],\n","         [ 0.0472,  0.1727, -0.0061,  ...,  0.0009,  0.1315,  0.0768],\n","         [ 0.1063,  0.1792, -0.0139,  ..., -0.0063,  0.0300, -0.0103]],\n","\n","        [[ 0.1030,  0.1836, -0.0314,  ..., -0.0645,  0.0428,  0.0612],\n","         [ 0.0645,  0.1716,  0.0292,  ..., -0.0233, -0.0997,  0.0327],\n","         [ 0.1060,  0.1247, -0.0521,  ..., -0.0405,  0.1831,  0.0476],\n","         ...,\n","         [ 0.0991,  0.1332, -0.0533,  ..., -0.0146,  0.1061,  0.0885],\n","         [ 0.0490,  0.1728, -0.0072,  ...,  0.0006,  0.1308,  0.0779],\n","         [ 0.1080,  0.1793, -0.0149,  ..., -0.0066,  0.0293, -0.0092]],\n","\n","        [[ 0.1027,  0.1842, -0.0310,  ..., -0.0642,  0.0442,  0.0605],\n","         [ 0.0643,  0.1722,  0.0296,  ..., -0.0229, -0.0983,  0.0320],\n","         [ 0.1057,  0.1253, -0.0517,  ..., -0.0402,  0.1846,  0.0469],\n","         ...,\n","         [ 0.0988,  0.1339, -0.0529,  ..., -0.0143,  0.1076,  0.0878],\n","         [ 0.0487,  0.1735, -0.0068,  ...,  0.0009,  0.1323,  0.0773],\n","         [ 0.1077,  0.1800, -0.0146,  ..., -0.0063,  0.0307, -0.0099]],\n","\n","        ...,\n","\n","        [[-0.0269,  0.0851,  0.0174,  ..., -0.1308,  0.0757,  0.0786],\n","         [-0.0653,  0.0731,  0.0780,  ..., -0.0896, -0.0668,  0.0501],\n","         [-0.0239,  0.0262, -0.0033,  ..., -0.1068,  0.2160,  0.0651],\n","         ...,\n","         [-0.0308,  0.0348, -0.0045,  ..., -0.0809,  0.1391,  0.1059],\n","         [-0.0809,  0.0744,  0.0416,  ..., -0.0657,  0.1638,  0.0954],\n","         [-0.0218,  0.0809,  0.0339,  ..., -0.0730,  0.0622,  0.0083]],\n","\n","        [[-0.0276,  0.0848,  0.0170,  ..., -0.1298,  0.0758,  0.0796],\n","         [-0.0660,  0.0728,  0.0776,  ..., -0.0885, -0.0667,  0.0511],\n","         [-0.0246,  0.0259, -0.0037,  ..., -0.1058,  0.2161,  0.0660],\n","         ...,\n","         [-0.0315,  0.0344, -0.0049,  ..., -0.0799,  0.1391,  0.1069],\n","         [-0.0816,  0.0740,  0.0412,  ..., -0.0646,  0.1638,  0.0964],\n","         [-0.0225,  0.0805,  0.0335,  ..., -0.0719,  0.0623,  0.0092]],\n","\n","        [[-0.0272,  0.0859,  0.0176,  ..., -0.1293,  0.0774,  0.0787],\n","         [-0.0656,  0.0739,  0.0782,  ..., -0.0881, -0.0651,  0.0501],\n","         [-0.0242,  0.0270, -0.0031,  ..., -0.1053,  0.2177,  0.0651],\n","         ...,\n","         [-0.0311,  0.0356, -0.0043,  ..., -0.0794,  0.1408,  0.1059],\n","         [-0.0812,  0.0752,  0.0418,  ..., -0.0642,  0.1655,  0.0954],\n","         [-0.0221,  0.0817,  0.0341,  ..., -0.0714,  0.0639,  0.0083]]],\n","       device='cuda:0')\n","Processing batch 8...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0108,  0.1006,  0.0070,  ..., -0.0519,  0.1791, -0.0513],\n","         [-0.0615,  0.1044,  0.0260,  ..., -0.0163, -0.0836, -0.0572],\n","         ...,\n","         [ 0.0190,  0.0728,  0.0442,  ..., -0.0263, -0.0451,  0.0164],\n","         [ 0.0171,  0.1233,  0.0542,  ...,  0.0286, -0.0081,  0.0483],\n","         [ 0.0827,  0.0979,  0.0058,  ..., -0.0148,  0.0381, -0.0239]],\n","\n","        [[ 0.0108, -0.1006, -0.0070,  ...,  0.0519, -0.1791,  0.0513],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0507,  0.0038,  0.0190,  ...,  0.0356, -0.2626, -0.0059],\n","         ...,\n","         [ 0.0298, -0.0278,  0.0372,  ...,  0.0256, -0.2242,  0.0677],\n","         [ 0.0279,  0.0227,  0.0472,  ...,  0.0805, -0.1871,  0.0996],\n","         [ 0.0935, -0.0027, -0.0012,  ...,  0.0371, -0.1410,  0.0275]],\n","\n","        [[ 0.0615, -0.1044, -0.0260,  ...,  0.0163,  0.0836,  0.0572],\n","         [ 0.0507, -0.0038, -0.0190,  ..., -0.0356,  0.2626,  0.0059],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0805, -0.0316,  0.0182,  ..., -0.0101,  0.0384,  0.0736],\n","         [ 0.0786,  0.0189,  0.0282,  ...,  0.0449,  0.0755,  0.1055],\n","         [ 0.1442, -0.0065, -0.0202,  ...,  0.0015,  0.1216,  0.0333]],\n","\n","        ...,\n","\n","        [[-0.0190, -0.0728, -0.0442,  ...,  0.0263,  0.0451, -0.0164],\n","         [-0.0298,  0.0278, -0.0372,  ..., -0.0256,  0.2242, -0.0677],\n","         [-0.0805,  0.0316, -0.0182,  ...,  0.0101, -0.0384, -0.0736],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0019,  0.0505,  0.0100,  ...,  0.0549,  0.0370,  0.0319],\n","         [ 0.0637,  0.0251, -0.0384,  ...,  0.0116,  0.0832, -0.0403]],\n","\n","        [[-0.0171, -0.1233, -0.0542,  ..., -0.0286,  0.0081, -0.0483],\n","         [-0.0279, -0.0227, -0.0472,  ..., -0.0805,  0.1871, -0.0996],\n","         [-0.0786, -0.0189, -0.0282,  ..., -0.0449, -0.0755, -0.1055],\n","         ...,\n","         [ 0.0019, -0.0505, -0.0100,  ..., -0.0549, -0.0370, -0.0319],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0656, -0.0254, -0.0484,  ..., -0.0433,  0.0461, -0.0722]],\n","\n","        [[-0.0827, -0.0979, -0.0058,  ...,  0.0148, -0.0381,  0.0239],\n","         [-0.0935,  0.0027,  0.0012,  ..., -0.0371,  0.1410, -0.0275],\n","         [-0.1442,  0.0065,  0.0202,  ..., -0.0015, -0.1216, -0.0333],\n","         ...,\n","         [-0.0637, -0.0251,  0.0384,  ..., -0.0116, -0.0832,  0.0403],\n","         [-0.0656,  0.0254,  0.0484,  ...,  0.0433, -0.0461,  0.0722],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 4.8509e-02, -7.4874e-02, -6.3332e-02,  ...,  5.0426e-02,\n","          -5.1307e-02, -7.9037e-02],\n","         [ 3.7744e-02,  2.5735e-02, -5.6303e-02,  ..., -1.4723e-03,\n","           1.2776e-01, -1.3038e-01],\n","         [-1.2957e-02,  2.9521e-02, -3.7335e-02,  ...,  3.4142e-02,\n","          -1.3486e-01, -1.3625e-01],\n","         ...,\n","         [ 6.7511e-02, -2.0597e-03, -1.9117e-02,  ...,  2.4078e-02,\n","          -9.6418e-02, -6.2648e-02],\n","         [ 6.5648e-02,  4.8421e-02, -9.1317e-03,  ...,  7.8997e-02,\n","          -5.9380e-02, -3.0750e-02],\n","         [ 1.3121e-01,  2.3028e-02, -5.7528e-02,  ...,  3.5668e-02,\n","          -1.3240e-02, -1.0291e-01]],\n","\n","        [[ 4.7725e-02, -7.6303e-02, -6.0680e-02,  ...,  5.0518e-02,\n","          -5.0678e-02, -7.8704e-02],\n","         [ 3.6960e-02,  2.4306e-02, -5.3651e-02,  ..., -1.3801e-03,\n","           1.2839e-01, -1.3004e-01],\n","         [-1.3741e-02,  2.8092e-02, -3.4683e-02,  ...,  3.4234e-02,\n","          -1.3423e-01, -1.3591e-01],\n","         ...,\n","         [ 6.6728e-02, -3.4885e-03, -1.6465e-02,  ...,  2.4170e-02,\n","          -9.5789e-02, -6.2315e-02],\n","         [ 6.4864e-02,  4.6992e-02, -6.4800e-03,  ...,  7.9089e-02,\n","          -5.8751e-02, -3.0418e-02],\n","         [ 1.3043e-01,  2.1599e-02, -5.4876e-02,  ...,  3.5760e-02,\n","          -1.2611e-02, -1.0258e-01]],\n","\n","        [[ 4.7576e-02, -7.7046e-02, -6.1492e-02,  ...,  5.1329e-02,\n","          -5.3134e-02, -7.9160e-02],\n","         [ 3.6811e-02,  2.3563e-02, -5.4463e-02,  ..., -5.6888e-04,\n","           1.2593e-01, -1.3050e-01],\n","         [-1.3890e-02,  2.7349e-02, -3.5494e-02,  ...,  3.5045e-02,\n","          -1.3669e-01, -1.3637e-01],\n","         ...,\n","         [ 6.6578e-02, -4.2315e-03, -1.7276e-02,  ...,  2.4981e-02,\n","          -9.8246e-02, -6.2771e-02],\n","         [ 6.4715e-02,  4.6249e-02, -7.2913e-03,  ...,  7.9900e-02,\n","          -6.1207e-02, -3.0874e-02],\n","         [ 1.3028e-01,  2.0856e-02, -5.5687e-02,  ...,  3.6571e-02,\n","          -1.5067e-02, -1.0304e-01]],\n","\n","        ...,\n","\n","        [[-4.0115e-02, -1.0959e-01, -5.6176e-03,  ..., -1.9663e-02,\n","          -1.2554e-01, -4.2746e-02],\n","         [-5.0880e-02, -8.9816e-03,  1.4116e-03,  ..., -7.1561e-02,\n","           5.3529e-02, -9.4085e-02],\n","         [-1.0158e-01, -5.1954e-03,  2.0380e-02,  ..., -3.5947e-02,\n","          -2.0909e-01, -9.9957e-02],\n","         ...,\n","         [-2.1112e-02, -3.6776e-02,  3.8598e-02,  ..., -4.6011e-02,\n","          -1.7065e-01, -2.6357e-02],\n","         [-2.2976e-02,  1.3705e-02,  4.8583e-02,  ...,  8.9080e-03,\n","          -1.3361e-01,  5.5399e-03],\n","         [ 4.2591e-02, -1.1689e-02,  1.8715e-04,  ..., -3.4421e-02,\n","          -8.7473e-02, -6.6624e-02]],\n","\n","        [[-3.9924e-02, -1.0954e-01, -4.9047e-03,  ..., -2.0445e-02,\n","          -1.2480e-01, -4.4513e-02],\n","         [-5.0689e-02, -8.9288e-03,  2.1245e-03,  ..., -7.2342e-02,\n","           5.4264e-02, -9.5852e-02],\n","         [-1.0139e-01, -5.1426e-03,  2.1093e-02,  ..., -3.6728e-02,\n","          -2.0836e-01, -1.0172e-01],\n","         ...,\n","         [-2.0921e-02, -3.6723e-02,  3.9311e-02,  ..., -4.6792e-02,\n","          -1.6992e-01, -2.8124e-02],\n","         [-2.2784e-02,  1.3757e-02,  4.9296e-02,  ...,  8.1269e-03,\n","          -1.3288e-01,  3.7730e-03],\n","         [ 4.2782e-02, -1.1636e-02,  9.0003e-04,  ..., -3.5202e-02,\n","          -8.6737e-02, -6.8391e-02]],\n","\n","        [[-3.9209e-02, -1.1117e-01, -3.9897e-03,  ..., -1.9727e-02,\n","          -1.2480e-01, -4.2330e-02],\n","         [-4.9974e-02, -1.0566e-02,  3.0395e-03,  ..., -7.1625e-02,\n","           5.4267e-02, -9.3669e-02],\n","         [-1.0068e-01, -6.7793e-03,  2.2008e-02,  ..., -3.6011e-02,\n","          -2.0836e-01, -9.9541e-02],\n","         ...,\n","         [-2.0207e-02, -3.8360e-02,  4.0226e-02,  ..., -4.6075e-02,\n","          -1.6991e-01, -2.5941e-02],\n","         [-2.2070e-02,  1.2121e-02,  5.0211e-02,  ...,  8.8442e-03,\n","          -1.3287e-01,  5.9566e-03],\n","         [ 4.3496e-02, -1.3273e-02,  1.8150e-03,  ..., -3.4485e-02,\n","          -8.6734e-02, -6.6207e-02]]], device='cuda:0')\n","Processing batch 9...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1653, -0.0303,  0.0174,  ..., -0.0790,  0.0800,  0.0676],\n","         [-0.1171, -0.0394, -0.0468,  ...,  0.0095,  0.1141,  0.0071],\n","         ...,\n","         [-0.0591,  0.0037, -0.0527,  ...,  0.0411,  0.2212,  0.0450],\n","         [-0.2251,  0.0142,  0.0027,  ...,  0.0113,  0.3092,  0.0298],\n","         [-0.1019,  0.0286, -0.0234,  ...,  0.0197,  0.0703,  0.0259]],\n","\n","        [[ 0.1653,  0.0303, -0.0174,  ...,  0.0790, -0.0800, -0.0676],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0482, -0.0091, -0.0642,  ...,  0.0884,  0.0341, -0.0605],\n","         ...,\n","         [ 0.1062,  0.0340, -0.0701,  ...,  0.1201,  0.1412, -0.0226],\n","         [-0.0598,  0.0445, -0.0147,  ...,  0.0903,  0.2292, -0.0378],\n","         [ 0.0635,  0.0589, -0.0408,  ...,  0.0987, -0.0098, -0.0417]],\n","\n","        [[ 0.1171,  0.0394,  0.0468,  ..., -0.0095, -0.1141, -0.0071],\n","         [-0.0482,  0.0091,  0.0642,  ..., -0.0884, -0.0341,  0.0605],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0580,  0.0431, -0.0059,  ...,  0.0317,  0.1071,  0.0379],\n","         [-0.1080,  0.0536,  0.0495,  ...,  0.0018,  0.1951,  0.0227],\n","         [ 0.0152,  0.0680,  0.0234,  ...,  0.0103, -0.0438,  0.0188]],\n","\n","        ...,\n","\n","        [[ 0.0591, -0.0037,  0.0527,  ..., -0.0411, -0.2212, -0.0450],\n","         [-0.1062, -0.0340,  0.0701,  ..., -0.1201, -0.1412,  0.0226],\n","         [-0.0580, -0.0431,  0.0059,  ..., -0.0317, -0.1071, -0.0379],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1660,  0.0105,  0.0554,  ..., -0.0298,  0.0880, -0.0152],\n","         [-0.0428,  0.0249,  0.0293,  ..., -0.0214, -0.1510, -0.0191]],\n","\n","        [[ 0.2251, -0.0142, -0.0027,  ..., -0.0113, -0.3092, -0.0298],\n","         [ 0.0598, -0.0445,  0.0147,  ..., -0.0903, -0.2292,  0.0378],\n","         [ 0.1080, -0.0536, -0.0495,  ..., -0.0018, -0.1951, -0.0227],\n","         ...,\n","         [ 0.1660, -0.0105, -0.0554,  ...,  0.0298, -0.0880,  0.0152],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1232,  0.0144, -0.0262,  ...,  0.0084, -0.2390, -0.0040]],\n","\n","        [[ 0.1019, -0.0286,  0.0234,  ..., -0.0197, -0.0703, -0.0259],\n","         [-0.0635, -0.0589,  0.0408,  ..., -0.0987,  0.0098,  0.0417],\n","         [-0.0152, -0.0680, -0.0234,  ..., -0.0103,  0.0438, -0.0188],\n","         ...,\n","         [ 0.0428, -0.0249, -0.0293,  ...,  0.0214,  0.1510,  0.0191],\n","         [-0.1232, -0.0144,  0.0262,  ..., -0.0084,  0.2390,  0.0040],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 0.0974,  0.0190, -0.2178,  ...,  0.0093,  0.0319,  0.0476],\n","         [-0.0679, -0.0114, -0.2004,  ..., -0.0697,  0.1120,  0.1152],\n","         [-0.0197, -0.0204, -0.2646,  ...,  0.0188,  0.1460,  0.0547],\n","         ...,\n","         [ 0.0383,  0.0226, -0.2705,  ...,  0.0504,  0.2532,  0.0926],\n","         [-0.1277,  0.0332, -0.2151,  ...,  0.0206,  0.3412,  0.0775],\n","         [-0.0045,  0.0475, -0.2412,  ...,  0.0290,  0.1022,  0.0735]],\n","\n","        [[ 0.0974,  0.0185, -0.2166,  ...,  0.0095,  0.0323,  0.0452],\n","         [-0.0680, -0.0119, -0.1992,  ..., -0.0694,  0.1124,  0.1128],\n","         [-0.0198, -0.0210, -0.2634,  ...,  0.0190,  0.1464,  0.0523],\n","         ...,\n","         [ 0.0382,  0.0221, -0.2693,  ...,  0.0507,  0.2536,  0.0902],\n","         [-0.1277,  0.0327, -0.2139,  ...,  0.0209,  0.3416,  0.0751],\n","         [-0.0045,  0.0470, -0.2401,  ...,  0.0293,  0.1026,  0.0711]],\n","\n","        [[ 0.0965,  0.0187, -0.2172,  ...,  0.0089,  0.0318,  0.0460],\n","         [-0.0688, -0.0117, -0.1998,  ..., -0.0701,  0.1118,  0.1136],\n","         [-0.0206, -0.0207, -0.2639,  ...,  0.0183,  0.1459,  0.0531],\n","         ...,\n","         [ 0.0374,  0.0224, -0.2698,  ...,  0.0500,  0.2530,  0.0910],\n","         [-0.1286,  0.0329, -0.2144,  ...,  0.0202,  0.3410,  0.0758],\n","         [-0.0053,  0.0473, -0.2406,  ...,  0.0286,  0.1020,  0.0719]],\n","\n","        ...,\n","\n","        [[ 0.0300, -0.0560,  0.0390,  ...,  0.0156, -0.0829, -0.0575],\n","         [-0.1354, -0.0863,  0.0564,  ..., -0.0634, -0.0028,  0.0101],\n","         [-0.0872, -0.0954, -0.0078,  ...,  0.0251,  0.0312, -0.0504],\n","         ...,\n","         [-0.0292, -0.0523, -0.0137,  ...,  0.0567,  0.1384, -0.0125],\n","         [-0.1951, -0.0418,  0.0417,  ...,  0.0269,  0.2264, -0.0277],\n","         [-0.0719, -0.0274,  0.0155,  ...,  0.0353, -0.0126, -0.0316]],\n","\n","        [[ 0.0295, -0.0547,  0.0376,  ...,  0.0148, -0.0841, -0.0573],\n","         [-0.1359, -0.0851,  0.0550,  ..., -0.0642, -0.0041,  0.0103],\n","         [-0.0876, -0.0941, -0.0092,  ...,  0.0242,  0.0300, -0.0502],\n","         ...,\n","         [-0.0296, -0.0510, -0.0151,  ...,  0.0559,  0.1371, -0.0123],\n","         [-0.1956, -0.0405,  0.0403,  ...,  0.0261,  0.2251, -0.0275],\n","         [-0.0724, -0.0262,  0.0141,  ...,  0.0345, -0.0139, -0.0314]],\n","\n","        [[ 0.0285, -0.0558,  0.0392,  ...,  0.0145, -0.0848, -0.0569],\n","         [-0.1369, -0.0862,  0.0566,  ..., -0.0644, -0.0048,  0.0107],\n","         [-0.0886, -0.0953, -0.0076,  ...,  0.0240,  0.0293, -0.0498],\n","         ...,\n","         [-0.0306, -0.0522, -0.0135,  ...,  0.0557,  0.1364, -0.0119],\n","         [-0.1966, -0.0416,  0.0420,  ...,  0.0259,  0.2244, -0.0271],\n","         [-0.0734, -0.0273,  0.0158,  ...,  0.0343, -0.0146, -0.0311]]],\n","       device='cuda:0')\n","Processing batch 10...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0190, -0.0141, -0.0315,  ..., -0.0055,  0.1889,  0.0160],\n","         [ 0.0596,  0.0141, -0.0781,  ..., -0.0536,  0.0722,  0.0942],\n","         ...,\n","         [ 0.0094,  0.0212, -0.0300,  ...,  0.0158,  0.0979, -0.0080],\n","         [-0.0600, -0.0212,  0.0107,  ..., -0.0307,  0.0253,  0.0494],\n","         [-0.0752, -0.0450, -0.0036,  ...,  0.0121,  0.1504, -0.0326]],\n","\n","        [[ 0.0190,  0.0141,  0.0315,  ...,  0.0055, -0.1889, -0.0160],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0785,  0.0282, -0.0465,  ..., -0.0481, -0.1167,  0.0782],\n","         ...,\n","         [ 0.0284,  0.0353,  0.0016,  ...,  0.0213, -0.0910, -0.0241],\n","         [-0.0410, -0.0071,  0.0423,  ..., -0.0251, -0.1636,  0.0334],\n","         [-0.0562, -0.0309,  0.0279,  ...,  0.0176, -0.0386, -0.0486]],\n","\n","        [[-0.0596, -0.0141,  0.0781,  ...,  0.0536, -0.0722, -0.0942],\n","         [-0.0785, -0.0282,  0.0465,  ...,  0.0481,  0.1167, -0.0782],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [-0.0501,  0.0071,  0.0481,  ...,  0.0694,  0.0258, -0.1022],\n","         [-0.1196, -0.0353,  0.0888,  ...,  0.0230, -0.0469, -0.0448],\n","         [-0.1347, -0.0591,  0.0744,  ...,  0.0657,  0.0782, -0.1268]],\n","\n","        ...,\n","\n","        [[-0.0094, -0.0212,  0.0300,  ..., -0.0158, -0.0979,  0.0080],\n","         [-0.0284, -0.0353, -0.0016,  ..., -0.0213,  0.0910,  0.0241],\n","         [ 0.0501, -0.0071, -0.0481,  ..., -0.0694, -0.0258,  0.1022],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0694, -0.0424,  0.0407,  ..., -0.0464, -0.0726,  0.0575],\n","         [-0.0846, -0.0662,  0.0263,  ..., -0.0037,  0.0524, -0.0246]],\n","\n","        [[ 0.0600,  0.0212, -0.0107,  ...,  0.0307, -0.0253, -0.0494],\n","         [ 0.0410,  0.0071, -0.0423,  ...,  0.0251,  0.1636, -0.0334],\n","         [ 0.1196,  0.0353, -0.0888,  ..., -0.0230,  0.0469,  0.0448],\n","         ...,\n","         [ 0.0694,  0.0424, -0.0407,  ...,  0.0464,  0.0726, -0.0575],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0152, -0.0238, -0.0144,  ...,  0.0428,  0.1251, -0.0820]],\n","\n","        [[ 0.0752,  0.0450,  0.0036,  ..., -0.0121, -0.1504,  0.0326],\n","         [ 0.0562,  0.0309, -0.0279,  ..., -0.0176,  0.0386,  0.0486],\n","         [ 0.1347,  0.0591, -0.0744,  ..., -0.0657, -0.0782,  0.1268],\n","         ...,\n","         [ 0.0846,  0.0662, -0.0263,  ...,  0.0037, -0.0524,  0.0246],\n","         [ 0.0152,  0.0238,  0.0144,  ..., -0.0428, -0.1251,  0.0820],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 5.9783e-02, -5.8810e-02, -1.1842e-01,  ...,  8.7635e-03,\n","          -2.0603e-01,  5.5932e-02],\n","         [ 4.0813e-02, -7.2900e-02, -1.4997e-01,  ...,  3.2485e-03,\n","          -1.7115e-02,  7.1973e-02],\n","         [ 1.1934e-01, -4.4705e-02, -1.9648e-01,  ..., -4.4861e-02,\n","          -1.3385e-01,  1.5013e-01],\n","         ...,\n","         [ 6.9228e-02, -3.7644e-02, -1.4841e-01,  ...,  2.4537e-02,\n","          -1.0810e-01,  4.7883e-02],\n","         [-2.1477e-04, -8.0001e-02, -1.0770e-01,  ..., -2.1892e-02,\n","          -1.8074e-01,  1.0536e-01],\n","         [-1.5371e-02, -1.0383e-01, -1.2206e-01,  ...,  2.0885e-02,\n","          -5.5673e-02,  2.3330e-02]],\n","\n","        [[ 6.1732e-02, -6.1474e-02, -1.1950e-01,  ...,  9.2668e-03,\n","          -2.0719e-01,  5.7518e-02],\n","         [ 4.2762e-02, -7.5565e-02, -1.5104e-01,  ...,  3.7518e-03,\n","          -1.8279e-02,  7.3559e-02],\n","         [ 1.2129e-01, -4.7369e-02, -1.9756e-01,  ..., -4.4358e-02,\n","          -1.3502e-01,  1.5172e-01],\n","         ...,\n","         [ 7.1177e-02, -4.0309e-02, -1.4949e-01,  ...,  2.5040e-02,\n","          -1.0926e-01,  4.9469e-02],\n","         [ 1.7339e-03, -8.2665e-02, -1.0878e-01,  ..., -2.1388e-02,\n","          -1.8191e-01,  1.0694e-01],\n","         [-1.3422e-02, -1.0650e-01, -1.2314e-01,  ...,  2.1388e-02,\n","          -5.6836e-02,  2.4916e-02]],\n","\n","        [[ 6.0149e-02, -5.8792e-02, -1.1750e-01,  ...,  8.0666e-03,\n","          -2.0565e-01,  5.7266e-02],\n","         [ 4.1179e-02, -7.2883e-02, -1.4904e-01,  ...,  2.5517e-03,\n","          -1.6736e-02,  7.3308e-02],\n","         [ 1.1970e-01, -4.4688e-02, -1.9555e-01,  ..., -4.5558e-02,\n","          -1.3347e-01,  1.5147e-01],\n","         ...,\n","         [ 6.9594e-02, -3.7627e-02, -1.4748e-01,  ...,  2.3840e-02,\n","          -1.0772e-01,  4.9218e-02],\n","         [ 1.5141e-04, -7.9984e-02, -1.0678e-01,  ..., -2.2589e-02,\n","          -1.8036e-01,  1.0669e-01],\n","         [-1.5004e-02, -1.0382e-01, -1.2114e-01,  ...,  2.0188e-02,\n","          -5.5293e-02,  2.4665e-02]],\n","\n","        ...,\n","\n","        [[-7.8604e-03, -1.4058e-01, -1.1730e-02,  ...,  5.1998e-02,\n","          -3.4580e-01, -7.2068e-02],\n","         [-2.6830e-02, -1.5467e-01, -4.3275e-02,  ...,  4.6483e-02,\n","          -1.5689e-01, -5.6026e-02],\n","         [ 5.1695e-02, -1.2648e-01, -8.9786e-02,  ..., -1.6263e-03,\n","          -2.7363e-01,  2.2133e-02],\n","         ...,\n","         [ 1.5847e-03, -1.1942e-01, -4.1716e-02,  ...,  6.7772e-02,\n","          -2.4787e-01, -8.0117e-02],\n","         [-6.7858e-02, -1.6178e-01, -1.0127e-03,  ...,  2.1343e-02,\n","          -3.2051e-01, -2.2644e-02],\n","         [-8.3014e-02, -1.8561e-01, -1.5370e-02,  ...,  6.4120e-02,\n","          -1.9545e-01, -1.0467e-01]],\n","\n","        [[-9.3145e-03, -1.4066e-01, -1.1658e-02,  ...,  4.9540e-02,\n","          -3.4368e-01, -7.2578e-02],\n","         [-2.8284e-02, -1.5475e-01, -4.3203e-02,  ...,  4.4025e-02,\n","          -1.5477e-01, -5.6536e-02],\n","         [ 5.0241e-02, -1.2655e-01, -8.9714e-02,  ..., -4.0851e-03,\n","          -2.7151e-01,  2.1623e-02],\n","         ...,\n","         [ 1.3061e-04, -1.1949e-01, -4.1644e-02,  ...,  6.5313e-02,\n","          -2.4575e-01, -8.0626e-02],\n","         [-6.9312e-02, -1.6185e-01, -9.3994e-04,  ...,  1.8884e-02,\n","          -3.1840e-01, -2.3154e-02],\n","         [-8.4468e-02, -1.8568e-01, -1.5297e-02,  ...,  6.1661e-02,\n","          -1.9333e-01, -1.0518e-01]],\n","\n","        [[-1.0387e-02, -1.3976e-01, -1.2163e-02,  ...,  5.0892e-02,\n","          -3.4630e-01, -7.1962e-02],\n","         [-2.9357e-02, -1.5385e-01, -4.3708e-02,  ...,  4.5378e-02,\n","          -1.5739e-01, -5.5921e-02],\n","         [ 4.9168e-02, -1.2565e-01, -9.0219e-02,  ..., -2.7322e-03,\n","          -2.7412e-01,  2.2239e-02],\n","         ...,\n","         [-9.4204e-04, -1.1859e-01, -4.2149e-02,  ...,  6.6666e-02,\n","          -2.4837e-01, -8.0011e-02],\n","         [-7.0385e-02, -1.6095e-01, -1.4450e-03,  ...,  2.0237e-02,\n","          -3.2101e-01, -2.2538e-02],\n","         [-8.5541e-02, -1.8478e-01, -1.5802e-02,  ...,  6.3014e-02,\n","          -1.9594e-01, -1.0456e-01]]], device='cuda:0')\n","Processing batch 11...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0421,  0.0327,  0.0751,  ...,  0.0589,  0.1220,  0.0030],\n","         [-0.0095,  0.0061,  0.0380,  ...,  0.0133,  0.0694,  0.0100],\n","         ...,\n","         [ 0.0877,  0.0277,  0.1230,  ...,  0.0415,  0.1872, -0.0138],\n","         [-0.0160,  0.0102,  0.0509,  ..., -0.0166, -0.0409, -0.0257],\n","         [ 0.1026,  0.0081,  0.0412,  ...,  0.0800,  0.1162,  0.0412]],\n","\n","        [[ 0.0421, -0.0327, -0.0751,  ..., -0.0589, -0.1220, -0.0030],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0326, -0.0266, -0.0371,  ..., -0.0456, -0.0526,  0.0070],\n","         ...,\n","         [ 0.1297, -0.0050,  0.0479,  ..., -0.0173,  0.0652, -0.0167],\n","         [ 0.0260, -0.0224, -0.0242,  ..., -0.0754, -0.1629, -0.0286],\n","         [ 0.1446, -0.0245, -0.0339,  ...,  0.0211, -0.0058,  0.0383]],\n","\n","        [[ 0.0095, -0.0061, -0.0380,  ..., -0.0133, -0.0694, -0.0100],\n","         [-0.0326,  0.0266,  0.0371,  ...,  0.0456,  0.0526, -0.0070],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.0971,  0.0216,  0.0850,  ...,  0.0282,  0.1178, -0.0238],\n","         [-0.0066,  0.0042,  0.0129,  ..., -0.0299, -0.1103, -0.0357],\n","         [ 0.1120,  0.0021,  0.0031,  ...,  0.0667,  0.0468,  0.0312]],\n","\n","        ...,\n","\n","        [[-0.0877, -0.0277, -0.1230,  ..., -0.0415, -0.1872,  0.0138],\n","         [-0.1297,  0.0050, -0.0479,  ...,  0.0173, -0.0652,  0.0167],\n","         [-0.0971, -0.0216, -0.0850,  ..., -0.0282, -0.1178,  0.0238],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1037, -0.0175, -0.0721,  ..., -0.0581, -0.2281, -0.0119],\n","         [ 0.0149, -0.0196, -0.0819,  ...,  0.0385, -0.0710,  0.0550]],\n","\n","        [[ 0.0160, -0.0102, -0.0509,  ...,  0.0166,  0.0409,  0.0257],\n","         [-0.0260,  0.0224,  0.0242,  ...,  0.0754,  0.1629,  0.0286],\n","         [ 0.0066, -0.0042, -0.0129,  ...,  0.0299,  0.1103,  0.0357],\n","         ...,\n","         [ 0.1037,  0.0175,  0.0721,  ...,  0.0581,  0.2281,  0.0119],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.1186, -0.0021, -0.0097,  ...,  0.0966,  0.1571,  0.0669]],\n","\n","        [[-0.1026, -0.0081, -0.0412,  ..., -0.0800, -0.1162, -0.0412],\n","         [-0.1446,  0.0245,  0.0339,  ..., -0.0211,  0.0058, -0.0383],\n","         [-0.1120, -0.0021, -0.0031,  ..., -0.0667, -0.0468, -0.0312],\n","         ...,\n","         [-0.0149,  0.0196,  0.0819,  ..., -0.0385,  0.0710, -0.0550],\n","         [-0.1186,  0.0021,  0.0097,  ..., -0.0966, -0.1571, -0.0669],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[-0.0570, -0.0843, -0.0141,  ..., -0.0670, -0.1355, -0.0424],\n","         [-0.0990, -0.0517,  0.0609,  ..., -0.0081, -0.0135, -0.0395],\n","         [-0.0664, -0.0783,  0.0239,  ..., -0.0537, -0.0661, -0.0324],\n","         ...,\n","         [ 0.0307, -0.0566,  0.1089,  ..., -0.0255,  0.0517, -0.0562],\n","         [-0.0730, -0.0741,  0.0367,  ..., -0.0836, -0.1764, -0.0681],\n","         [ 0.0456, -0.0762,  0.0270,  ...,  0.0130, -0.0193, -0.0012]],\n","\n","        [[-0.0601, -0.0838, -0.0138,  ..., -0.0669, -0.1347, -0.0432],\n","         [-0.1021, -0.0511,  0.0613,  ..., -0.0081, -0.0127, -0.0403],\n","         [-0.0695, -0.0777,  0.0242,  ..., -0.0536, -0.0653, -0.0332],\n","         ...,\n","         [ 0.0276, -0.0561,  0.1092,  ..., -0.0254,  0.0525, -0.0570],\n","         [-0.0761, -0.0735,  0.0371,  ..., -0.0835, -0.1755, -0.0689],\n","         [ 0.0425, -0.0756,  0.0274,  ...,  0.0131, -0.0185, -0.0020]],\n","\n","        [[-0.0568, -0.0853, -0.0134,  ..., -0.0676, -0.1346, -0.0430],\n","         [-0.0988, -0.0526,  0.0617,  ..., -0.0087, -0.0126, -0.0401],\n","         [-0.0662, -0.0792,  0.0246,  ..., -0.0543, -0.0652, -0.0330],\n","         ...,\n","         [ 0.0309, -0.0576,  0.1096,  ..., -0.0261,  0.0526, -0.0568],\n","         [-0.0728, -0.0751,  0.0375,  ..., -0.0842, -0.1755, -0.0687],\n","         [ 0.0458, -0.0772,  0.0277,  ...,  0.0124, -0.0184, -0.0018]],\n","\n","        ...,\n","\n","        [[ 0.0453,  0.0432,  0.0467,  ..., -0.0238, -0.2397, -0.0212],\n","         [ 0.0032,  0.0758,  0.1218,  ...,  0.0351, -0.1177, -0.0182],\n","         [ 0.0358,  0.0492,  0.0848,  ..., -0.0104, -0.1703, -0.0112],\n","         ...,\n","         [ 0.1330,  0.0709,  0.1698,  ...,  0.0178, -0.0525, -0.0350],\n","         [ 0.0293,  0.0534,  0.0976,  ..., -0.0403, -0.2806, -0.0469],\n","         [ 0.1479,  0.0513,  0.0879,  ...,  0.0563, -0.1235,  0.0200]],\n","\n","        [[ 0.0442,  0.0420,  0.0451,  ..., -0.0236, -0.2398, -0.0171],\n","         [ 0.0021,  0.0747,  0.1202,  ...,  0.0352, -0.1178, -0.0142],\n","         [ 0.0347,  0.0481,  0.0831,  ..., -0.0103, -0.1703, -0.0072],\n","         ...,\n","         [ 0.1319,  0.0697,  0.1681,  ...,  0.0179, -0.0525, -0.0309],\n","         [ 0.0282,  0.0522,  0.0960,  ..., -0.0402, -0.2806, -0.0428],\n","         [ 0.1468,  0.0501,  0.0863,  ...,  0.0564, -0.1236,  0.0241]],\n","\n","        [[ 0.0460,  0.0421,  0.0485,  ..., -0.0232, -0.2413, -0.0183],\n","         [ 0.0039,  0.0748,  0.1236,  ...,  0.0356, -0.1193, -0.0153],\n","         [ 0.0365,  0.0482,  0.0865,  ..., -0.0099, -0.1719, -0.0083],\n","         ...,\n","         [ 0.1336,  0.0698,  0.1715,  ...,  0.0183, -0.0541, -0.0321],\n","         [ 0.0299,  0.0523,  0.0994,  ..., -0.0398, -0.2822, -0.0440],\n","         [ 0.1485,  0.0502,  0.0897,  ...,  0.0568, -0.1251,  0.0229]]],\n","       device='cuda:0')\n","Processing batch 12...\n","torch.Size([256, 256, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1719, -0.0226,  0.0649,  ...,  0.0299,  0.0375,  0.0572],\n","         [-0.2215, -0.0410,  0.1020,  ...,  0.1044,  0.1818,  0.0939],\n","         ...,\n","         [-0.1031, -0.0002, -0.0013,  ...,  0.0111,  0.0346,  0.0562],\n","         [-0.2126, -0.0498,  0.0047,  ..., -0.0146,  0.0202,  0.0968],\n","         [-0.1205, -0.0059,  0.0203,  ...,  0.0212,  0.0237,  0.0158]],\n","\n","        [[ 0.1719,  0.0226, -0.0649,  ..., -0.0299, -0.0375, -0.0572],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.0495, -0.0184,  0.0372,  ...,  0.0745,  0.1443,  0.0367],\n","         ...,\n","         [ 0.0689,  0.0223, -0.0662,  ..., -0.0188, -0.0029, -0.0011],\n","         [-0.0407, -0.0272, -0.0602,  ..., -0.0445, -0.0173,  0.0396],\n","         [ 0.0514,  0.0167, -0.0446,  ..., -0.0087, -0.0138, -0.0414]],\n","\n","        [[ 0.2215,  0.0410, -0.1020,  ..., -0.1044, -0.1818, -0.0939],\n","         [ 0.0495,  0.0184, -0.0372,  ..., -0.0745, -0.1443, -0.0367],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         ...,\n","         [ 0.1184,  0.0408, -0.1033,  ..., -0.0932, -0.1472, -0.0378],\n","         [ 0.0088, -0.0088, -0.0973,  ..., -0.1190, -0.1616,  0.0029],\n","         [ 0.1010,  0.0351, -0.0818,  ..., -0.0832, -0.1581, -0.0781]],\n","\n","        ...,\n","\n","        [[ 0.1031,  0.0002,  0.0013,  ..., -0.0111, -0.0346, -0.0562],\n","         [-0.0689, -0.0223,  0.0662,  ...,  0.0188,  0.0029,  0.0011],\n","         [-0.1184, -0.0408,  0.1033,  ...,  0.0932,  0.1472,  0.0378],\n","         ...,\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [-0.1096, -0.0495,  0.0060,  ..., -0.0257, -0.0144,  0.0407],\n","         [-0.0174, -0.0057,  0.0216,  ...,  0.0101, -0.0109, -0.0404]],\n","\n","        [[ 0.2126,  0.0498, -0.0047,  ...,  0.0146, -0.0202, -0.0968],\n","         [ 0.0407,  0.0272,  0.0602,  ...,  0.0445,  0.0173, -0.0396],\n","         [-0.0088,  0.0088,  0.0973,  ...,  0.1190,  0.1616, -0.0029],\n","         ...,\n","         [ 0.1096,  0.0495, -0.0060,  ...,  0.0257,  0.0144, -0.0407],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","         [ 0.0921,  0.0438,  0.0156,  ...,  0.0358,  0.0035, -0.0810]],\n","\n","        [[ 0.1205,  0.0059, -0.0203,  ..., -0.0212, -0.0237, -0.0158],\n","         [-0.0514, -0.0167,  0.0446,  ...,  0.0087,  0.0138,  0.0414],\n","         [-0.1010, -0.0351,  0.0818,  ...,  0.0832,  0.1581,  0.0781],\n","         ...,\n","         [ 0.0174,  0.0057, -0.0216,  ..., -0.0101,  0.0109,  0.0404],\n","         [-0.0921, -0.0438, -0.0156,  ..., -0.0358, -0.0035,  0.0810],\n","         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n","       device='cuda:0')\n","torch.Size([500, 256, 256])\n","tensor([[[ 2.3494e-01,  2.9328e-02,  7.6197e-03,  ...,  2.1244e-02,\n","          -3.4866e-01,  1.4312e-02],\n","         [ 6.2988e-02,  6.7451e-03,  7.2478e-02,  ...,  5.1149e-02,\n","          -3.1116e-01,  7.1529e-02],\n","         [ 1.3485e-02, -1.1672e-02,  1.0965e-01,  ...,  1.2562e-01,\n","          -1.6687e-01,  1.0823e-01],\n","         ...,\n","         [ 1.3188e-01,  2.9088e-02,  6.3056e-03,  ...,  3.2376e-02,\n","          -3.1408e-01,  7.0466e-02],\n","         [ 2.2327e-02, -2.0429e-02,  1.2317e-02,  ...,  6.6397e-03,\n","          -3.2848e-01,  1.1113e-01],\n","         [ 1.1444e-01,  2.3398e-02,  2.7873e-02,  ...,  4.2460e-02,\n","          -3.2498e-01,  3.0098e-02]],\n","\n","        [[ 2.3479e-01,  2.8730e-02,  6.4743e-03,  ...,  2.1685e-02,\n","          -3.4617e-01,  1.4664e-02],\n","         [ 6.2841e-02,  6.1463e-03,  7.1333e-02,  ...,  5.1590e-02,\n","          -3.0867e-01,  7.1881e-02],\n","         [ 1.3338e-02, -1.2271e-02,  1.0850e-01,  ...,  1.2606e-01,\n","          -1.6437e-01,  1.0858e-01],\n","         ...,\n","         [ 1.3174e-01,  2.8490e-02,  5.1601e-03,  ...,  3.2818e-02,\n","          -3.1158e-01,  7.0818e-02],\n","         [ 2.2180e-02, -2.1028e-02,  1.1171e-02,  ...,  7.0809e-03,\n","          -3.2598e-01,  1.1149e-01],\n","         [ 1.1429e-01,  2.2799e-02,  2.6728e-02,  ...,  4.2901e-02,\n","          -3.2248e-01,  3.0450e-02]],\n","\n","        [[ 2.3443e-01,  2.8254e-02,  7.6727e-03,  ...,  2.1147e-02,\n","          -3.4706e-01,  1.5519e-02],\n","         [ 6.2477e-02,  5.6703e-03,  7.2531e-02,  ...,  5.1051e-02,\n","          -3.0956e-01,  7.2735e-02],\n","         [ 1.2974e-02, -1.2747e-02,  1.0970e-01,  ...,  1.2552e-01,\n","          -1.6527e-01,  1.0944e-01],\n","         ...,\n","         [ 1.3137e-01,  2.8014e-02,  6.3586e-03,  ...,  3.2279e-02,\n","          -3.1248e-01,  7.1673e-02],\n","         [ 2.1817e-02, -2.1504e-02,  1.2370e-02,  ...,  6.5425e-03,\n","          -3.2687e-01,  1.1234e-01],\n","         [ 1.1393e-01,  2.2323e-02,  2.7926e-02,  ...,  4.2362e-02,\n","          -3.2338e-01,  3.1304e-02]],\n","\n","        ...,\n","\n","        [[ 1.7035e-01,  5.3679e-02, -2.8303e-02,  ..., -3.1764e-02,\n","          -2.0058e-01,  1.9622e-02],\n","         [-1.6032e-03,  3.1095e-02,  3.6556e-02,  ..., -1.8590e-03,\n","          -1.6308e-01,  7.6838e-02],\n","         [-5.1106e-02,  1.2678e-02,  7.3724e-02,  ...,  7.2610e-02,\n","          -1.8786e-02,  1.1354e-01],\n","         ...,\n","         [ 6.7293e-02,  5.3439e-02, -2.9617e-02,  ..., -2.0631e-02,\n","          -1.6600e-01,  7.5775e-02],\n","         [-4.2264e-02,  3.9215e-03, -2.3606e-02,  ..., -4.6368e-02,\n","          -1.8039e-01,  1.1644e-01],\n","         [ 4.9846e-02,  4.7748e-02, -8.0494e-03,  ..., -1.0548e-02,\n","          -1.7690e-01,  3.5407e-02]],\n","\n","        [[ 1.7167e-01,  5.4011e-02, -2.5772e-02,  ..., -3.0807e-02,\n","          -1.9908e-01,  1.8860e-02],\n","         [-2.8393e-04,  3.1427e-02,  3.9086e-02,  ..., -9.0213e-04,\n","          -1.6158e-01,  7.6076e-02],\n","         [-4.9787e-02,  1.3010e-02,  7.6254e-02,  ...,  7.3567e-02,\n","          -1.7289e-02,  1.1278e-01],\n","         ...,\n","         [ 6.8612e-02,  5.3771e-02, -2.7086e-02,  ..., -1.9674e-02,\n","          -1.6450e-01,  7.5014e-02],\n","         [-4.0945e-02,  4.2536e-03, -2.1075e-02,  ..., -4.5411e-02,\n","          -1.7890e-01,  1.1568e-01],\n","         [ 5.1165e-02,  4.8080e-02, -5.5187e-03,  ..., -9.5910e-03,\n","          -1.7540e-01,  3.4646e-02]],\n","\n","        [[ 1.7112e-01,  5.3181e-02, -2.7816e-02,  ..., -2.8420e-02,\n","          -1.9918e-01,  1.8841e-02],\n","         [-8.2954e-04,  3.0598e-02,  3.7042e-02,  ...,  1.4848e-03,\n","          -1.6168e-01,  7.6057e-02],\n","         [-5.0333e-02,  1.2181e-02,  7.4211e-02,  ...,  7.5954e-02,\n","          -1.7388e-02,  1.1276e-01],\n","         ...,\n","         [ 6.8066e-02,  5.2941e-02, -2.9130e-02,  ..., -1.7287e-02,\n","          -1.6460e-01,  7.4995e-02],\n","         [-4.1490e-02,  3.4239e-03, -2.3119e-02,  ..., -4.3024e-02,\n","          -1.7900e-01,  1.1566e-01],\n","         [ 5.0619e-02,  4.7250e-02, -7.5626e-03,  ..., -7.2041e-03,\n","          -1.7550e-01,  3.4626e-02]]], device='cuda:0')\n","Processing batch 13...\n","torch.Size([256, 256, 256])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-2b8627e92b6c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mlatent_ood_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_latent_ood_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mood_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_ood_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-7d16e8449da6>\u001b[0m in \u001b[0;36mfind_latent_ood_points\u001b[0;34m(points, k, m)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# 1. find boundary points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mkth_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kth_dists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mboundary_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-7d16e8449da6>\u001b[0m in \u001b[0;36mget_kth_dists\u001b[0;34m(pts1, pts2)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mdiffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpts2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffs\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0msorted_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     def backward(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mguard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DisableFuncTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    595\u001b[0m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m                         \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n\u001b[1;32m    348\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             nonzero_finite_vals = torch.masked_select(\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["ood_synsetid_to_pc = {}\n","for i in range(len(cates_for_oods)):\n","  pc = all_oods[i]\n","  synsetid = cate_to_synsetid[cates_for_oods[i]]\n","  if synsetid not in ood_synsetid_to_pc:\n","    ood_synsetid_to_pc[synsetid] = []\n","  ood_synsetid_to_pc[synsetid].append(torch.unsqueeze(pc, 0))\n","\n","for key in ood_synsetid_to_pc:\n","  ood_synsetid_to_pc[key] = torch.concat(ood_synsetid_to_pc[key])\n","  print(key)\n","  print(ood_synsetid_to_pc[key].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lcc3RBOlB2hj","executionInfo":{"status":"ok","timestamp":1712751354454,"user_tz":240,"elapsed":3,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"28087d60-0ecf-4336-face-a45e4c90d145"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03001627\n","torch.Size([107, 2048, 3])\n"]}]},{"cell_type":"code","source":["i = torch.randint(low=0, high=all_oods.shape[0]-1, size=(1,1)).item()\n","pc_truth = refs_for_oods[i].cpu().numpy()\n","pc_ood = all_oods[i].cpu().numpy()\n","print(all_oods.shape)\n","plot_point_cloud(pc_truth, color='blue')\n","plot_point_cloud(pc_ood, color='red')"],"metadata":{"id":"02Lq_LB8cwb4","executionInfo":{"status":"ok","timestamp":1712751361748,"user_tz":240,"elapsed":1096,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ffb2939a-fba6-4312-ecfb-00d53d5c0986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([107, 2048, 3])\n"]},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"90cbb7a1-ee05-45f4-82b3-959ec0209b9a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"90cbb7a1-ee05-45f4-82b3-959ec0209b9a\")) {                    Plotly.newPlot(                        \"90cbb7a1-ee05-45f4-82b3-959ec0209b9a\",                        [{\"marker\":{\"color\":\"blue\",\"size\":1},\"mode\":\"markers\",\"x\":[-0.08159765601158142,0.18615961074829102,0.2011519968509674,-0.12001972645521164,-0.1229570284485817,0.1906460076570511,0.17364242672920227,-0.1007218211889267,-0.07850727438926697,-0.15494999289512634,-0.005703584291040897,-0.13790826499462128,-0.17979399859905243,0.05939680337905884,-0.15881982445716858,0.17631252110004425,-0.12279818207025528,-0.09806298464536667,0.018764838576316833,-0.13894596695899963,0.054871588945388794,0.0587385930120945,-0.17570970952510834,-0.14318087697029114,-0.042077526450157166,0.01383945345878601,-0.17979399859905243,-0.14019910991191864,0.2011519968509674,0.09777196496725082,0.12673363089561462,-0.09439440816640854,-0.03572067618370056,-0.08586038649082184,-0.08868981152772903,0.04194396734237671,0.15824458003044128,0.13220570981502533,0.06667594611644745,-0.037308529019355774,0.16108541190624237,-0.013271168805658817,-0.1358705461025238,0.15151138603687286,0.19363023340702057,-0.11539801210165024,0.009144008159637451,-0.17479166388511658,-0.01808933913707733,0.16991591453552246,-0.03766787052154541,-0.12059801071882248,-0.006537885405123234,-0.12774354219436646,0.2011519968509674,-0.1381726861000061,0.18766716122627258,-0.03271633759140968,0.14187562465667725,0.18389813601970673,-0.04573255777359009,-0.04631704092025757,0.02035161852836609,-0.026610493659973145,0.1448192596435547,0.19733679294586182,-0.11276224255561829,-0.05893678963184357,0.13070333003997803,0.031051844358444214,-0.17829644680023193,-0.10794974118471146,0.03624969720840454,0.17847751080989838,-0.020081087946891785,0.1961899846792221,0.039885833859443665,-0.01671937108039856,-0.021769016981124878,0.1743643879890442,0.057970065623521805,-0.1449136584997177,-0.17979399859905243,-0.09257170557975769,0.11282355338335037,-0.04993688315153122,-0.11609330028295517,0.1818656474351883,0.12142910063266754,0.19892309606075287,-0.07553358376026154,0.18807293474674225,-0.003970176912844181,0.16856157779693604,-0.16511200368404388,0.13008259236812592,0.0391814261674881,0.020281240344047546,-0.17979399859905243,-0.053705424070358276,0.11114487797021866,0.1733763962984085,-0.022872835397720337,-0.17979399859905243,-0.04041530191898346,0.030616536736488342,-0.13391166925430298,0.13342569768428802,0.08285882323980331,0.1901199221611023,-0.03219115734100342,-0.1658332645893097,0.17880043387413025,0.05325658619403839,-0.16205547749996185,0.10018681734800339,0.06532609462738037,0.098472960293293,-0.1703157275915146,0.03290688991546631,0.2011519968509674,-0.0833873599767685,0.0949399545788765,-0.10181650519371033,-0.007622436620295048,0.09439701586961746,-0.07209637761116028,0.1763080209493637,0.18722090125083923,0.05703532695770264,-0.1026105135679245,0.14168816804885864,-0.06714305281639099,0.1438218057155609,0.18978694081306458,-0.075204998254776,0.042454689741134644,-0.17915013432502747,0.19179046154022217,-0.0967390313744545,-0.0697115957736969,-0.17979399859905243,-0.13494399189949036,-0.04647921025753021,0.12125568091869354,0.18655423820018768,0.11760525405406952,-0.02662883698940277,-0.08357030898332596,0.12271890044212341,0.02538798749446869,0.09758996963500977,-0.07180207967758179,-0.05484268069267273,-0.17979399859905243,0.18051955103874207,-0.14489434659481049,0.06662056595087051,0.13055291771888733,0.05656857788562775,-0.16264328360557556,-0.15494999289512634,0.19703145325183868,0.18314270675182343,-0.04270757734775543,0.1289931684732437,0.1985010802745819,0.015803784132003784,-0.07299552857875824,0.1295415163040161,-0.12770594656467438,0.10734928399324417,0.15191954374313354,-0.06752260029315948,-0.12327346950769424,-0.1773177683353424,-0.1425158977508545,-0.07460828125476837,0.049839798361063004,0.1924964189529419,-0.0986476019024849,-0.09501324594020844,-0.08011020720005035,0.008258670568466187,0.13795679807662964,-0.15559592843055725,-0.1377684772014618,0.14520065486431122,-0.12133168429136276,-0.17156264185905457,0.14602917432785034,0.12254920601844788,-0.08050323277711868,0.18461807072162628,-0.018709436058998108,-0.1553126573562622,-0.07162579894065857,0.046912506222724915,-0.027865201234817505,0.004577010869979858,-0.17556698620319366,-0.05632701516151428,-0.1219383254647255,0.040929585695266724,-0.17902736365795135,-0.15599863231182098,-0.14276222884655,0.06392556428909302,0.05584380030632019,-0.15878912806510925,0.11196264624595642,0.2011519968509674,0.1962878555059433,0.1165681928396225,-0.17979399859905243,-0.05389252305030823,0.17714886367321014,-0.17236189544200897,0.10842174291610718,-0.12770946323871613,-0.15494999289512634,-0.09029630571603775,-0.0969468355178833,-0.17979399859905243,-0.012174845673143864,0.013756886124610901,-0.015164912678301334,0.16539734601974487,-0.019424021244049072,-0.12367331236600876,0.17445118725299835,0.023270413279533386,-0.019590795040130615,0.19949503242969513,-0.12886369228363037,-0.1527949869632721,-0.17979399859905243,-0.06527441740036011,0.046825483441352844,-0.17979399859905243,-0.15494999289512634,-0.09231477975845337,-0.12296020239591599,0.0683470368385315,-0.15265586972236633,0.1971009522676468,0.17466731369495392,-0.17302237451076508,-0.09182468801736832,-0.17979399859905243,0.18893322348594666,-0.11598757654428482,-0.10591736435890198,0.047051191329956055,0.16647504270076752,0.13107989728450775,0.08089233934879303,0.11926296353340149,0.16143958270549774,-0.12609314918518066,0.2011519968509674,0.19167065620422363,-0.15518486499786377,0.04892466962337494,0.0486607700586319,-0.1661078929901123,0.19013048708438873,0.0920155793428421,0.1576361060142517,-0.11755096167325974,-0.04322312772274017,0.1979503184556961,0.19789379835128784,0.17289818823337555,-0.17979399859905243,-0.0628506988286972,0.138189896941185,0.007176309823989868,0.11022701859474182,0.2011519968509674,0.14738935232162476,0.015523672103881836,0.17812807857990265,0.17993475496768951,0.2011519968509674,0.04321672022342682,-0.05563732981681824,-0.17143911123275757,0.07749758660793304,0.16611747443675995,-0.08486806601285934,0.08074791729450226,-0.1531595140695572,-0.1628306806087494,-0.17616254091262817,0.020609021186828613,0.011622816324234009,0.011067137122154236,-0.10838979482650757,0.16667619347572327,0.08667336404323578,-0.036877524107694626,-0.07591550797224045,0.2011519968509674,0.006713539827615023,0.09344305098056793,0.2011519968509674,0.09836254268884659,-0.12182541936635971,0.04051654040813446,-0.010564685799181461,0.2011519968509674,0.1892940253019333,-0.09759073704481125,0.1857762336730957,-0.005384341813623905,0.07885894179344177,-0.12695708870887756,-0.01508388016372919,-0.05598317086696625,0.024605855345726013,0.14298029243946075,0.14000284671783447,-0.07754425704479218,0.12003976851701736,-0.174988254904747,-0.0905846506357193,0.18131442368030548,0.08913341164588928,-0.17979399859905243,0.2011519968509674,-0.06034393608570099,0.16744162142276764,0.16315263509750366,0.2011519968509674,-0.17979399859905243,0.16088715195655823,-0.17247001826763153,-0.033749207854270935,0.07831481844186783,0.04538726806640625,0.012207239866256714,0.1604222059249878,0.2011519968509674,0.19213147461414337,-0.17979399859905243,-0.17444610595703125,0.19625084102153778,-0.05867689102888107,-0.01349483523517847,-0.0741540715098381,0.1322188675403595,0.12143586575984955,0.06123311072587967,0.19504313170909882,0.14060936868190765,-0.1632673591375351,0.020329341292381287,0.18830417096614838,-0.028732270002365112,0.1789626032114029,0.04446999728679657,-0.16851752996444702,-0.10361012816429138,-0.12261388450860977,-0.17979399859905243,-0.17979399859905243,0.19919449090957642,0.006332219112664461,-0.17979399859905243,0.1013452410697937,0.06447845697402954,0.11184399574995041,-0.11530116945505142,0.03998979926109314,0.06117178499698639,-0.10728922486305237,0.01191781461238861,-0.11792799085378647,-0.1663406491279602,-0.12680204212665558,0.03599230572581291,0.0651048943400383,0.15393564105033875,-0.059262558817863464,-0.11376658827066422,-0.17135801911354065,0.009270057082176208,-0.17490698397159576,0.19838787615299225,0.05477544665336609,0.07334516942501068,-0.0837436094880104,0.14578993618488312,-0.1418500393629074,-0.17635847628116608,-0.15940235555171967,0.10775620490312576,-0.17979399859905243,-0.0214175283908844,0.17340171337127686,0.01579907536506653,0.14267820119857788,-0.11015290021896362,0.09676449000835419,0.07100926339626312,0.08099640905857086,-0.15494999289512634,-0.1578771024942398,-0.1609230786561966,0.15310335159301758,-0.13670146465301514,0.11160414665937424,0.11638647317886353,-0.15184496343135834,-0.17979399859905243,-0.12237174063920975,0.10377328097820282,-0.07294036448001862,0.059604570269584656,-0.17451438307762146,-0.08993535488843918,0.04450139403343201,0.08327850699424744,-0.1530759185552597,-0.0952783077955246,-0.025906845927238464,-0.05771232396364212,0.06568539142608643,0.1963249146938324,0.1504364013671875,-0.11034896224737167,0.14788976311683655,-0.04796673357486725,0.018663331866264343,-0.06881755590438843,0.0162799209356308,-0.02857668697834015,0.16169342398643494,0.0575796514749527,-0.07136557996273041,-0.020240962505340576,-0.1646033078432083,-0.07607294619083405,-0.17979399859905243,-0.03539907559752464,0.09690304100513458,0.17724105715751648,0.0655699223279953,-0.03206266462802887,-0.17979399859905243,-0.053955599665641785,0.13166114687919617,-0.06404660642147064,0.053454816341400146,0.08983148634433746,-0.129567950963974,0.06556161493062973,0.20093189179897308,-0.08409241586923599,0.15891611576080322,-0.11326645314693451,-0.053031325340270996,0.13588255643844604,0.06188948452472687,0.1935512125492096,0.0008003264665603638,-0.17636334896087646,0.1894214004278183,-0.11648497730493546,-0.09191643446683884,0.161586731672287,-0.16451126337051392,0.17113028466701508,-0.1435975879430771,0.19070374965667725,0.04299257695674896,0.17138825356960297,0.1548304557800293,0.0074920207262039185,-0.17535234987735748,0.2011519968509674,0.19484679400920868,0.1914469450712204,0.051758453249931335,-0.05520765483379364,-0.17979399859905243,-0.17979399859905243,-0.10393277555704117,0.13074439764022827,0.20002302527427673,0.16609226167201996,-0.17744097113609314,-0.04529757797718048,-0.10049355030059814,0.006014689803123474,0.1339796930551529,-0.1407051980495453,-0.16824793815612793,-0.07613254338502884,0.1438567042350769,0.17750327289104462,0.045421913266181946,-0.173455610871315,-0.11942052096128464,-0.1581927388906479,-0.11532729119062424,-0.07621579617261887,0.07218374311923981,0.17670612037181854,0.14450594782829285,0.052418068051338196,-0.07508410513401031,-0.15609553456306458,0.08769448846578598,0.033694565296173096,0.01774980127811432,-0.15150175988674164,-0.17979399859905243,0.2011519968509674,-0.01721915602684021,-0.17731033265590668,-0.1262848824262619,-0.06896087527275085,-0.0046851495280861855,-0.08822326362133026,0.11197349429130554,-0.17979399859905243,0.1179589256644249,0.0423416793346405,-0.06919194012880325,0.1763080209493637,0.10002728551626205,-0.12184392660856247,0.12652669847011566,0.021476715803146362,-0.008102969266474247,0.11032356321811676,0.0041727423667907715,0.16010341048240662,0.1834382712841034,0.01596866548061371,0.19656804203987122,0.19532161951065063,-0.12790444493293762,-0.17979399859905243,0.1335882693529129,-0.03867608308792114,-0.1750902682542801,-0.17043206095695496,0.1105332225561142,-0.07816462218761444,0.18712887167930603,0.14976665377616882,-0.02480451762676239,-0.14028769731521606,-0.12171851843595505,-0.1702025830745697,0.006594493985176086,-0.14959292113780975,0.09168409556150436,-0.02424316108226776,0.07432349771261215,-0.139177605509758,0.001191452145576477,-0.07333170622587204,-0.10829166322946548,-0.0920882523059845,0.012912750244140625,-0.10963068157434464,-0.10233988612890244,0.2011519968509674,0.16242702305316925,-0.01316049788147211,-0.16141533851623535,-0.15871140360832214,-0.16686733067035675,-0.1565348505973816,-0.04318007826805115,0.14058662950992584,0.15866950154304504,0.05604839324951172,0.18880340456962585,-0.1759173721075058,-0.15494999289512634,-0.17979399859905243,0.002291068434715271,0.2011519968509674,-0.02458895742893219,0.08204440772533417,0.16626772284507751,-0.08069607615470886,-0.0037764161825180054,0.09545041620731354,0.03311102092266083,0.04572683572769165,0.109469473361969,0.05561951547861099,0.1106877326965332,-0.037660256028175354,0.1086495965719223,0.07459746301174164,0.1478399634361267,-0.050768230110406876,0.2011519968509674,0.14385277032852173,-0.07723799347877502,0.17106109857559204,-0.041387490928173065,0.12536276876926422,0.10533054172992706,0.19668245315551758,-0.021706238389015198,0.047708526253700256,0.14480310678482056,0.08654987066984177,-0.036060139536857605,0.08079084008932114,-0.1614159196615219,-0.17979399859905243,0.15228889882564545,0.08844086527824402,-0.15880009531974792,-0.17979399859905243,0.1890394687652588,-0.1574767529964447,-0.01743900775909424,-0.1251320093870163,-0.09662675857543945,-0.15872009098529816,-0.10888563096523285,0.2011519968509674,0.18753628432750702,-0.12127583473920822,0.2011519968509674,-0.009491966105997562,-0.15857335925102234,0.2007564753293991,0.047056734561920166,0.035458266735076904,0.1433727890253067,-0.005651355721056461,0.19500355422496796,-0.06413274258375168,-0.17414042353630066,-0.09506560862064362,-0.011963889934122562,-0.0997994989156723,0.000811263918876648,0.18111774325370789,0.08500707894563675,0.07301618903875351,0.049251556396484375,0.19577831029891968,0.08193729817867279,0.1763080209493637,0.0852309912443161,-0.12385048717260361,-0.0067657241597771645,-0.13377299904823303,0.037672996520996094,-0.0595502108335495,-0.04949626326560974,0.06888435781002045,-0.04363726079463959,-0.09952385723590851,-0.17624391615390778,0.1841934621334076,-0.1015089601278305,-0.12283677607774734,-0.10707798600196838,-0.0018266737461090088,-0.08760710060596466,-0.10820493847131729,-0.10098212957382202,-0.03398854285478592,0.140090212225914,-0.11847283691167831,-0.15664184093475342,-0.17366722226142883,-0.06749442219734192,0.06985124200582504,0.004826143383979797,0.0887400209903717,-0.17035198211669922,-0.11992102116346359,-0.03781522810459137,0.07214455306529999,0.04957301914691925,-0.13423188030719757,-0.07690701633691788,0.045503124594688416,-0.11152315139770508,0.03016480803489685,0.1896246075630188,-0.15139098465442657,-0.17294348776340485,0.15969686210155487,0.17464929819107056,-0.064069002866745,0.15351930260658264,0.056807827204465866,-0.14558856189250946,-0.07030896842479706,0.06432703137397766,0.06816054880619049,-0.09319239109754562,-0.17051197588443756,0.1803414523601532,-0.07810671627521515,-0.17706073820590973,-0.14526472985744476,-0.0346301794052124,-0.06637494266033173,0.13046766817569733,-0.04255884885787964,0.10933522880077362,0.12986469268798828,0.18588081002235413,-0.17565488815307617,-0.16022787988185883,-0.17471477389335632,0.11415809392929077,0.07178915292024612,0.14016924798488617,0.013097986578941345,-0.0064385393634438515,0.053953662514686584,0.1763080209493637,0.1763080209493637,0.1331942230463028,-0.15695123374462128,0.1566189080476761,-0.17545661330223083,-0.007597045041620731,-0.1002352237701416,-0.07871393114328384,0.2011519968509674,-0.0810086727142334,-0.1656876802444458,0.10147431492805481,-0.0015895366668701172,0.16299352049827576,0.18749450147151947,0.1763080209493637,-0.12512052059173584,0.04543669521808624,0.1720401644706726,-0.16702622175216675,0.06839107722043991,0.11263998597860336,0.03584565222263336,-0.09088815003633499,-0.0069718072190880775,0.007573217153549194,-0.16390176117420197,-0.17979399859905243,-0.015552910976111889,0.0023310035467147827,0.04540301859378815,0.14600799977779388,0.12968827784061432,-0.14016731083393097,0.12449531257152557,0.19615618884563446,0.08705048263072968,-0.062343403697013855,0.12089689075946808,0.14077095687389374,0.030325472354888916,-0.13792070746421814,0.028712347149848938,0.01959514617919922,-0.025967732071876526,0.10808427631855011,-0.17428232729434967,0.2011519968509674,0.193358913064003,-0.04880811274051666,-0.15329138934612274,-0.04700814187526703,-0.0954974889755249,0.2011519968509674,0.18125082552433014,0.1960567682981491,-0.1706427037715912,0.1761663556098938,-0.09197176992893219,-0.17979399859905243,0.2011519968509674,-0.17764224112033844,-0.03982551395893097,-0.09632889181375504,-0.13640424609184265,0.1888001263141632,-0.1303909868001938,0.19286490976810455,-0.16453607380390167,0.20099520683288574,0.10312779992818832,0.19387517869472504,-0.16713277995586395,0.15417005121707916,0.04926423728466034,0.032576218247413635,0.17795245349407196,0.004965752363204956,-0.17979399859905243,-0.08158162236213684,-0.1070924773812294,-0.16771391034126282,0.13236884772777557,0.06702080368995667,0.1175004243850708,-0.03392200171947479,0.10253024846315384,0.2011519968509674,0.1873573362827301,-0.15935266017913818,-0.013721526600420475,0.1557551622390747,-0.05259573459625244,0.06311286985874176,-0.11017438024282455,-0.1795627921819687,0.04972614347934723,-0.040245458483695984,0.16594117879867554,0.12732982635498047,0.009052172303199768,-0.00967750046402216,0.2002900391817093,0.14345812797546387,-0.1295282542705536,-0.05081285536289215,0.1847119778394699,-0.05826025456190109,0.18881116807460785,-0.1662588268518448,0.2011519968509674,-0.100123330950737,-0.0647382065653801,0.2011519968509674,-0.174297034740448,0.1763080209493637,0.1625901162624359,-0.06666600704193115,0.07196786999702454,-0.111665740609169,0.13406342267990112,-0.09308455884456635,-0.06807462871074677,-0.06219594180583954,-0.1011810377240181,0.08928214013576508,-0.17979399859905243,0.06009897217154503,-0.08749645948410034,0.14180685579776764,0.07780995219945908,0.16629557311534882,-0.13741299510002136,0.13992978632450104,0.17681169509887695,0.13361938297748566,-0.1726580709218979,-0.0014308840036392212,-0.13446813821792603,0.1812991499900818,-0.05535431206226349,-0.052496910095214844,0.09014573693275452,-0.1303822249174118,0.05434022843837738,-0.16384388506412506,0.1999664455652237,0.20094749331474304,-0.0677066370844841,0.16912910342216492,0.06906577944755554,-0.06949637085199356,0.17947961390018463,-0.05276164412498474,0.07066689431667328,-0.08915571123361588,0.15914322435855865,0.19781705737113953,-0.1610865592956543,-0.11319676786661148,0.030709952116012573,-0.01627267897129059,0.1994134932756424,-0.0055148908868432045,0.19414858520030975,-0.16311104595661163,-0.14896230399608612,-0.027288541197776794,-0.032608762383461,0.1702866554260254,-0.14572641253471375,-0.17134857177734375,-0.0041794488206505775,-0.17138594388961792,-0.024476423859596252,0.11807295680046082,-0.15494999289512634,-0.07054182887077332,-0.022715076804161072,0.045634523034095764,-0.17691560089588165,0.003092333674430847,-0.14337800443172455,0.1675429493188858,0.18552210927009583,0.2011519968509674,-0.03779095411300659,-0.0005805045366287231,0.2011519968509674,0.03307957947254181,0.015088707208633423,0.2011519968509674,0.1733441948890686,-0.004775256849825382,0.08085910975933075,0.09884393960237503,-0.016078967601060867,0.18898242712020874,-0.07316355407238007,-0.09511475265026093,0.18402333557605743,0.08004426956176758,0.037145450711250305,-0.17979399859905243,-0.15494999289512634,-0.03605334460735321,-0.15494999289512634,-0.06465134024620056,0.1784353256225586,-0.0674976259469986,-0.13952749967575073,-0.16278143227100372,-0.058369532227516174,-0.03912809491157532,0.16464167833328247,0.19714289903640747,0.027029648423194885,-0.027317393571138382,0.013395100831985474,-0.16966257989406586,0.18910449743270874,-0.15494999289512634,0.023403316736221313,-0.15057902038097382,0.16015899181365967,-0.09215480834245682,0.0277368426322937,-0.07009223103523254,-0.17979399859905243,0.17784184217453003,-0.08564300835132599,0.19547635316848755,0.1588716208934784,0.08267620950937271,0.10682368278503418,0.10543305426836014,-0.05811235308647156,-0.17621123790740967,-0.16255918145179749,-0.054116204380989075,0.08550994098186493,-0.11785278469324112,-0.17755016684532166,-0.04893931746482849,-0.17107412219047546,0.1382167935371399,0.0326533168554306,0.19833485782146454,0.2011519968509674,-0.17979399859905243,0.18711398541927338,0.1763080209493637,0.10591575503349304,0.17313635349273682,0.15148237347602844,0.007983416318893433,-0.14073383808135986,0.03882679343223572,-0.05869947373867035,-0.010629520751535892,-0.10347768664360046,-0.13799922168254852,-0.17197012901306152,-0.17736944556236267,-0.1334536224603653,-0.14795252680778503,0.175727978348732,0.194364532828331,0.07562007009983063,-0.09421884268522263,0.19782234728336334,0.14898037910461426,-0.15749506652355194,-0.09239324927330017,0.10934801399707794,0.028515130281448364,0.1367601901292801,0.1281435638666153,-0.04241718724370003,-0.14413361251354218,0.060156263411045074,0.16413922607898712,-0.169387549161911,-0.06948715448379517,0.1333947479724884,0.2011519968509674,-0.13109129667282104,-0.17979399859905243,0.17803941667079926,0.08632804453372955,0.08294184505939484,-0.1566648930311203,0.16416507959365845,-0.11974500864744186,0.2011519968509674,0.05146384611725807,-0.008374900557100773,0.1760721206665039,0.11478032171726227,0.06128518283367157,0.041820675134658813,0.07090557366609573,-0.042490504682064056,0.2011519968509674,0.2011519968509674,0.1829727590084076,0.05649423599243164,0.023555666208267212,0.12011086195707321,0.07966767251491547,0.18053162097930908,0.20111556351184845,-0.17979399859905243,0.1397840529680252,0.021160989999771118,-0.16105075180530548,-0.12911251187324524,0.15176266431808472,-0.13232699036598206,0.12653183937072754,0.14253456890583038,0.1758062094449997,-0.15719084441661835,0.03771412372589111,0.02056308090686798,0.10718217492103577,0.04168400168418884,0.10223720967769623,-0.1570119857788086,0.057245150208473206,0.19266277551651,-0.15518265962600708,-0.011501953937113285,0.05751028656959534,0.111844502389431,0.11926694959402084,-0.07684967666864395,-0.09836249053478241,0.05356137454509735,-0.1589919626712799,0.09264400601387024,0.2011519968509674,0.0685587227344513,-0.1470467746257782,-0.047010600566864014,-0.16167587041854858,0.011278033256530762,0.17233291268348694,0.1502726823091507,0.13546009361743927,0.14418037235736847,0.04008488357067108,0.16372624039649963,0.1889953911304474,-0.13916099071502686,0.2011519968509674,0.028849318623542786,0.027448177337646484,0.2011519968509674,0.126828134059906,0.2011519968509674,0.1763080209493637,0.02951858937740326,0.07450234889984131,0.037408679723739624,0.05401361733675003,0.039546892046928406,-0.09300003200769424,-0.0954977422952652,-0.04305025935173035,-0.13674092292785645,0.11271019279956818,-0.03357881307601929,0.1763080209493637,0.13346035778522491,-0.17048443853855133,-0.15369780361652374,0.18292289972305298,0.14046752452850342,0.04588630795478821,-0.16634364426136017,-0.17979399859905243,0.09057866036891937,0.08217056095600128,-0.055843859910964966,0.11931587755680084,0.061041802167892456,0.11259927600622177,0.055684953927993774,0.13780450820922852,-0.1165151372551918,0.041960954666137695,0.1763080209493637,-0.16751012206077576,0.1502225697040558,-0.15908902883529663,0.2011519968509674,0.03675264120101929,0.2011519968509674,0.18100875616073608,-0.020198002457618713,0.1763080209493637,-0.06832800805568695,-0.15409493446350098,-0.03102792799472809,0.15921881794929504,0.17613013088703156,-0.024932190775871277,0.11433599889278412,-0.009684399701654911,-0.003242000937461853,-0.06560864299535751,-0.05247020721435547,-0.014965656213462353,-0.006430045701563358,0.1778896152973175,-0.06027178466320038,0.0188208669424057,0.13110004365444183,0.07901910692453384,0.17464885115623474,0.050517693161964417,0.2011519968509674,-0.06678934395313263,-0.061638884246349335,0.18194374442100525,0.026707366108894348,-0.14204974472522736,0.04795926809310913,0.1952151358127594,-0.14735649526119232,0.011037901043891907,-0.04807464778423309,-0.1250159740447998,-0.04753626883029938,-0.15372541546821594,-0.03553037345409393,-0.16849614679813385,0.17609627544879913,-0.15494999289512634,0.03993525728583336,0.13292309641838074,-0.07032738626003265,0.059738773852586746,0.08752693980932236,0.08881089836359024,0.04549935460090637,0.1763080209493637,-0.07514099031686783,0.19006304442882538,0.199520543217659,-0.05256661772727966,-0.17979399859905243,0.06917956471443176,-0.17979399859905243,-0.01981407403945923,-0.06503592431545258,-0.0887359082698822,-0.13811947405338287,-0.17742255330085754,0.05368867143988609,0.1782466173171997,-0.06815704703330994,0.08477698266506195,-0.1164596900343895,-0.06409747153520584,-0.06968267261981964,-0.04072368144989014,0.19714899361133575,0.15846607089042664,-0.1549801379442215,0.014876633882522583,0.16754122078418732,-0.02553398907184601,-0.16290244460105896,-0.00874760840088129,-0.17047414183616638,-0.09277894347906113,-0.16960705816745758,0.002240583300590515,0.1841963529586792,-0.021724358201026917,-0.13464795053005219,-0.10723210126161575,0.11107833683490753,-0.10536038875579834,0.1579074263572693,-0.0029606223106384277,-0.15507638454437256,0.1963135153055191,-0.00911215040832758,0.002704039216041565,-0.011440039612352848,0.088509701192379,0.05913226306438446,-0.02363012731075287,0.19708584249019623,0.04165944457054138,0.13356046378612518,-0.16910922527313232,0.16627001762390137,-0.16588543355464935,-0.013310701586306095,0.2011519968509674,-0.05048981308937073,-0.07653529942035675,-0.07398845255374908,0.19984585046768188,0.1763080209493637,-0.036787450313568115,0.1706295758485794,0.04704698920249939,0.057890385389328,-0.1705225259065628,0.2011519968509674,-0.15752294659614563,-0.056102171540260315,0.196980282664299,-0.15494999289512634,0.0441649854183197,0.20058345794677734,-0.00601008627563715,0.0334821492433548,0.16749480366706848,0.06347745656967163,-0.13586463034152985,0.09221971035003662,-0.0748174637556076,-0.16749009490013123,-0.09057045727968216,0.1763080209493637,-0.15494999289512634,-0.05666813254356384,-0.019865229725837708,-0.07408221065998077,-0.08410433679819107,-0.06769062578678131,0.2011519968509674,0.10627380013465881,-0.16844959557056427,0.19619670510292053,0.09612523764371872,-0.1638125330209732,0.08017350733280182,-0.11922412365674973,-0.10138329863548279,0.009012207388877869,0.17579308152198792,0.05682072043418884,-0.08032318204641342,0.030320804566144943,0.13740192353725433,0.05854372680187225,-0.17089681327342987,-0.1343432366847992,-0.17979399859905243,-0.17394964396953583,0.1834017038345337,-0.08343110978603363,0.1447334736585617,0.13113540410995483,-0.0808514654636383,-0.10462261736392975,0.18719302117824554,0.10283826291561127,-0.08300979435443878,0.08473542332649231,-0.008530200459063053,0.04838864505290985,-0.06703448295593262,0.18318647146224976,0.02901579439640045,0.0262296199798584,0.10843086242675781,-0.12505659461021423,0.07400244474411011,0.1440536230802536,-0.0729072242975235,0.18914631009101868,0.1388145387172699,-0.0921468660235405,0.2011519968509674,-0.05163662135601044,0.15680037438869476,-0.043445490300655365,-0.014216096140444279,0.018846601247787476,-0.013403029181063175,0.04521860182285309,-0.022219717502593994,-0.16703297197818756,-0.16907072067260742,0.1760789155960083,0.1763080209493637,0.19374561309814453,0.2011519968509674,0.2011519968509674,0.12268465757369995,-0.036550283432006836,0.19808049499988556,-0.11196241527795792,-0.044753625988960266,0.0909603089094162,0.18961986899375916,-0.0258139967918396,-0.029409989714622498,-0.06640904396772385,0.16051416099071503,0.19963155686855316,0.1763080209493637,-0.11493252962827682,0.1481245756149292,0.17200349271297455,-0.004971862770617008,0.1763080209493637,-0.05728950351476669,-0.14240425825119019,-0.16082105040550232,-0.10197453200817108,0.011103644967079163,0.0024463385343551636,0.15481401979923248,-0.00039725005626678467,0.2011519968509674,0.19606007635593414,-0.12316136807203293,-0.019955217838287354,-0.10498017072677612,-0.04536491632461548,0.03419691324234009,-0.10243789851665497,0.18348349630832672,0.10030447691679001,-0.1101602166891098,-0.16625665128231049,0.08769974112510681,-0.004204274155199528,-0.1551133692264557,0.1894122064113617,-0.16602052748203278,-0.16374166309833527,0.08422686159610748,-0.09889086335897446,0.024794071912765503,-0.06995173543691635,-0.16165131330490112,-0.16888836026191711,-0.17979399859905243,-0.020590409636497498,-0.15494999289512634,-0.07368738949298859,0.00458814250305295,-0.15729917585849762,-0.17979399859905243,0.008618995547294617,-0.14892160892486572,0.1763080209493637,-0.1173982247710228,0.15090051293373108,-0.08473358303308487,-0.17055708169937134,0.1250729262828827,-0.04872626066207886,0.053879231214523315,-0.05759258568286896,-0.004754872061312199,-0.11862356215715408,-0.1266201287508011,0.08932562172412872,0.17223963141441345,0.039362579584121704,0.020753905177116394,0.10205332934856415,0.02034778892993927,0.09429250657558441,0.18732447922229767,0.17809292674064636,0.06679841876029968,-0.15165328979492188,0.17964424192905426,0.1089543029665947,-0.17979399859905243,-0.16625170409679413,-0.10908082872629166,-0.04815681278705597,-0.17934808135032654,-0.002519085071980953,-0.09000632911920547,0.18875393271446228,-0.062010183930397034,0.13105401396751404,-0.05082803964614868,-0.06826461851596832,-0.13692519068717957,0.1767188459634781,-0.16554571688175201,0.19433270394802094,-0.13850408792495728,-0.15820810198783875,-0.025189533829689026,0.13466262817382812,-0.047751933336257935,-0.17372988164424896,-0.10843440890312195,0.06776928156614304,0.05808085575699806,0.1455189287662506,0.0932675302028656,0.2011519968509674,-0.13333484530448914,0.04572427272796631,-0.17068760097026825,-0.17728745937347412,-0.17979399859905243,0.17820724844932556,0.19681885838508606,0.08147907257080078,-0.032253239303827286,-0.14965961873531342,0.001575559377670288,-0.1704249382019043,-0.15757186710834503,-0.012670070864260197,-0.13032394647598267,-0.17979399859905243,0.09838582575321198,-0.11053318530321121,-0.17979399859905243,0.13454055786132812,0.012753993272781372,0.08314977586269379,0.18713825941085815,-0.13724291324615479,0.001972481608390808,0.16764968633651733,0.1802239567041397,-0.06035351753234863,-0.03903110325336456,-0.01658819615840912,0.13621941208839417,0.13450582325458527,0.11350453644990921,0.007641121745109558,0.18751108646392822,-0.09806719422340393,-0.025984838604927063,0.1865435391664505,-0.1582600474357605,-0.10591986775398254,-0.026500552892684937,-0.15494999289512634,-0.03468422219157219,-0.08511372655630112,-0.17568917572498322,0.11040925979614258,0.19040508568286896,0.07601131498813629,-0.15494999289512634,0.1795593798160553,-0.17979399859905243,-0.03765878081321716,-0.15494999289512634,0.04579804837703705,0.009176373481750488,-0.10120727866888046,0.14693161845207214,0.025343969464302063,0.19472450017929077,-0.017536893486976624,0.009979993104934692,-0.1318427324295044,0.19079189002513885,0.05825847387313843,0.1763080209493637,-0.15494999289512634,0.03240637481212616,-0.15664875507354736,-0.05852912366390228,0.20024673640727997,0.2011519968509674,0.15121501684188843,0.12285565584897995,-0.02792249619960785,-0.17416909337043762,-0.09964300692081451,-0.05136028677225113,0.07978089898824692,-0.09306113421916962,-0.17979399859905243,0.0019178837537765503,0.09573112428188324,0.03407791256904602,-0.1635819375514984,-0.17979399859905243,0.12462742626667023,0.19367964565753937,-0.1664981245994568,-0.054466813802719116,-0.15002743899822235,0.1315508484840393,-0.16692757606506348,0.2011519968509674,0.15386676788330078,0.14889293909072876,-0.13820531964302063,-0.17979399859905243,0.13845692574977875,0.11657167971134186,0.04479256272315979,-0.006948263384401798,-0.018830031156539917,0.062334075570106506,0.020589619874954224,-0.17281676828861237,0.13882525265216827,-0.16632822155952454,-0.13296864926815033,0.18268020451068878,0.09920493513345718,0.2011519968509674,0.2011519968509674,0.18959873914718628,0.007369980216026306,-0.09552033245563507,-0.16823014616966248,-0.10082205384969711,0.14362667500972748,0.03448086977005005,-0.01756700873374939,0.03317265212535858,0.1129339262843132,0.16920551657676697,-0.09482273459434509,0.0034352242946624756,0.002347514033317566,-0.00023230817168951035,-0.17469120025634766,0.048678576946258545,0.10947361588478088,0.1416628211736679,0.1769658476114273,-0.1514943689107895,0.19481271505355835,-0.01521442923694849,0.11714355647563934,0.1763080209493637,-0.17979399859905243,0.1812823861837387,0.07482083141803741,-0.08794954419136047,-0.15849712491035461,0.000974997878074646,0.010479643940925598,0.06188581883907318,0.003948703408241272,0.13225588202476501,0.20094385743141174,-0.16344855725765228,-0.04818202555179596,0.2011519968509674,-0.17817267775535583,-0.06531760096549988,-0.1426834762096405,0.2011519968509674,-0.047688085585832596,-0.16453585028648376,-0.11269324272871017,0.19801649451255798,-0.17979399859905243,-0.1092412993311882,-0.15494999289512634,-0.12946370244026184,-0.17488890886306763,0.11595027148723602,-0.12880529463291168,0.06109381094574928,0.1310701221227646,-0.11793477088212967,0.09083159267902374,0.1019839197397232,-0.021121039986610413,-0.15494999289512634,0.026991456747055054,0.08626456558704376,-0.14985528588294983,0.1786159873008728,0.18350985646247864,0.16797293722629547,-0.10237070918083191,0.2011519968509674,0.18881163001060486,0.18837031722068787,0.12784795463085175,-0.16585853695869446,-0.17492590844631195,0.0446137934923172,0.1852436512708664,0.10935287922620773,-0.06884320080280304,0.00877353549003601,-0.14390157163143158,-0.16935190558433533,-0.10948921740055084,0.045323446393013,-0.038270026445388794,-0.06741705536842346,0.06841044127941132,0.010312408208847046,-0.08944766223430634,-0.16896389424800873,-0.07617663592100143,0.09634383767843246,0.06909818947315216,-0.05075845867395401,0.19982320070266724,-0.10096637904644012,-0.0725252628326416,-0.01161362323909998,0.053683310747146606,0.12193760275840759,-0.1274564415216446,0.2011519968509674,0.16164207458496094,0.14653441309928894,0.2011519968509674,-0.06949552893638611,-0.07352765649557114,0.2011519968509674,-0.10714048147201538,0.030092984437942505,-0.1244904175400734,0.17895039916038513,-0.15822874009609222,-0.10709696263074875,0.04694162309169769,-0.07303375005722046,-0.1753987818956375,-0.0478072315454483,-0.17979399859905243,0.2011519968509674,-0.1551758348941803,-0.11175694316625595,-0.002209410071372986,-0.000913083553314209,-0.04358457773923874,0.15865333378314972,0.11228848993778229,0.1778639405965805,-0.023994237184524536,0.11112606525421143,-0.04363858699798584,-0.04694448411464691,0.04867696762084961,0.046840980648994446,0.1763080209493637,-0.0782003104686737,0.12265578657388687,0.03657969832420349,0.11157289147377014,-0.003002554178237915,-0.11131463944911957,0.14895404875278473,0.19334442913532257,0.038186222314834595,0.15133696794509888,-0.1485816091299057,0.1912074238061905,0.021724343299865723,0.16432690620422363,-0.002904638648033142,0.11777162551879883,0.1469404548406601,0.14612071216106415,0.2011519968509674,0.029864341020584106,-0.03188998997211456,0.1261919140815735,0.2011519968509674,-0.034725114703178406,0.15044233202934265,0.1932976394891739,0.11038359999656677,-0.16886194050312042,-0.03283897042274475,0.010655492544174194,-0.17979399859905243,0.1522967666387558,-0.11614429205656052,-0.06230756640434265,-0.0923634022474289,-0.01128303911536932,-0.1553298532962799,0.02708674967288971,0.15115532279014587,-0.046204451471567154,0.1999775916337967,-0.14609621465206146,-0.17662106454372406,0.18856218457221985,-0.03312675654888153,-0.15605461597442627,-0.1516750603914261,0.16356953978538513,0.019020527601242065,-0.15854069590568542,-0.04010162129998207,-0.17050516605377197,-0.00272330641746521,0.2011519968509674,-0.1682242453098297,0.2011519968509674,-0.14215555787086487,-0.11784959584474564,-0.02252475917339325,0.17891567945480347,-0.1466255784034729,-0.08122950047254562,0.11759083718061447,0.16777971386909485,0.15187761187553406,0.08006374537944794,-0.170836940407753,-0.13920439779758453,0.10669568926095963,0.11884043365716934,-0.12134065479040146,-0.16008628904819489,0.1504431664943695,0.2011519968509674,-0.17979399859905243,-0.1112772524356842,0.01741468906402588,0.1763080209493637,-0.15528230369091034,-0.040099844336509705,0.14262625575065613,0.10682142525911331,0.18908622860908508,0.13592445850372314,0.03613342344760895,-0.03389686346054077,0.07407160103321075,0.2011519968509674,0.14080862700939178,0.11239942163228989,0.15930652618408203,0.047730132937431335,0.17508536577224731,-0.034225016832351685,0.12320064753293991,-0.16130511462688446,-0.058633625507354736,-0.15494999289512634,0.2011519968509674,-0.17979399859905243,-0.15881113708019257,-0.004210011102259159,0.038829952478408813,0.06629504263401031,0.2011519968509674,-0.09849520772695541,0.08071830868721008,0.06831507384777069,-0.17542314529418945,0.10860319435596466,0.15799127519130707,-0.17466750741004944,-0.032563626766204834,-0.13749836385250092,-0.013591007329523563,-0.09978055953979492,0.18239839375019073,0.025824248790740967,0.0963430181145668,0.009128376841545105,-0.1487373560667038,0.1867062747478485,0.13361865282058716,-0.16351360082626343,-0.0546199232339859,0.14198851585388184,0.018514886498451233,-0.12354195863008499,-0.15910769999027252,-0.05695700645446777,-0.137518972158432,-0.17979399859905243,0.08317694067955017,0.15172907710075378,0.08938965946435928,0.1586044281721115,-0.008510365150868893,-0.17979399859905243,-0.05953868478536606,-0.15625925362110138,0.1576504409313202,0.047682493925094604,0.08291768282651901,-0.04505762457847595,0.1499265879392624,0.054933249950408936,-0.0026418715715408325,0.02591957151889801,-0.12764734029769897,0.12125831097364426,0.05060921981930733,0.10779402405023575,-0.04144999384880066,-0.11459270864725113,0.1376575529575348,0.16042014956474304,-0.10820003598928452,0.18761470913887024,-0.1351909339427948,-0.0015438944101333618,0.2011519968509674,0.2011519968509674,0.1629861295223236,0.06939011812210083,-0.07190845161676407,0.19628968834877014,0.010817959904670715,-0.15092772245407104,-0.07633428275585175,0.1699652075767517,-0.13720694184303284,-0.16596487164497375,0.05271058902144432,-0.16448815166950226,0.2011519968509674,-0.10967671871185303,0.09614580869674683,0.18814799189567566,0.10584872215986252,-0.17979399859905243,-0.11229771375656128,-0.08539125323295593,0.1763080209493637,-0.10004998743534088,-0.17979399859905243,-0.06032542884349823,0.18135492503643036,0.05038927495479584,-0.12790778279304504,-0.17979399859905243,0.020046472549438477,0.18256452679634094,-0.13828566670417786,0.10005023330450058,-0.09361694753170013,0.18033528327941895,0.08567319810390472,0.06058122217655182,0.18979677557945251,-0.013406486250460148,-0.11802247911691666,0.07851844280958176,-0.003140002489089966,-0.16018538177013397,0.05795653164386749,0.16158084571361542,0.1401691734790802,0.09857255220413208,0.1402531862258911,-0.1721741110086441,0.11867831647396088,0.02514927089214325,0.0401364266872406,0.1698068380355835,0.18990807235240936,-0.02678944543004036,0.19392520189285278,0.16873566806316376,0.2011519968509674,0.10855387151241302,-0.0064597586169838905,-0.15494999289512634,-0.16258858144283295,-0.0833522379398346,-0.004891262389719486,-0.1440797597169876,0.13971441984176636,0.1955256164073944,0.0710141509771347,-0.17979399859905243,-0.17243997752666473,0.13853943347930908,-0.17979399859905243,0.1763080209493637,-0.17979399859905243,-0.020584508776664734,0.17756325006484985,-0.17140159010887146,0.09691133350133896,0.2011519968509674,0.20046377182006836,0.055111706256866455,-0.00010284781455993652,0.12045051157474518,0.17642894387245178,-0.15494999289512634,0.08347323536872864,0.025984138250350952,0.07800312340259552,0.16532374918460846,-0.16737796366214752,-0.00815057847648859,-0.05084750056266785,0.059364259243011475,0.20105843245983124,0.06386353820562363,-0.0387263149023056,0.038904815912246704,0.1299217790365219,0.13727933168411255,-0.051201485097408295,-0.12097471207380295,0.023068219423294067,0.14558830857276917,-0.15494999289512634,0.07972961664199829,0.05274173617362976,0.030545424669981003,-0.053851090371608734,0.08820749819278717,-0.020358949899673462,-0.12675639986991882,0.11837245523929596,-0.15638504922389984,-0.03540301322937012,0.13039036095142365,0.019010096788406372,-0.15494999289512634,-0.08966086059808731,0.07524152100086212,-0.02366785705089569,0.12392447888851166,-0.032779961824417114,0.05879233777523041,-0.12595143914222717,-0.17723841965198517,0.1499314308166504,-0.07500571012496948,0.01860158145427704,-0.11759371310472488,0.09770531952381134,0.2011519968509674,-0.17979399859905243,-0.17979399859905243,-0.036356955766677856,0.2011519968509674,0.06792914867401123,0.1763080209493637,0.03284814953804016,0.1397106796503067,-0.16119010746479034,0.18825672566890717,-0.1752384752035141,0.023367062211036682,-0.15494999289512634,-0.17679280042648315,-0.052621349692344666,0.10668183863162994,-0.02266041934490204,0.1560894250869751,-0.10833527892827988,0.09990303218364716,0.11901170015335083,0.17783759534358978,-0.052077099680900574,0.02182045578956604,0.1625250279903412,-0.0002636164426803589,-0.06922392547130585,-0.17979399859905243,0.0005037933588027954,0.2011519968509674,-0.07044249773025513,-0.10000655800104141,0.00573749840259552,-0.17065313458442688,-0.17979399859905243,0.1763080209493637,0.016651973128318787,-0.14904385805130005,0.18819808959960938,-0.10401448607444763,-0.080742746591568,-0.08950374275445938,-0.1787937730550766,0.04446989297866821,0.1763080209493637,0.09513989835977554,0.11463620513677597,-0.09186314791440964,0.11880946159362793,-0.10474349558353424,-0.15933364629745483,-0.03485347330570221,0.18842469155788422,0.09573167562484741,-0.006253109313547611,-0.13361825048923492,0.2011519968509674,0.03057132661342621,-0.048672616481781006,0.15501253306865692,-0.16005802154541016,0.14451633393764496,0.2011519968509674,0.0020430535078048706,-0.000988110899925232,0.16577009856700897,-0.08864079415798187,0.1103319302201271,-0.17979399859905243,0.15265408158302307,0.034695982933044434,0.12135110795497894],\"y\":[-0.07329399883747101,0.37676793336868286,-0.0499541237950325,-0.04844899848103523,-0.04844899848103523,-0.3945842981338501,-0.009412553161382675,0.08668405562639236,-0.07329399883747101,-0.18982984125614166,-0.04844899848103523,0.059593565762043,0.24091926217079163,0.07890152931213379,-0.04844899848103523,-0.3733571469783783,-0.005179000087082386,-0.04844899848103523,0.17310868203639984,-0.005179000087082386,0.027506254613399506,-0.005179000087082386,-0.3164530396461487,-0.04844899848103523,0.3932967185974121,-0.005179000087082386,-0.05625782534480095,-0.005179000087082386,0.2661120891571045,0.3987489938735962,0.21642965078353882,-0.023120936006307602,-0.04844899848103523,-0.005179000087082386,-0.03719208762049675,-0.07329399883747101,0.11694404482841492,-0.07329399883747101,-0.04844899848103523,-0.04509548470377922,-0.011807594448328018,-0.024770697578787804,-0.07329399883747101,0.257264107465744,-0.04844899848103523,-0.04844899848103523,0.20044894516468048,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,0.3987489938735962,-0.047050029039382935,-0.04844899848103523,-0.19638216495513916,-0.07329399883747101,-0.07329399883747101,-0.24697141349315643,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,0.35724881291389465,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.07070569694042206,-0.005179000087082386,-0.005179000087082386,0.08233431726694107,0.19945643842220306,0.14976905286312103,-0.13778603076934814,-0.005179000087082386,-0.04384758695960045,-0.04844899848103523,0.39011913537979126,-0.04844899848103523,-0.07243502885103226,0.38541409373283386,-0.04844899848103523,-0.06628210842609406,-0.04844899848103523,-0.021283544600009918,-0.04844899848103523,0.05881329998373985,-0.04844899848103523,0.09785146266222,-0.016548441722989082,-0.04844899848103523,0.06182852014899254,-0.07329399883747101,-0.005179000087082386,-0.04722239822149277,-0.04844899848103523,-0.04844899848103523,0.2666175961494446,0.13200171291828156,-0.047396112233400345,-0.04844899848103523,-0.07329399883747101,0.15607722103595734,-0.16694970428943634,-0.04844899848103523,0.27699708938598633,0.1928027719259262,-0.07329399883747101,-0.07329399883747101,0.2639167904853821,0.00723576545715332,-0.09917008876800537,-0.04844899848103523,-0.017261864617466927,-0.03890630230307579,0.10663904249668121,0.2727258503437042,0.29826080799102783,-0.16580909490585327,-0.04844899848103523,-0.3585984706878662,-0.005179000087082386,0.11029227077960968,0.251569002866745,-0.005179000087082386,-0.005179000087082386,0.14444638788700104,-0.22963547706604004,-0.005179000087082386,0.32414236664772034,-0.005179000087082386,-0.005179000087082386,-0.07329399883747101,0.35729822516441345,0.10911470651626587,-0.04844899848103523,-0.07329399883747101,-0.22895611822605133,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,-0.36029309034347534,0.2542707920074463,-0.04844899848103523,0.02116573043167591,-0.04844899848103523,-0.013637960888445377,-0.07329399883747101,0.017800606787204742,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,0.02836063876748085,-0.07329399883747101,-0.3208160996437073,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,0.1423218995332718,-0.07329399883747101,-0.2688847780227661,-0.36762428283691406,-0.3000117838382721,-0.014460424892604351,-0.04844899848103523,-0.015555213205516338,-0.005179000087082386,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,0.22698597609996796,-0.005179000087082386,-0.005179000087082386,-0.04844899848103523,0.14194653928279877,-0.04844899848103523,-0.04844899848103523,-0.045966699719429016,-0.025397371500730515,0.24778695404529572,-0.04844899848103523,-0.0074227601289749146,-0.04844899848103523,0.19117994606494904,0.3987489938735962,0.21493831276893616,-0.04844899848103523,0.025282416492700577,-0.3962700068950653,-0.04844899848103523,-0.005179000087082386,0.25849446654319763,-0.07329399883747101,0.2138584703207016,-0.04178507253527641,-0.07329399883747101,-0.04844899848103523,0.2765780985355377,-0.04844899848103523,-0.17515414953231812,-0.07329399883747101,-0.005179000087082386,0.3980385959148407,-0.39534857869148254,-0.005179000087082386,-0.04844899848103523,0.26268312335014343,-0.005179000087082386,-0.07329399883747101,0.001796480268239975,-0.16265568137168884,0.03167569637298584,-0.07329399883747101,-0.2893311679363251,-0.07329399883747101,0.047171805053949356,-0.3310816287994385,-0.04844899848103523,0.19911424815654755,-0.1530335247516632,0.3359217941761017,-0.07329399883747101,0.35764405131340027,-0.04844899848103523,0.073701411485672,-0.011087089776992798,0.009486177936196327,-0.07329399883747101,-0.02597484365105629,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.03573347255587578,0.03364724665880203,-0.07329399883747101,-0.06152811273932457,-0.0383969321846962,-0.04844899848103523,0.3733428418636322,-0.13117069005966187,0.20752498507499695,0.14231915771961212,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,0.2845367193222046,0.22539252042770386,0.09291646629571915,0.3882763385772705,0.033328261226415634,0.2495587170124054,-0.07329399883747101,-0.04844899848103523,0.26709747314453125,-0.07329399883747101,-0.005179000087082386,-0.07329399883747101,0.027661606669425964,-0.14561960101127625,-0.04690641909837723,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,0.21492457389831543,-0.04822804033756256,0.31870004534721375,-0.04723839834332466,-0.07329399883747101,0.3987489938735962,-0.014437277801334858,-0.07841518521308899,-0.04844899848103523,-0.15799321234226227,-0.005179000087082386,-0.07329399883747101,0.3290383815765381,0.17648160457611084,-0.34755411744117737,-0.04844899848103523,-0.04844899848103523,-0.37326204776763916,-0.10909163951873779,-0.19020475447177887,-0.005179000087082386,0.2957611382007599,0.3987489938735962,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,0.3938709497451782,-0.035460107028484344,-0.04844899848103523,-0.034992240369319916,-0.005984891206026077,-0.04844899848103523,0.2471449375152588,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.10049942880868912,-0.07329399883747101,-0.04844899848103523,0.31047946214675903,0.0690956562757492,-0.07329399883747101,-0.01583065092563629,-0.04844899848103523,-0.06034150719642639,0.2781311571598053,-0.04224800691008568,-0.005179000087082386,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.005179000087082386,-0.005179000087082386,-0.05030207708477974,-0.005179000087082386,-0.005179000087082386,0.16636626422405243,-0.005179000087082386,-0.19242297112941742,-0.005179000087082386,-0.04844899848103523,0.1570734828710556,-0.05804791674017906,-0.05796496570110321,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.028146401047706604,0.0635489895939827,-0.017073817551136017,-0.03769636154174805,0.2606346905231476,-0.07329399883747101,-0.04844899848103523,0.08908897638320923,-0.005179000087082386,-0.11259046196937561,0.09083538502454758,-0.22879542410373688,-0.02971458062529564,-0.011717632412910461,-0.07329399883747101,-0.07329399883747101,-0.01982913725078106,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.19530673325061798,0.042552970349788666,-0.07329399883747101,-0.04844899848103523,-0.12745186686515808,0.1655053049325943,-0.2632930278778076,-0.008282830007374287,-0.009795702993869781,-0.07329399883747101,-0.04844899848103523,-0.09243863821029663,-0.24490900337696075,-0.07329399883747101,-0.07329399883747101,-0.052124764770269394,-0.04844899848103523,0.06095393747091293,-0.007958034053444862,-0.06254078447818756,-0.03275959566235542,-0.005179000087082386,0.0062831100076437,0.01222375500947237,-0.005179000087082386,-0.26701727509498596,-0.005179000087082386,-0.07329399883747101,-0.00562368705868721,-0.005179000087082386,-0.04844899848103523,-0.005179000087082386,-0.013794577680528164,-0.07329399883747101,-0.39263495802879333,0.3987489938735962,0.18553444743156433,0.22443482279777527,-0.04844899848103523,0.1260577142238617,-0.005179000087082386,0.13526205718517303,0.04086810722947121,-0.005179000087082386,-0.3461979031562805,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,0.3751649558544159,-0.005179000087082386,0.2077724039554596,0.3420273959636688,0.029184220358729362,-0.2339969277381897,-0.21725599467754364,-0.107004314661026,0.25375014543533325,-0.04844899848103523,0.06710391491651535,-0.05656708776950836,-0.005179000087082386,-0.04482543095946312,0.19725070893764496,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.022237451747059822,-0.001943943090736866,-0.04844899848103523,-0.07329399883747101,0.1650664359331131,-0.04844899848103523,0.3671361207962036,-0.005179000087082386,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,0.30202189087867737,-0.005179000087082386,-0.008751839399337769,-0.07329399883747101,0.02633960172533989,-0.07329399883747101,-0.02127918228507042,-0.07329399883747101,0.03655223920941353,-0.005179000087082386,-0.01729319430887699,-0.005761798471212387,-0.005179000087082386,-0.14280767738819122,-0.04844899848103523,-0.07329399883747101,0.169422447681427,-0.04844899848103523,-0.04844899848103523,0.28058725595474243,-0.005179000087082386,0.3437938094139099,-0.005179000087082386,0.079644113779068,0.26119282841682434,-0.005179000087082386,-0.04844899848103523,-0.029529739171266556,-0.04844899848103523,0.057685915380716324,0.3512468934059143,0.0013089701533317566,-0.005179000087082386,0.14795440435409546,-0.07329399883747101,-0.07329399883747101,0.3523957133293152,-0.17439506947994232,-0.005179000087082386,-0.07329399883747101,-0.005179000087082386,-0.005890201777219772,-0.07329399883747101,-0.07329399883747101,0.20603981614112854,-0.07329399883747101,-0.04476061463356018,0.062017329037189484,-0.04844899848103523,0.06278473138809204,-0.059873972088098526,-0.009956598281860352,-0.0065300799906253815,0.22297610342502594,-0.07329399883747101,-0.08723573386669159,-0.06264788657426834,-0.04844899848103523,0.2423761934041977,-0.02868068963289261,-0.07329399883747101,-0.07329399883747101,0.39388132095336914,-0.04844899848103523,-0.011212116107344627,-0.07329399883747101,-0.07329399883747101,-0.34861016273498535,-0.04844899848103523,-0.04844899848103523,-0.20240847766399384,-0.005179000087082386,-0.16437757015228271,-0.04844899848103523,-0.17023822665214539,-0.005179000087082386,0.18639886379241943,-0.005179000087082386,-0.016726799309253693,0.21982429921627045,-0.005179000087082386,-0.005179000087082386,0.3193047344684601,-0.005179000087082386,-0.04844899848103523,0.08757780492305756,-0.04844899848103523,0.16748656332492828,0.23331066966056824,-0.04844899848103523,-0.07329399883747101,0.3369549810886383,0.029241211712360382,-0.044169045984745026,-0.039334625005722046,-0.04395754262804985,-0.12006811797618866,-0.07329399883747101,-0.005179000087082386,-0.005179000087082386,-0.23345795273780823,-0.04844899848103523,0.10019750148057938,-0.07329399883747101,0.1968572586774826,-0.04844899848103523,0.3987489938735962,0.10117421299219131,0.05678180605173111,-0.005179000087082386,-0.020741313695907593,-0.18815059959888458,-0.043121252208948135,-0.02160729095339775,-0.3468754291534424,0.38726383447647095,-0.04344082251191139,-0.20971567928791046,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,-0.054810069501399994,0.19651398062705994,-0.07329399883747101,-0.07329399883747101,-0.005179000087082386,-0.12309947609901428,-0.07329399883747101,-0.005179000087082386,-0.0662773996591568,-0.005179000087082386,-0.005179000087082386,0.29661619663238525,0.1533866822719574,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,0.3777291476726532,-0.04844899848103523,-0.160983607172966,0.35844162106513977,0.15685729682445526,-0.04844899848103523,-0.005179000087082386,-0.007913264445960522,-0.07329399883747101,-0.04844899848103523,0.28689199686050415,-0.04844899848103523,0.01623597927391529,0.3624909520149231,-0.04844899848103523,-0.3244069218635559,-0.058213457465171814,-0.04844899848103523,-0.0525350384414196,0.29456016421318054,-0.005179000087082386,0.17244701087474823,-0.07329399883747101,-0.04844899848103523,0.09424728155136108,-0.07329399883747101,-0.005179000087082386,-0.005179000087082386,-0.07329399883747101,-0.04190653935074806,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.07797358930110931,-0.04844899848103523,-0.005179000087082386,0.3987489938735962,-0.04844899848103523,-0.04844899848103523,0.3987489938735962,-0.01240830309689045,-0.005179000087082386,0.38002800941467285,0.3747580647468567,0.13937941193580627,-0.005179000087082386,0.18721038103103638,-0.2604394257068634,0.12786854803562164,0.10164418071508408,-0.04844899848103523,-0.07329399883747101,0.16575534641742706,-0.07329399883747101,-0.20056812465190887,-0.005179000087082386,-0.04844899848103523,0.3277062177658081,-0.19675526022911072,0.1452999860048294,-0.056504055857658386,-0.005179000087082386,-0.026444070041179657,-0.17144352197647095,-0.005179000087082386,-0.04844899848103523,0.3987489938735962,-0.07329399883747101,-0.038715068250894547,-0.04844899848103523,-0.04844899848103523,-0.22003211081027985,-0.04844899848103523,0.2520360052585602,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.29398131370544434,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,0.30195075273513794,-0.22828511893749237,-0.04844899848103523,-0.329698771238327,0.2860904335975647,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,0.2514543831348419,0.19639228284358978,-0.005179000087082386,-0.04844899848103523,0.30264878273010254,-0.005179000087082386,-0.36872759461402893,-0.3770368993282318,-0.005179000087082386,-0.04844899848103523,0.02737616002559662,-0.04844899848103523,0.18402424454689026,0.32057252526283264,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,0.09575734287500381,0.3987489938735962,-0.1562531441450119,-0.0021107159554958344,-0.005179000087082386,-0.005179000087082386,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,0.2800518274307251,-0.04844899848103523,-0.07329399883747101,0.3891821801662445,0.1711752712726593,-0.04844899848103523,0.2585964798927307,-0.06739400327205658,0.34322941303253174,-0.04844899848103523,0.1158197894692421,-0.005179000087082386,-0.005179000087082386,0.3675113618373871,0.27036648988723755,0.31158939003944397,-0.04844899848103523,-0.049447689205408096,-0.005179000087082386,0.3268960118293762,-0.005179000087082386,-0.07743395864963531,-0.206334188580513,-0.03499516099691391,0.3034322261810303,-0.04844899848103523,0.3987489938735962,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.014587252400815487,-0.04844899848103523,-0.02377263829112053,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.005179000087082386,0.39321011304855347,-0.005179000087082386,-0.30503663420677185,-0.366051584482193,-0.005179000087082386,-0.039024241268634796,-0.04844899848103523,0.018575269728899002,-0.07329399883747101,0.22214749455451965,-0.005179000087082386,-0.33744269609451294,-0.01924407109618187,0.35074692964553833,0.3042791187763214,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.15949507057666779,-0.005179000087082386,-0.06227348372340202,-0.04844899848103523,-0.08827030658721924,-0.006668531335890293,0.31546086072921753,-0.04844899848103523,0.09808967262506485,-0.056487906724214554,-0.005179000087082386,0.03598616272211075,-0.05482519418001175,0.3128811717033386,0.1359139084815979,-0.04844899848103523,-0.07329399883747101,0.13119100034236908,-0.04844899848103523,0.26014938950538635,-0.04077484458684921,-0.07329399883747101,-0.01380757987499237,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.005179000087082386,-0.005179000087082386,0.37254998087882996,-0.032329339534044266,-0.06165507063269615,-0.04539690166711807,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,0.23529264330863953,-0.0704936608672142,0.3987489938735962,-0.07329399883747101,-0.04844899848103523,-0.005179000087082386,0.2853248119354248,-0.22445692121982574,0.3484730124473572,-0.09228479862213135,0.11935225129127502,-0.005179000087082386,0.009057577699422836,-0.3374454379081726,-0.04844899848103523,-0.2805691063404083,-0.2268107682466507,-0.024879461154341698,-0.005179000087082386,-0.1402989774942398,-0.008191223256289959,-0.04844899848103523,-0.005179000087082386,0.3096810281276703,0.28073596954345703,0.1439880132675171,-0.26371344923973083,0.33934012055397034,0.03450632095336914,0.09706234186887741,-0.04844899848103523,0.34913796186447144,0.04519001394510269,0.18126216530799866,-0.005179000087082386,-0.2512587010860443,0.24084021151065826,-0.0743393748998642,-0.07329399883747101,0.24315793812274933,-0.04844899848103523,0.3095249533653259,0.04167648404836655,-0.14451190829277039,0.13227589428424835,-0.07329399883747101,0.14767064154148102,-0.04844899848103523,-0.026844970881938934,0.3987489938735962,-0.07329399883747101,0.36536139249801636,-0.01564246229827404,-0.04844899848103523,-0.07329399883747101,0.08643806725740433,-0.04844899848103523,-0.007390775717794895,-0.23661209642887115,-0.04844899848103523,0.03892695903778076,-0.09206333756446838,-0.021358203142881393,-0.27977412939071655,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,-0.005179000087082386,0.19586658477783203,0.38106387853622437,-0.04844899848103523,-0.0558365099132061,0.3631073534488678,-0.07329399883747101,-0.04844899848103523,0.1041017696261406,-0.07329399883747101,0.37991422414779663,-0.07329399883747101,-0.01021117065101862,-0.005179000087082386,-0.016374217346310616,-0.07329399883747101,0.08264829218387604,-0.3376716673374176,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.3551381826400757,-0.07329399883747101,-0.029337216168642044,0.3987489938735962,-0.04844899848103523,-0.04844899848103523,0.1924315243959427,-0.3311896324157715,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.2114461064338684,0.3810247778892517,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.07329399883747101,-0.35817766189575195,0.053478069603443146,-0.07329399883747101,0.0835733562707901,-0.010593350045382977,-0.005179000087082386,0.3024773597717285,-0.19986961781978607,0.24503226578235626,-0.07329399883747101,-0.07329399883747101,0.04910249263048172,-0.33654510974884033,0.33018937706947327,-0.005179000087082386,0.2063608020544052,0.3987489938735962,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.29247018694877625,0.25106894969940186,-0.02464987337589264,-0.04923813417553902,-0.07329399883747101,-0.03650299832224846,0.006939515471458435,0.10471165925264359,0.17779822647571564,0.06610898673534393,-0.05577271431684494,0.22595037519931793,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.3421357274055481,-0.06698069721460342,-0.005179000087082386,-0.09394712001085281,-0.3694022297859192,-0.005179000087082386,-0.20996949076652527,-0.07329399883747101,-0.05821971595287323,-0.047517191618680954,0.048312608152627945,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.06120549142360687,-0.040482841432094574,0.3368303179740906,-0.005179000087082386,-0.027470042929053307,-0.042505595833063126,0.10369448363780975,-0.2505231499671936,-0.005179000087082386,-0.005179000087082386,0.2727822959423065,0.37611010670661926,0.18189051747322083,-0.0653693675994873,0.26479971408843994,-0.26359105110168457,-0.005179000087082386,-0.3705191910266876,-0.04844899848103523,0.36213651299476624,-0.07329399883747101,0.2119949758052826,0.14696447551250458,-0.13985700905323029,-0.3962700068950653,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.3649349808692932,-0.04844899848103523,-0.040287915617227554,-0.07329399883747101,-0.04844899848103523,0.20517843961715698,0.23645024001598358,-0.05972777307033539,-0.04844899848103523,-0.17128491401672363,-0.029537001624703407,-0.04844899848103523,-0.05882459133863449,0.19363531470298767,0.3987489938735962,-0.07329399883747101,-0.005179000087082386,-0.005179000087082386,0.3705047070980072,-0.04844899848103523,-0.038853418081998825,-0.20991334319114685,-0.04844899848103523,0.06598226726055145,-0.005179000087082386,-0.00939762033522129,-0.07329399883747101,0.18576878309249878,-0.0393955260515213,-0.07329399883747101,0.1316903680562973,0.3987489938735962,0.3735032081604004,0.21125319600105286,0.3558596968650818,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.010803759098052979,-0.005179000087082386,-0.04844899848103523,-0.14172938466072083,0.31559446454048157,0.3788232207298279,-0.07329399883747101,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,0.28850239515304565,-0.07171305269002914,0.3322284519672394,0.004656288772821426,-0.04844899848103523,-0.04844899848103523,-0.03675714135169983,-0.007630042731761932,-0.07329399883747101,-0.07329399883747101,-0.23538953065872192,-0.09110426157712936,-0.005179000087082386,0.04211011156439781,-0.005179000087082386,0.37706810235977173,-0.0635945051908493,-0.04844899848103523,-0.027263520285487175,-0.050581298768520355,0.158853679895401,-0.03503334894776344,0.12000012397766113,0.18462614715099335,-0.07329399883747101,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,0.13822630047798157,-0.23612158000469208,-0.005179000087082386,-0.022931581363081932,-0.005179000087082386,-0.07329399883747101,-0.07329399883747101,-0.03926501423120499,-0.04844899848103523,-0.28442737460136414,-0.17806969583034515,-0.005179000087082386,0.33901479840278625,0.03635662794113159,0.20048314332962036,0.09406115859746933,-0.04844899848103523,-0.015130504034459591,-0.04844899848103523,0.3987489938735962,0.2616967558860779,-0.04844899848103523,-0.04844899848103523,-0.02668256126344204,0.03023466467857361,-0.07094266265630722,0.2993718087673187,-0.04844899848103523,-0.07329399883747101,0.03267018124461174,0.16429945826530457,-0.04844899848103523,-0.07329399883747101,0.27811774611473083,-0.10053051263093948,0.07221625745296478,-0.051216430962085724,-0.12775561213493347,0.057728737592697144,-0.3924649953842163,-0.15713083744049072,-0.04844899848103523,-0.039542727172374725,-0.07329399883747101,-0.07329399883747101,0.05669784173369408,0.23153139650821686,-0.005179000087082386,0.2518356442451477,-0.005179000087082386,-0.07329399883747101,-0.07329399883747101,-0.09382149577140808,-0.005179000087082386,-0.04123197868466377,0.0890234038233757,-0.3080191910266876,-0.07329399883747101,-0.04844899848103523,-0.15370294451713562,-0.06932801753282547,-0.04844899848103523,-0.005179000087082386,0.06541843712329865,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.005179000087082386,0.3112289011478424,-0.04844899848103523,-0.1115746945142746,-0.38819050788879395,0.23357509076595306,-0.005179000087082386,-0.23441803455352783,-0.07329399883747101,-0.3815135955810547,-0.04844899848103523,-0.005179000087082386,-0.38857653737068176,0.054640237241983414,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.07217387855052948,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,0.2709055244922638,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.1965053230524063,-0.005179000087082386,-0.04844899848103523,0.2633490264415741,-0.07329399883747101,-0.04844899848103523,-0.040323562920093536,0.09679080545902252,-0.04413721337914467,0.30163878202438354,-0.39387279748916626,-0.07329399883747101,-0.005179000087082386,-0.030870091170072556,-0.04324471578001976,-0.04844899848103523,0.06765744835138321,-0.07329399883747101,-0.04844899848103523,0.2383970469236374,-0.005179000087082386,-0.04844899848103523,0.02431192807853222,-0.04844899848103523,-0.3891095519065857,0.002534959465265274,0.36790937185287476,-0.04844899848103523,0.09435926377773285,0.21943996846675873,-0.07329399883747101,-0.07329399883747101,-0.2801690399646759,-0.02928795851767063,-0.11336947977542877,-0.14258569478988647,-0.04844899848103523,-0.06248385086655617,-0.06585133820772171,0.011524670757353306,-0.04844899848103523,-0.04844899848103523,0.13781434297561646,-0.04844899848103523,-0.07329399883747101,-0.005179000087082386,0.3369591236114502,-0.07329399883747101,0.18004979193210602,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.35181257128715515,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,0.2544137239456177,-0.38243502378463745,-0.04844899848103523,-0.012379520572721958,-0.04844899848103523,-0.11089196801185608,0.28624415397644043,-0.005179000087082386,-0.04844899848103523,0.13120315968990326,0.29859957098960876,0.016222216188907623,0.3720546066761017,0.30566513538360596,-0.07329399883747101,-0.07329399883747101,-0.2258310616016388,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,0.12876011431217194,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.0070701707154512405,-0.005179000087082386,-0.29865026473999023,-0.04844899848103523,-0.005179000087082386,0.15957121551036835,-0.01876058615744114,-0.3132033944129944,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.012456988915801048,0.2514152228832245,0.34947746992111206,-0.07329399883747101,-0.38607853651046753,-0.1986657977104187,-0.005179000087082386,-0.02174423635005951,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.2576703429222107,0.1936221867799759,-0.14570105075836182,-0.2123841792345047,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,0.10461138188838959,-0.04844899848103523,0.3878934979438782,-0.04844899848103523,-0.1434231400489807,-0.04844899848103523,-0.07329399883747101,-0.3962700068950653,0.023735452443361282,-0.04844899848103523,-0.07329399883747101,-0.005179000087082386,-0.04844899848103523,-0.01150853093713522,-0.04844899848103523,-0.005179000087082386,0.14751048386096954,-0.07329399883747101,0.032583389431238174,-0.04844899848103523,0.12024210393428802,-0.033675044775009155,0.17029598355293274,-0.04561016708612442,-0.04844899848103523,0.27420249581336975,-0.04844899848103523,-0.04844899848103523,0.05596320703625679,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,0.3987489938735962,-0.07329399883747101,-0.04844899848103523,-0.16646213829517365,0.13316220045089722,-0.07329399883747101,-0.005179000087082386,-0.005179000087082386,-0.07329399883747101,0.31986650824546814,-0.005179000087082386,-0.16026483476161957,-0.04844899848103523,-0.04844899848103523,-0.3381352424621582,-0.04844899848103523,-0.07329399883747101,-0.005179000087082386,-0.043296393007040024,-0.005179000087082386,0.14905007183551788,0.17202821373939514,-0.07329399883747101,-0.04555338993668556,-0.04844899848103523,0.0968506932258606,-0.2218540608882904,-0.10691764950752258,-0.2491208016872406,0.005301712080836296,0.07485988736152649,0.09351032972335815,-0.11665791273117065,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,-0.005179000087082386,-0.04844899848103523,0.17494863271713257,-0.01821579411625862,-0.23855790495872498,-0.04844899848103523,-0.052367355674505234,-0.005179000087082386,0.21401062607765198,-0.29397356510162354,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,0.14180448651313782,-0.04844899848103523,-0.36110326647758484,-0.07329399883747101,0.1796395182609558,-0.04163654148578644,-0.07329399883747101,0.12871971726417542,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,0.2398766130208969,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.07329399883747101,0.1979846954345703,-0.18586385250091553,-0.046442557126283646,-0.04844899848103523,0.1339946836233139,0.3691495656967163,-0.005179000087082386,-0.005179000087082386,-0.07329399883747101,-0.2448500096797943,-0.07136188447475433,0.040653228759765625,-0.26260077953338623,-0.07329399883747101,-0.07329399883747101,-0.026063960045576096,0.3808397054672241,0.16264939308166504,-0.005179000087082386,-0.16728462278842926,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,-0.35937243700027466,-0.04844899848103523,0.37773075699806213,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.005179000087082386,0.3475046455860138,-0.07329399883747101,0.3637349307537079,-0.04844899848103523,-0.07329399883747101,-0.042165741324424744,-0.005179000087082386,-0.01776748336851597,-0.04844899848103523,0.19989359378814697,-0.005179000087082386,0.14172136783599854,-0.3541822135448456,0.28887417912483215,-0.04872242733836174,-0.007384654134511948,0.06411236524581909,-0.07329399883747101,-0.08088281005620956,-0.04844899848103523,0.20854707062244415,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,0.3930254280567169,0.307540625333786,-0.07329399883747101,-0.3086988627910614,0.27697914838790894,-0.04426784813404083,-0.07329399883747101,-0.07329399883747101,0.06536053121089935,-0.005179000087082386,0.21965572237968445,-0.07329399883747101,-0.07329399883747101,0.0013379119336605072,-0.07329399883747101,-0.07329399883747101,-0.005179000087082386,-0.05949084833264351,0.21795281767845154,-0.07329399883747101,-0.19200488924980164,-0.36534398794174194,-0.17700526118278503,0.19678619503974915,-0.3096720278263092,-0.04844899848103523,0.2746134102344513,-0.04844899848103523,-0.03313294053077698,-0.01230074092745781,-0.26385900378227234,-0.07329399883747101,0.39724472165107727,0.020557215437293053,-0.002738594776019454,0.29855024814605713,-0.05708795040845871,-0.07329399883747101,0.14217516779899597,-0.04844899848103523,0.2988540232181549,-0.07329399883747101,0.04092668369412422,-0.04844899848103523,-0.3962700068950653,-0.0090104965493083,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.043168891221284866,-0.07329399883747101,-0.24065354466438293,-0.04844899848103523,-0.04442262649536133,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.02394060604274273,-0.09988240152597427,-0.07329399883747101,-0.0038119012024253607,-0.057168520987033844,-0.04844899848103523,-0.005635753273963928,0.23643144965171814,-0.18264375627040863,-0.07329399883747101,-0.06514066457748413,0.10666147619485855,-0.11873242259025574,-0.005179000087082386,-0.008912759833037853,0.3413744270801544,-0.012160137295722961,-0.07329399883747101,-0.28456854820251465,-0.005179000087082386,-0.04844899848103523,0.2873341143131256,-0.04844899848103523,0.38583657145500183,-0.3497913181781769,-0.3214395344257355,-0.018327917903661728,0.10201157629489899,-0.07329399883747101,0.2928358316421509,0.3825119137763977,-0.005179000087082386,0.28641849756240845,0.24997517466545105,-0.020840687677264214,-0.005744047462940216,-0.022369077429175377,-0.04844899848103523,-0.005179000087082386,0.04144877567887306,0.23804186284542084,0.2987923324108124,-0.039170775562524796,0.07003481686115265,-0.054547060281038284,0.251025915145874,-0.07329399883747101,-0.007596243172883987,-0.039246559143066406,-0.005179000087082386,-0.07329399883747101,-0.07329399883747101,-0.3889172077178955,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,0.03540147840976715,0.37337782979011536,-0.04844899848103523,-0.04844899848103523,0.39311346411705017,-0.04844899848103523,-0.04844899848103523,0.2878302037715912,-0.036880310624837875,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.039765167981386185,0.05596724897623062,0.05233082175254822,-0.07305466383695602,0.3987489938735962,-0.04844899848103523,-0.04844899848103523,0.29710498452186584,-0.07329399883747101,0.0785316675901413,-0.04844899848103523,0.36778759956359863,-0.027153588831424713,-0.005179000087082386,-0.04947016015648842,-0.015974804759025574,-0.005179000087082386,-0.04844899848103523,-0.030145321041345596,-0.04844899848103523,0.07192622870206833,-0.04606340080499649,-0.1783474087715149,-0.04844899848103523,0.20314455032348633,-0.005179000087082386,0.3684141933917999,-0.142909973859787,-0.17781955003738403,-0.07329399883747101,0.0911186933517456,0.054597318172454834,-0.2981905937194824,-0.07329399883747101,0.13312304019927979,-0.04844899848103523,0.1686376929283142,-0.04844899848103523,-0.37640631198883057,-0.04844899848103523,0.045889634639024734,-0.02759004943072796,0.20575925707817078,0.17689456045627594,-0.07329399883747101,-0.37499165534973145,-0.005179000087082386,-0.01682235114276409,-0.04844899848103523,-0.014543152414262295,-0.29312288761138916,0.2999405860900879,-0.0966227725148201,-0.07329399883747101,-0.02591482363641262,-0.005179000087082386,-0.04844899848103523,0.15606753528118134,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,0.1070307195186615,-0.04844899848103523,-0.35193753242492676,-0.005179000087082386,0.21004390716552734,-0.07329399883747101,-0.005179000087082386,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.15739184617996216,-0.13876035809516907,-0.390092134475708,-0.04844899848103523,-0.0070470720529556274,-0.027258044108748436,0.3347706198692322,-0.3391970992088318,-0.04844899848103523,-0.008441256359219551,0.2998269498348236,-0.005179000087082386,-0.2401532679796219,-0.04844899848103523,0.21008220314979553,0.04523472860455513,-0.04844899848103523,-0.005179000087082386,0.26804888248443604,-0.005179000087082386,-0.04331539198756218,0.15546198189258575,-0.07033205777406693,-0.005179000087082386,-0.04844899848103523,-0.034959156066179276,-0.07329399883747101,0.17647506296634674,-0.005179000087082386,-0.005179000087082386,-0.07329399883747101,-0.04844899848103523,-0.11172544956207275,-0.04844899848103523,-0.04844899848103523,-0.06034543737769127,-0.042136166244745255,-0.04844899848103523,-0.06971131265163422,-0.005179000087082386,0.3359549045562744,-0.042046621441841125,0.26639798283576965,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,0.3987489938735962,-0.25281471014022827,0.08568183332681656,-0.07399723678827286,0.23570771515369415,0.3378943204879761,-0.03783993050456047,-0.07329399883747101,-0.07329399883747101,-0.005179000087082386,0.3987489938735962,-0.005179000087082386,-0.044414177536964417,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,0.1479443907737732,-0.14412148296833038,-0.07329399883747101,0.28377100825309753,0.12282035499811172,-0.04842749983072281,-0.06732670217752457,-0.04844899848103523,0.3338022530078888,0.3240160644054413,-0.02189544215798378,0.3600166440010071,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,-0.03414559364318848,-0.04844899848103523,0.31062033772468567,-0.07329399883747101,-0.1915113925933838,-0.005179000087082386,0.12974414229393005,-0.04844899848103523,0.20812614262104034,-0.04844899848103523,0.28851839900016785,-0.04546796530485153,-0.04844899848103523,-0.16916555166244507,0.23857924342155457,-0.04844899848103523,-0.34161633253097534,0.16314157843589783,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.03742605447769165,-0.0016369875520467758,0.3774922788143158,-0.04844899848103523,0.09106002002954483,0.07061508297920227,-0.04844899848103523,0.18961544334888458,-0.01581120304763317,-0.04844899848103523,-0.04844899848103523,-0.06244098022580147,-0.07329399883747101,0.29740214347839355,-0.29393911361694336,0.19566865265369415,-0.041198812425136566,-0.04844899848103523,0.3668198585510254,-0.07329399883747101,-0.05280860513448715,0.3443414866924286,-0.04844899848103523,-0.04844899848103523,0.01527387648820877,-0.005179000087082386,-0.04844899848103523,0.0787101462483406,-0.04844899848103523,-0.04812011122703552,0.2614482045173645,-0.040667638182640076,-0.07329399883747101,-0.005179000087082386,0.19376136362552643,0.20364440977573395,-0.04844899848103523,-0.04844899848103523,-0.06762392073869705,-0.06266778707504272,-0.02024659886956215,-0.04844899848103523,-0.38216227293014526,-0.2913976013660431,0.08808756619691849,-0.029797717928886414,-0.07329399883747101,-0.005179000087082386,-0.04844899848103523,0.34211716055870056,0.09702782332897186,0.12545010447502136,0.36812230944633484,0.06414639204740524,-0.0446494035422802,-0.04844899848103523,0.20496295392513275,0.3822682499885559,-0.04844899848103523,-0.005179000087082386,-0.1814805120229721,-0.05543190985918045,-0.2261252999305725,0.24139675498008728,-0.15571489930152893,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.07329399883747101,-0.04868282377719879,-0.01277470588684082,-0.07329399883747101,-0.04844899848103523,-0.07793602347373962,0.0982900857925415,0.001689232885837555,-0.030436817556619644,-0.04844899848103523,-0.005179000087082386,-0.005179000087082386,-0.005179000087082386,0.16490131616592407,-0.011544905602931976,-0.04844899848103523,0.1163325235247612,-0.005179000087082386,-0.07329399883747101,-0.04844899848103523,-0.22883275151252747,0.3494592607021332,0.3769236207008362,0.3839392364025116,-0.005179000087082386,-0.04844899848103523,-0.07329399883747101,0.12982715666294098,-0.05922175198793411,0.3746407628059387,-0.0662815049290657,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.3748677372932434,0.09198160469532013,0.12306613475084305,-0.07329399883747101,0.34939444065093994,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,0.3454904854297638,-0.025296177715063095,-0.005179000087082386,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.04844899848103523,-0.07329399883747101,-0.07329399883747101,-0.07329399883747101,-0.04844899848103523,0.3004339337348938,0.11952073872089386,0.11028192192316055,-0.052680712193250656,0.3961975872516632,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.326662540435791,-0.04844899848103523,-0.03249034658074379,-0.07329399883747101,0.27630650997161865,0.035167500376701355,-0.21380825340747833,-0.0198067594319582,-0.07329399883747101,-0.054926104843616486,-0.07329399883747101,-0.005179000087082386,-0.37215352058410645,-0.07329399883747101,-0.051354438066482544,-0.07329399883747101,-0.005179000087082386,-0.07745306193828583,0.08659259229898453,-0.35893675684928894,-0.01565416529774666,-0.07329399883747101,0.2655479907989502,0.11796937882900238,-0.06798902899026871,0.339830219745636,-0.1156107485294342,-0.07329399883747101,0.31979161500930786,-0.005179000087082386,-0.2708306312561035,-0.07329399883747101,-0.07329399883747101,-0.3312106728553772,0.24887076020240784,-0.013974585570394993,0.3619352877140045,-0.07329399883747101,-0.23890841007232666,-0.07329399883747101,-0.04844899848103523,0.0042083971202373505,-0.04844899848103523,0.36907899379730225,-0.015235175378620625,-0.07329399883747101,-0.04844899848103523,0.01054229773581028,0.37113621830940247,-0.3909114599227905,-0.04369252175092697,0.20175129175186157,-0.04844899848103523,-0.26851987838745117,-0.04844899848103523,-0.005179000087082386,-0.2651081681251526,-0.005179000087082386,-0.07329399883747101,0.3987489938735962,0.050012893974781036,-0.04844899848103523,-0.3095192015171051,-0.07329399883747101,-0.3041568100452423,-0.03776707872748375,-0.005179000087082386,-0.21517176926136017,-0.25944823026657104,-0.31545698642730713,-0.005179000087082386,-0.005179000087082386,0.35434016585350037,0.01526364590972662,-0.25490039587020874,-0.07329399883747101,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,-0.22967436909675598,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.04515352472662926,-0.005179000087082386,0.37285202741622925,-0.005179000087082386,-0.025659509003162384,-0.005179000087082386,-0.04844899848103523,-0.005179000087082386,0.3593977689743042,0.2625499963760376,-0.005179000087082386,0.24645762145519257,0.15714307129383087,-0.07329399883747101,-0.31740906834602356,-0.005179000087082386,-0.04844899848103523,0.21393071115016937,-0.005179000087082386,0.19778117537498474,-0.010193354450166225,0.2553844153881073,0.3977062404155731,-0.04844899848103523,-0.04844899848103523,-0.04844899848103523,-0.07329399883747101,-0.12635520100593567,0.0010272394865751266,-0.01950518600642681,-0.07329399883747101,0.19212579727172852,-0.005179000087082386,-0.005179000087082386,0.03612000122666359,0.047070860862731934,-0.007032978348433971,-0.04844899848103523,0.3112921714782715,0.207861989736557,0.2254636436700821,-0.26827043294906616,0.057662710547447205,-0.26414042711257935,0.23318979144096375,-0.303188681602478,-0.005179000087082386,-0.36047983169555664,-0.010642639361321926,-0.04844899848103523,-0.13465625047683716,-0.1687069535255432,-0.04844899848103523,0.32566526532173157,-0.20964153110980988,0.05584005266427994,-0.04844899848103523,0.005314812064170837,-0.04844899848103523,-0.026558903977274895,-0.00830013770610094,-0.07329399883747101,-0.005179000087082386,-0.04844899848103523,-0.04844899848103523,-0.005179000087082386,0.10586239397525787,-0.07329399883747101,-0.005179000087082386,-0.3579912483692169,-0.07329399883747101,-0.10617360472679138,-0.07329399883747101,0.02371503785252571,-0.04844899848103523,0.3880462944507599,-0.387851357460022,-0.28596311807632446,0.06818928569555283,-0.07329399883747101,-0.3863348960876465,-0.07329399883747101,0.15014268457889557,0.16762760281562805,-0.25679224729537964,-0.07329399883747101,-0.1810452938079834,-0.031569771468639374,0.02875543013215065,0.2243906706571579,-0.07329399883747101,0.2441314309835434,-0.3962700068950653,-0.058964475989341736,-0.04844899848103523,-0.005179000087082386,-0.04844899848103523,0.18720616400241852,0.21520939469337463,-0.04844899848103523,0.08152853697538376,0.06948116421699524,-0.04844899848103523,-0.04844899848103523,0.36088070273399353,0.3146686255931854,0.1107943058013916,-0.005179000087082386,-0.07329399883747101,-0.005179000087082386,-0.06710340827703476,-0.0345468632876873,-0.04844899848103523,-0.04844899848103523],\"z\":[0.08195540308952332,0.14626091718673706,-0.025297507643699646,0.0505979023873806,-0.22710227966308594,0.11864499747753143,-0.2883520722389221,0.1268148571252823,-0.2862022817134857,0.10556745529174805,0.0032245218753814697,0.12450745701789856,0.1431291103363037,0.10118885338306427,-0.2532639801502228,-0.27058300375938416,-0.23536664247512817,0.004965454339981079,0.11114458739757538,0.04896625503897667,0.12221349775791168,-0.08671903610229492,-0.27058300375938416,-0.12679995596408844,0.17539964616298676,-0.23547309637069702,-0.15392258763313293,-0.09342561662197113,0.13785205781459808,0.1611810326576233,0.14213384687900543,0.09475763142108917,0.07831414043903351,-0.17109417915344238,0.09498502314090729,-0.18938127160072327,0.10475018620491028,-0.08142581582069397,0.03301169350743294,0.11877402663230896,0.09230376780033112,-0.2952544689178467,0.05578070506453514,0.12335172295570374,-0.21932452917099,-0.14963743090629578,0.11480595171451569,-0.1878127157688141,-0.183804452419281,-0.09401823580265045,-0.1818905919790268,0.11116456985473633,-0.02666037529706955,0.15573567152023315,0.10020703077316284,-0.26986363530158997,-0.27058300375938416,0.08545301854610443,-0.1668165922164917,0.09380100667476654,-0.18808116018772125,-0.17512471973896027,-0.18858812749385834,0.16744603216648102,-0.102088063955307,-0.08686912059783936,0.059470172971487045,0.11864499747753143,-0.027040019631385803,-0.05019110441207886,0.1264318972826004,0.11466899514198303,0.10831379890441895,0.11864499747753143,0.03936564549803734,0.09473900496959686,-0.17925673723220825,0.14923721551895142,-0.2878304123878479,-0.29542800784111023,0.17364253103733063,-0.17548158764839172,0.03679194673895836,-0.19837352633476257,0.11969013512134552,0.08001035451889038,0.099543496966362,-0.010786054655909538,0.12779803574085236,0.043237973004579544,-0.20003384351730347,0.0997590571641922,0.034475620836019516,0.047207798808813095,-0.07298237085342407,0.0786878913640976,-0.2701544165611267,0.12495757639408112,0.1308296173810959,0.08686581254005432,-0.19209693372249603,-0.011406540870666504,0.10907889902591705,-0.2797372043132782,-0.10115967690944672,0.15194690227508545,0.13883046805858612,-0.01409883052110672,0.084686279296875,0.14970152080059052,0.12108604609966278,-0.27058300375938416,-0.1259976327419281,0.09535163640975952,0.09473900496959686,0.1286521852016449,0.12600626051425934,0.15576885640621185,0.09380100667476654,-0.10531486570835114,-0.2754605710506439,-0.22751139104366302,0.10405425727367401,0.12243300676345825,-0.12013551592826843,0.0013496428728103638,0.13269439339637756,-0.28831011056900024,-0.04239344596862793,0.13536907732486725,-0.05001997947692871,-0.21958144009113312,-0.17314523458480835,0.14209115505218506,0.10393105447292328,-0.24714097380638123,0.08561788499355316,0.09380100667476654,-0.025858134031295776,-0.036096930503845215,0.0466994009912014,-0.2879449427127838,0.14804565906524658,-0.24947589635849,0.12185220420360565,0.07868298888206482,0.09512260556221008,-0.11781808733940125,0.12166711688041687,-0.06295442581176758,0.06407296657562256,-0.292709618806839,0.12227457761764526,-0.06619246304035187,0.1016165018081665,-0.14055943489074707,-0.04156140983104706,0.06071827933192253,-0.13181282579898834,0.10741056501865387,-0.14534485340118408,-0.2800426483154297,0.11864499747753143,-0.29542800784111023,0.09509094059467316,-0.059438541531562805,0.06376446783542633,-0.2686299979686737,-0.054426804184913635,-0.12165102362632751,-0.2532272934913635,0.14376793801784515,0.034060556441545486,-0.16518820822238922,-0.21337205171585083,0.10736599564552307,-0.1148984432220459,-0.2284562885761261,-0.2866686284542084,-0.2953076958656311,0.1469878852367401,0.06892925500869751,-0.286377489566803,0.0018377527594566345,0.11352691054344177,0.153952494263649,0.11680534482002258,-0.18208348751068115,0.12207861244678497,0.0975857526063919,-0.08720715343952179,-0.26237887144088745,0.12356294691562653,0.00022491812705993652,0.1417359560728073,-0.29039081931114197,-0.17830540239810944,-0.2876914441585541,0.15187498927116394,-0.2195616364479065,-0.27058300375938416,-0.1027178019285202,-0.08160924911499023,0.15100263059139252,-0.29542800784111023,-0.15002620220184326,-0.08701212704181671,0.12428209185600281,-0.10026940703392029,0.09360694885253906,0.1207868754863739,-0.288225919008255,0.12251158058643341,-0.013180434703826904,0.11360780894756317,-0.2763157784938812,0.09871126711368561,-0.27058300375938416,0.07789686322212219,0.11462177336215973,0.09928938746452332,0.16305822134017944,-0.17199775576591492,0.14320722222328186,-0.240485280752182,0.125671848654747,0.09179748594760895,0.12120983004570007,-0.15795889496803284,0.09473900496959686,-0.2409077286720276,-0.023434698581695557,-0.2062772810459137,-0.07481929659843445,0.12265253067016602,-0.0635092705488205,0.04288792982697487,0.09473900496959686,-0.28844815492630005,0.16086730360984802,0.10380223393440247,0.14086198806762695,0.13243640959262848,-0.1014663577079773,0.04858287796378136,-0.08024665713310242,0.013551712036132812,0.12803399562835693,0.11838111281394958,0.12290802597999573,0.14882640540599823,0.09772160649299622,0.12212182581424713,-0.13682179152965546,-0.27811548113822937,0.1250399649143219,0.0817146897315979,0.06496460735797882,-0.2102174460887909,0.1222245991230011,-0.28287190198898315,-0.08577446639537811,-0.15418016910552979,-0.0574491024017334,-0.27663862705230713,0.11680345237255096,-0.2277008593082428,0.13434244692325592,-0.2851216495037079,0.08313027024269104,0.15297342836856842,-0.22615090012550354,-0.29542800784111023,-0.08173227310180664,-0.2872014045715332,-0.15737199783325195,-0.23422536253929138,0.16164202988147736,0.11155366897583008,-0.28084924817085266,-0.13174638152122498,-0.02198168635368347,-0.27058300375938416,0.11864499747753143,-0.2731987535953522,-0.13166195154190063,0.1300152689218521,0.17580780386924744,0.06335482001304626,-0.2722699046134949,-0.05793629586696625,-0.10948123037815094,0.1755276471376419,0.09473900496959686,0.04318741336464882,0.09596146643161774,0.1203589141368866,0.01142670214176178,0.12174820899963379,0.03177487850189209,-0.0448775589466095,-0.2477385252714157,0.029081087559461594,0.1090507060289383,0.0675385594367981,-0.1239854246377945,0.14044424891471863,0.10032549500465393,-0.14300672709941864,0.09503820538520813,-0.04771939665079117,-0.2896391749382019,0.15214158594608307,-0.29008886218070984,-0.23101076483726501,-0.2944427728652954,-0.1484299898147583,-0.12665733695030212,-0.11247283220291138,-0.17811845242977142,0.11864499747753143,-0.2097114622592926,-0.22616712749004364,0.11032682657241821,-0.17214715480804443,-0.27058300375938416,-0.08912578225135803,-0.1603366881608963,0.13422593474388123,-0.16763350367546082,-0.07610194385051727,-0.28227049112319946,-0.1943754404783249,-0.037189118564128876,0.11826100945472717,0.11039794981479645,0.09526817500591278,-0.007247472181916237,0.12393040955066681,-0.04066652059555054,-0.08031322062015533,0.10208581387996674,-0.2609351575374603,-0.2751764953136444,0.10223956406116486,0.09618818759918213,0.09473900496959686,0.06884190440177917,-0.10296711325645447,-0.2852744460105896,-0.29433876276016235,-0.07302135229110718,-0.1724482774734497,-0.28796786069869995,-0.27058300375938416,0.09838107228279114,-0.1607942283153534,-0.1754874587059021,0.09380100667476654,0.13524864614009857,0.09380100667476654,0.12023253738880157,-0.147242933511734,-0.00824841856956482,-0.05530045926570892,-0.27687349915504456,0.10190820693969727,0.00930502638220787,-0.19490645825862885,-0.07655589282512665,-0.2817525267601013,0.09969653189182281,-0.28702840209007263,-0.29542800784111023,-0.294607013463974,-0.13507241010665894,0.09615226089954376,0.12136037647724152,-0.17340457439422607,0.09380100667476654,-0.017021507024765015,-0.28143179416656494,-0.28418976068496704,-0.0014275088906288147,-0.22389724850654602,0.06587947905063629,-0.09005564451217651,0.08830617368221283,-0.29542800784111023,0.15814723074436188,0.13782750070095062,0.1433730274438858,-0.004777818918228149,0.1306835412979126,-0.004999309778213501,0.13164639472961426,0.12316875159740448,-0.1787315011024475,-0.28480902314186096,0.0418928824365139,-0.21598710119724274,0.07421241700649261,0.1459035724401474,-0.1937943398952484,0.14089615643024445,0.13894934952259064,0.09742535650730133,0.09843811392784119,0.11864499747753143,-0.27058300375938416,0.12277062237262726,-0.2619461715221405,0.12509100139141083,0.11864499747753143,-0.003201901912689209,0.10123808681964874,0.1394442319869995,0.06304261088371277,0.06528857350349426,-0.21087315678596497,0.06663881242275238,0.09570008516311646,-0.20565415918827057,-0.26667478680610657,0.1101691722869873,0.04355073347687721,0.1441151648759842,-0.21602678298950195,-0.2674718201160431,-0.15057338774204254,-0.2791356146335602,0.1311963051557541,0.01674254983663559,0.12020674347877502,-0.2683311700820923,0.09725460410118103,-0.025578543543815613,0.0948285162448883,-0.07380151748657227,0.09795208275318146,-0.1037166565656662,0.09536553919315338,0.12037119269371033,-0.24962078034877777,0.10175460577011108,-0.26827356219291687,-0.0009193494915962219,0.11069749295711517,0.03106546401977539,-0.06504113972187042,0.13600918650627136,0.0015134736895561218,0.16467779874801636,0.057252224534749985,0.10125423967838287,0.14923392236232758,-0.004139155149459839,-0.05371539294719696,-0.02098201960325241,0.0038689076900482178,0.09946291148662567,0.1408461481332779,0.12076009809970856,-0.2396823763847351,0.1331198811531067,-0.13899660110473633,0.055247049778699875,0.1410825103521347,-0.29542800784111023,-0.2655424177646637,-0.21144314110279083,-0.20465093851089478,-0.019132286310195923,-0.006859026849269867,-0.2508373558521271,0.14065705239772797,-0.0654878169298172,0.11878690123558044,0.09977255761623383,-0.1883591115474701,0.12473560869693756,-0.06457319110631943,-0.20659561455249786,-0.11178284883499146,0.11800707876682281,0.022130407392978668,0.10317666828632355,-0.019515380263328552,-0.2552155554294586,0.12101002037525177,0.09720537066459656,-0.26785939931869507,-0.010538160800933838,0.15007589757442474,-0.1720643937587738,0.09191018342971802,-0.23628662526607513,-0.06939591467380524,0.11864499747753143,-0.025294587016105652,-0.14592832326889038,0.09380100667476654,0.028046865016222,0.11864499747753143,-0.20645131170749664,0.11864499747753143,-0.047598764300346375,0.11286716163158417,-0.17665976285934448,0.11986544728279114,0.14265933632850647,-0.21433642506599426,0.08312053978443146,0.15973913669586182,-0.005101300776004791,-0.06178933382034302,0.1268935203552246,0.015179544687271118,0.1342964470386505,0.13262724876403809,0.03940419480204582,-0.2627452313899994,0.16327078640460968,0.12233753502368927,0.09033039212226868,0.11899565160274506,-0.28858497738838196,0.10745686292648315,-0.0509929358959198,-0.16631276905536652,-0.1293102502822876,0.11675581336021423,0.02453857660293579,0.10306386649608612,-0.20084503293037415,0.13938994705677032,-0.014239788055419922,0.1758417785167694,0.10314986109733582,0.12430644035339355,0.017361849546432495,-0.29457423090934753,-0.29542800784111023,0.004230082035064697,-0.29479774832725525,-0.29164013266563416,0.17405486106872559,-0.2890506088733673,-0.29542800784111023,-0.2701532244682312,-0.06505224108695984,-0.2673373520374298,0.11864499747753143,0.11426296830177307,0.04694853350520134,0.050585146993398666,-0.18270054459571838,0.11864499747753143,0.055602576583623886,-0.18421809375286102,-0.29542800784111023,-0.19130906462669373,-0.11267803609371185,0.1301765888929367,0.10875257849693298,0.00958661362528801,-0.001773027703166008,-0.24476999044418335,-0.004006817936897278,0.1464751660823822,-0.09356501698493958,0.09538449347019196,0.14232636988162994,0.10917350649833679,0.07099460065364838,0.019771695137023926,-0.21329239010810852,-0.24584859609603882,-0.23577316105365753,0.12843836843967438,-0.252225786447525,0.1215810626745224,0.14315949380397797,0.018785923719406128,-0.29180100560188293,-0.050983309745788574,-0.21413597464561462,0.05445316061377525,0.12978875637054443,-0.11612772941589355,0.11106432974338531,0.029067933559417725,-0.132960245013237,0.12748071551322937,-0.024446070194244385,-0.014058656990528107,-0.18320602178573608,-0.12949055433273315,-0.2903115749359131,-0.16652826964855194,-0.08469992876052856,-0.24293635785579681,-0.08944372832775116,-0.22093558311462402,-0.271206796169281,-0.08434109389781952,-0.20137733221054077,0.16037322580814362,-0.020363517105579376,-0.05879184603691101,0.15176069736480713,-0.20241469144821167,-0.06477801501750946,0.146987646818161,0.14581286907196045,0.1320798248052597,-0.16248124837875366,0.13805876672267914,0.11864499747753143,0.12536393105983734,0.10319124162197113,-0.0764879584312439,-0.01855006068944931,0.12950816750526428,-0.2585783302783966,0.09380100667476654,-0.14460060000419617,-0.015933968126773834,0.16136795282363892,0.09380100667476654,0.10777176916599274,-0.171162948012352,-0.1378852128982544,0.1194915920495987,-0.27303022146224976,-0.07983174920082092,-0.17072793841362,0.17326165735721588,-0.037889264523983,-0.29237303137779236,-0.10165511071681976,-0.11485424637794495,-0.27058300375938416,-0.26234760880470276,0.1225052922964096,-0.07602828741073608,-0.011141873896121979,-0.1334853619337082,0.12967954576015472,-0.1463039219379425,-0.16490036249160767,-0.2564927935600281,0.15646503865718842,-0.29542800784111023,-0.24594631791114807,0.11029894649982452,0.15350790321826935,0.04484134539961815,0.08194935321807861,-0.17461887001991272,0.12241525948047638,0.1142461746931076,-0.030666157603263855,-0.16540314257144928,0.13131456077098846,0.015889015048742294,-0.29542800784111023,0.09380100667476654,-0.03278312087059021,0.010131143033504486,0.09731157124042511,0.08807751536369324,0.11253949999809265,0.15997833013534546,-0.010408548638224602,-0.10448728501796722,0.057697322219610214,0.10267291963100433,0.17067216336727142,0.09380100667476654,0.12057200074195862,-0.24272386729717255,-0.27817097306251526,-0.27279332280158997,-0.06597670912742615,-0.2746812105178833,0.12726402282714844,-0.12135779857635498,0.03840091452002525,0.17448247969150543,0.13593633472919464,-0.1868143528699875,0.12358047068119049,0.11864499747753143,0.1645616888999939,0.017017316073179245,0.12961256504058838,-0.152494877576828,-0.2627730667591095,0.16965186595916748,0.12560120224952698,0.13300110399723053,0.010256752371788025,0.11864499747753143,-0.042832642793655396,0.13588851690292358,-0.18326681852340698,0.11864499747753143,-0.29542800784111023,0.09596017003059387,0.13146235048770905,-0.23946082592010498,0.17131879925727844,-0.021468624472618103,-0.07858017086982727,-0.09014078974723816,-0.231490820646286,-0.1515716016292572,-0.1850975751876831,0.11994776129722595,-0.07816915214061737,-0.1539572775363922,-0.08731594681739807,0.05755661055445671,-0.04969903826713562,-0.21899250149726868,0.17538031935691833,-0.09855519235134125,-0.29336974024772644,-0.279567688703537,-0.02780342847108841,0.09412756562232971,-0.19509153068065643,0.09682784974575043,-0.13647381961345673,0.1430189609527588,-0.0788203626871109,-0.27469876408576965,-0.2941877245903015,0.16610834002494812,0.1316221058368683,-0.11942680180072784,-0.046489372849464417,-0.05048780143260956,0.11834263801574707,0.04758687689900398,0.11864499747753143,-0.12078751623630524,-0.27058300375938416,-0.28546032309532166,0.15901392698287964,0.003840140998363495,0.10287827253341675,-0.29542800784111023,-0.07072478532791138,0.12281975150108337,-0.05493640899658203,0.15852724015712738,0.10673484206199646,-0.13980276882648468,-0.1473459005355835,0.1312205195426941,-0.04495805501937866,0.12384708225727081,0.09473900496959686,-0.19490259885787964,0.11997775733470917,-0.11636276543140411,-0.049143314361572266,0.08562624454498291,0.08268280327320099,-0.001513749361038208,0.05381929501891136,-0.19485047459602356,0.14532062411308289,0.06147206202149391,-0.040779538452625275,0.08473387360572815,0.05991613492369652,-0.13291598856449127,-0.21597233414649963,0.11991357803344727,-0.10020779073238373,0.16382478177547455,-0.08919788151979446,-0.09722186625003815,-0.11143748462200165,0.1533764749765396,-0.2741353213787079,0.1640949547290802,-0.29542800784111023,0.1299820989370346,-0.2522936165332794,0.1211862564086914,-0.29542800784111023,-0.20802584290504456,0.11864499747753143,0.11864499747753143,0.0866522490978241,-0.2511887550354004,0.09380100667476654,-0.022497467696666718,0.07454977929592133,0.07562890648841858,0.13264112174510956,0.12738148868083954,0.13263878226280212,-0.2799488306045532,0.1383964866399765,0.09780582785606384,0.12772856652736664,-0.0356307327747345,0.1657773107290268,0.0985695868730545,0.11215835809707642,-0.07474608719348907,-0.29079005122184753,0.1459125429391861,0.11864499747753143,0.04030391201376915,0.1211310476064682,-0.2667299807071686,0.15789403021335602,0.12322656810283661,-0.27058300375938416,0.13133402168750763,-0.1579740345478058,0.10805928707122803,0.04199058189988136,0.09473900496959686,0.15643805265426636,0.05147772654891014,0.14375004172325134,-0.29269683361053467,-0.14880451560020447,-0.09527242183685303,0.1018524020910263,-0.05750471353530884,-0.06397183239459991,-0.2949713170528412,-0.15547102689743042,0.09812185168266296,0.10425332188606262,0.013213485479354858,0.10619038343429565,0.05727985128760338,0.09344211220741272,-0.26603391766548157,-0.08835485577583313,0.0690203607082367,-0.1111566573381424,-0.07966040074825287,0.13925324380397797,0.14721857011318207,-0.28687071800231934,-0.25538119673728943,0.14328628778457642,0.084686279296875,-0.2687561810016632,0.12838676571846008,-0.09587951004505157,0.1724165827035904,0.06328415870666504,0.09100793302059174,0.025683362036943436,0.005975209176540375,0.010074105113744736,0.126459538936615,-0.29542800784111023,-0.13402940332889557,-0.20110970735549927,0.026014167815446854,-0.011801868677139282,-0.07867632806301117,-0.29542800784111023,-0.021462246775627136,-0.06589967757463455,0.15751735866069794,-0.03741180896759033,-0.04971836507320404,0.13877923786640167,0.09380100667476654,-0.2111288160085678,-0.15607258677482605,0.044153403490781784,-0.14704759418964386,0.11864499747753143,0.1726641207933426,-0.05849014222621918,0.06641191244125366,-0.20735549926757812,-0.11477576196193695,-0.2671780288219452,-0.27058300375938416,0.09916210174560547,-0.29191678762435913,0.12654098868370056,-0.2894164025783539,-0.19292882084846497,0.13128222525119781,-0.29542800784111023,0.12142117321491241,-0.2169373631477356,0.06959465146064758,0.12375743687152863,0.1014154851436615,0.1618788242340088,-0.27627748250961304,0.11562173068523407,0.16045603156089783,-0.06733408570289612,0.013101667165756226,-0.2611127495765686,-0.2541724443435669,0.10770292580127716,0.1474960297346115,0.1195606142282486,0.0772709846496582,-0.1337698996067047,-0.29335489869117737,0.11167505383491516,0.12845057249069214,0.11171336472034454,0.1250033974647522,-0.29542800784111023,0.11846746504306793,-0.257917582988739,-0.09912392497062683,0.04415345564484596,0.1643366664648056,0.11864499747753143,-0.2379773110151291,-0.2785443067550659,-0.28889262676239014,-0.26340222358703613,-0.27120885252952576,0.08914753794670105,0.11864499747753143,-0.2847825288772583,0.09879282116889954,0.011570163071155548,-0.09825530648231506,-0.2820641100406647,0.09971451759338379,-0.1358271688222885,0.1378801465034485,-0.1560870260000229,-0.29537227749824524,-0.03312264382839203,0.1033717542886734,-0.2917034924030304,-0.25313591957092285,0.006401568651199341,0.12601596117019653,0.1461142748594284,0.13732466101646423,0.11864499747753143,0.13651886582374573,-0.29542800784111023,-0.21167241036891937,-0.27058300375938416,-0.16060401499271393,0.16845376789569855,-0.23171748220920563,0.11639919877052307,0.13299982249736786,0.11864499747753143,0.1069147139787674,-0.28038156032562256,-0.21035870909690857,-0.07166019082069397,0.11864499747753143,-0.07864344865083694,-0.07321925461292267,-0.04225200414657593,0.09319256246089935,0.11545857787132263,0.12179483473300934,-0.0002539064735174179,-0.16757044196128845,0.10408654808998108,0.09473900496959686,-0.26512327790260315,0.11864499747753143,0.1389453411102295,0.15662963688373566,-0.06565125286579132,-0.003145948052406311,-0.05452421307563782,0.14486469328403473,-0.1296246349811554,0.041498418897390366,-0.29542800784111023,-0.20930439233779907,0.1249922513961792,-0.2779420018196106,-0.10649968683719635,0.11374558508396149,0.13785983622074127,-0.146644726395607,-0.19851264357566833,0.13127276301383972,0.1684325784444809,0.17098750174045563,0.11629684269428253,0.1671602427959442,-0.22479265928268433,-0.20632517337799072,-0.2551997900009155,0.06526258587837219,-0.04622639715671539,-0.06616370379924774,-0.2119176983833313,-0.23825757205486298,-0.28905296325683594,0.1337565928697586,0.16295771300792694,-0.2144557535648346,-0.2701212763786316,-0.20477721095085144,-0.16466154158115387,-0.05046401917934418,0.12871485948562622,-0.03566477447748184,0.1369333416223526,0.12094418704509735,-0.2345326989889145,-0.21477816998958588,0.09517808258533478,0.12026843428611755,-0.10916368663311005,0.11149491369724274,0.10093049705028534,0.09451095759868622,-0.25101137161254883,0.0983494222164154,-0.05479125678539276,0.17178215086460114,-0.29542800784111023,-0.17486362159252167,-0.061824649572372437,0.004023194313049316,0.10941565036773682,-0.29400721192359924,0.10506992042064667,0.11262255907058716,0.02649277076125145,-0.02693849802017212,-0.1263839304447174,-0.010598331689834595,0.13195647299289703,0.11864499747753143,-0.03371507674455643,-0.29509827494621277,0.05683430656790733,0.023484501987695694,0.06627747416496277,0.09397052228450775,-0.09954741597175598,-0.29542800784111023,0.09380100667476654,-0.0780714750289917,0.1636945754289627,0.12284621596336365,0.11481067538261414,0.12746433913707733,-0.25533127784729004,0.09440559148788452,-0.282858669757843,0.1659383475780487,0.13230694830417633,-0.1189153790473938,-0.0562155544757843,0.09473900496959686,0.09750045835971832,0.11864499747753143,0.1306964010000229,-0.16830775141716003,-0.23239651322364807,0.12258267402648926,0.13510237634181976,-0.12544213235378265,0.08667027950286865,0.15213929116725922,-0.2899739444255829,0.125541090965271,0.11864499747753143,-0.29375043511390686,0.09946596622467041,0.09849205613136292,0.11668327450752258,-0.09064316749572754,0.09378938376903534,0.07463127374649048,-0.08381437510251999,0.12430045008659363,0.14447155594825745,-0.035156309604644775,0.12247428297996521,-0.20206302404403687,-0.2264648675918579,0.03959604725241661,0.1087857186794281,-0.13714294135570526,-0.08372409641742706,0.12702080607414246,0.11864499747753143,-0.06619229912757874,0.07496616244316101,0.11864499747753143,-0.29078802466392517,0.02877333015203476,0.031623829156160355,0.12494261562824249,0.07567225396633148,-0.24107885360717773,-0.18428868055343628,-0.03193776309490204,-0.15836118161678314,0.1582155078649521,-0.15928298234939575,-0.2891276776790619,0.11864499747753143,0.14478792250156403,-0.06610789895057678,0.11253470182418823,-0.26454904675483704,-0.28994935750961304,-0.10269753634929657,-0.25202494859695435,0.09408527612686157,0.09924517571926117,0.062728151679039,-0.17246583104133606,-0.026444226503372192,-0.12479601800441742,0.11864499747753143,-0.052268482744693756,-0.21387536823749542,-0.10545474290847778,0.12569373846054077,0.004926115274429321,-0.2151097059249878,0.07959035038948059,0.13934138417243958,-0.09504471719264984,-0.0488092303276062,0.124396413564682,-0.15718048810958862,0.012343257665634155,-0.2913440763950348,0.11751526594161987,0.09035907685756683,0.13112403452396393,0.11864499747753143,-0.22215357422828674,-0.2758057415485382,0.09702049195766449,-0.1952337622642517,0.07163792848587036,0.10019886493682861,-0.02176278829574585,-0.1445007175207138,0.14553435146808624,-0.12401758134365082,-0.26991090178489685,0.12202523648738861,0.0035046637058258057,-0.27353164553642273,0.12082751095294952,0.16974058747291565,0.0650193840265274,0.10254983603954315,0.14259985089302063,-0.1634366810321808,-0.09152659773826599,-0.2932050824165344,-0.295217901468277,-0.29542800784111023,0.09380100667476654,0.053281668573617935,-0.2006365805864334,0.11864499747753143,0.10983933508396149,-0.005031682550907135,-0.016680918633937836,0.10693366825580597,0.007487982511520386,-0.190383642911911,0.07978922128677368,0.16327163577079773,-0.04381763935089111,0.1370706707239151,-0.06686972081661224,-0.14917492866516113,0.03581042215228081,0.14096252620220184,-0.20621266961097717,-0.08468152582645416,-0.2656209170818329,-0.006573418155312538,-0.07331664860248566,0.12287333607673645,-0.29542800784111023,-0.1674846112728119,-0.2782641649246216,-0.1459367722272873,-0.29542800784111023,0.15353429317474365,-0.25116467475891113,-0.11816094815731049,0.10624200105667114,0.13055072724819183,0.09669852256774902,0.14521020650863647,0.1571657955646515,0.014300480484962463,-0.25029444694519043,-0.29542800784111023,-0.26094523072242737,-0.28478050231933594,-0.036980874836444855,-0.078978031873703,-0.27725809812545776,-0.1729966104030609,-0.26550403237342834,0.10598640143871307,-0.2526082694530487,-0.2462402582168579,-0.1985388547182083,-0.1794394850730896,-0.2819405496120453,0.10314662754535675,-0.09092894196510315,0.07452620565891266,0.10950267314910889,0.05548427626490593,0.10894463956356049,-0.13610613346099854,-0.0305483341217041,-0.005998531356453896,-0.014939330518245697,-0.2713738977909088,0.14038576185703278,0.16584715247154236,-0.0400678813457489,0.09380100667476654,0.11863210797309875,-0.1417049914598465,-0.059162966907024384,-0.14286449551582336,-0.2678297758102417,-0.14597153663635254,0.016047097742557526,0.056137796491384506,-0.10674859583377838,-0.25757038593292236,-0.27058300375938416,0.13894353806972504,-0.2755998969078064,0.09894080460071564,-0.27789586782455444,-0.048005253076553345,-0.007310152053833008,0.12844009697437286,-0.19747480750083923,0.1552720069885254,-0.25672683119773865,0.09380100667476654,-0.020513266324996948,0.055629100650548935,-0.2792111933231354,0.09711146354675293,0.02907755970954895,-0.09394979476928711,-0.22633755207061768,-0.09433619678020477,-0.2901726961135864,-0.04712739586830139,-0.21052972972393036,0.13306604325771332,-0.18018996715545654,0.12257646024227142,-0.24072274565696716,0.12646974623203278,-0.17092403769493103,0.11080344021320343,0.08903177082538605,-0.12971091270446777,0.1514671891927719,-0.15060961246490479,-0.01847469061613083,0.09933975338935852,0.014731165021657944,-0.15349124372005463,-0.2190907895565033,0.17456313967704773,-0.11971482634544373,-0.2498875856399536,0.09380100667476654,0.10644695162773132,-0.010396137833595276,-0.2014092206954956,-0.2722053527832031,-0.07261184602975845,0.15984512865543365,0.07192005217075348,-0.27058300375938416,-0.24481235444545746,0.034388091415166855,-0.2875308692455292,-0.2093430459499359,-0.14265157282352448,-0.24856539070606232,-0.289180725812912,-0.06567570567131042,0.10822659730911255,0.11101354658603668,0.0008891522884368896,-0.09777726233005524,-0.04371050000190735,0.12770992517471313,0.1058666855096817,-0.29542800784111023,0.11202786862850189,0.120786651968956,0.1257738471031189,0.12741583585739136,-0.27058300375938416,-0.0497010201215744,-0.0921916589140892,-0.02191239595413208,0.02253490686416626,-0.2244856208562851,-0.16499146819114685,-0.06259404122829437,0.13639400899410248,-0.2888469398021698,0.11466103792190552,-0.162281796336174,-0.29542800784111023,-0.19971299171447754,0.14175938069820404,0.11021006107330322,-0.10251311957836151,-0.10395042598247528,-0.18231600522994995,-0.007101891562342644,-0.26958975195884705,-0.08172377943992615,0.13237395882606506,-0.2189110815525055,-0.2944876551628113,-0.03442848473787308,0.13701404631137848,0.09473900496959686,-0.15005668997764587,0.13096201419830322,0.07920651137828827,0.05842013284564018,-0.06588263809680939,0.14576338231563568,-0.07114739716053009,-0.17847728729248047,-0.017674550414085388,0.005852594971656799,0.1395455151796341,0.11864499747753143,-0.21189318597316742,-0.22679969668388367,0.10653404891490936,0.1700170338153839,-0.2638755738735199,-0.2720673084259033,0.01024492084980011,0.09380100667476654,0.025332149118185043,0.09824526309967041,-0.2823578119277954,-0.12387782335281372,-0.17412397265434265,0.09729966521263123,0.16187113523483276,0.10987602174282074,0.07378777861595154,-0.28486061096191406,-0.10567720234394073,-0.14867554605007172,-0.02448698878288269,-0.27058300375938416,-0.07949583232402802,0.14647553861141205,0.06532205641269684,-0.13244019448757172,0.03632694110274315,-0.14734278619289398,0.16544127464294434,-0.2380756288766861,0.16881006956100464,-0.054247066378593445,-0.21045958995819092,-0.2901425063610077,-0.2174227237701416,0.11982540786266327,-0.26103147864341736,0.11472931504249573,-0.033682242035865784,0.1073424369096756,0.11864499747753143,0.15399792790412903,0.11236180365085602,0.07016776502132416,0.12483051419258118,-0.1737448275089264,-0.27058300375938416,-0.22142142057418823,0.14100304245948792,0.027212679386138916,0.05369281396269798,-0.27012383937835693,0.14988508820533752,0.1575196534395218,0.043833281844854355,0.11864499747753143,0.12673649191856384,0.05354025587439537,-0.23145189881324768,-0.13131818175315857,0.1249375194311142,-0.2694605886936188,0.11749310791492462,-0.14230990409851074,0.019005149602890015,0.09588046371936798,-0.0737404003739357,-0.1015525609254837,0.006385862827301025,0.08533243834972382,0.14236964285373688,0.03766996040940285,-0.29542800784111023,0.09380100667476654,0.11181589961051941,0.13938014209270477,-0.27058300375938416,-0.08772517740726471,0.1515377312898636,-0.039198510348796844,-0.2945106625556946,-0.07412940263748169,-0.29542800784111023,-0.13062015175819397,0.15082566440105438,0.10299815237522125,0.09565640985965729,0.1558234691619873,0.01701633632183075,-0.1503218114376068,0.1324189156293869,-0.251757949590683,0.13059872388839722,-0.23955662548542023,0.09826481342315674,-0.2709267735481262,0.10458478331565857,0.1201925128698349,-0.1613227128982544,-0.11531290411949158,-0.20672129094600677,0.078715980052948,0.11884814500808716,0.014311693608760834,0.09380100667476654,-0.16243286430835724,0.09010188281536102,-0.29222527146339417,-0.011034565046429634,-0.049816831946372986,-0.2951839566230774,-0.2859991490840912,-0.13731986284255981,0.09559741616249084,0.11864499747753143,-0.13228096067905426,-0.28016164898872375,0.1200898289680481,-0.28888648748397827,-0.07536563277244568,0.0800558477640152,0.10367439687252045,0.11150515079498291,-0.017302490770816803,0.0953170657157898,0.1388150006532669,0.09253372251987457,-0.11948028206825256,0.11864499747753143,-0.06530211865901947,-0.16723230481147766,0.1537213921546936,-0.008694915100932121,0.17373670637607574,-0.29216915369033813,0.11220777034759521,0.11980384588241577,0.10322357714176178,-0.2853599190711975,0.15474535524845123,0.14875227212905884,-0.19798970222473145,0.1535642147064209,0.14732660353183746,-0.22924551367759705,0.08627212047576904,-0.29499438405036926,-0.2035209834575653,-0.18658849596977234,0.11256188154220581,0.12033912539482117,0.15586914122104645,0.09403197467327118,0.1253490447998047,-0.03553009033203125,0.1474892795085907,-0.25626340508461,-0.011852085590362549,0.11899904906749725,0.019045528024435043,0.1139945238828659,-0.010678611695766449,-0.2758270800113678,-0.23188531398773193,-0.199164018034935,0.06260645389556885,0.11559432744979858,0.1455051749944687,-0.04513201117515564,-0.25892287492752075,0.17535878717899323,0.08234532177448273,-0.005725031718611717,0.12859944999217987,-0.0242319256067276,-0.0937013179063797,-0.1933271884918213,-0.24161970615386963,-0.11229801177978516,0.09364430606365204,0.12406672537326813,0.10472646355628967,-0.29542800784111023,0.15645915269851685,-0.03156788647174835,-0.11306045949459076,0.13026881217956543,-0.024516135454177856,0.12609711289405823,-0.10527034103870392,0.1442589908838272,0.09733432531356812,0.06912840902805328,0.11864499747753143,0.09503266215324402,-0.2007397711277008,0.0343768410384655,-0.19763068854808807,-0.10526245832443237,0.12551556527614594,-0.28655099868774414,0.11864499747753143,-0.2904777526855469,0.11517791450023651,-0.06408961117267609,0.14439865946769714,0.09871718287467957,-0.2857244908809662,-0.15453267097473145,0.10226450860500336,0.09924210608005524,-0.29542800784111023,0.004689663648605347,0.10644285380840302,-0.04978953301906586,0.13562853634357452,-0.2295582890510559,0.09380100667476654,0.0441589318215847,0.09861961007118225,0.11898662149906158,0.11553871631622314,0.13663527369499207,-0.2857418656349182,0.10294012725353241,-0.11736126244068146,-0.29322054982185364,-0.04375073313713074,-0.27445727586746216,-0.2892128825187683,0.1560857892036438,0.0985957533121109,0.03195673227310181,-0.04402858018875122,-0.12334369122982025,0.06362411379814148,0.134103924036026,-0.234337717294693,0.048277389258146286,-0.20216530561447144,0.1037130355834961,-0.1794901341199875,-0.2912231683731079,0.01687677949666977,0.14120958745479584,-0.1773032546043396,-0.11463563144207001,-0.10677945613861084,-0.02489827573299408,-0.04878437519073486,0.10596378147602081,-0.27058300375938416,-0.27058300375938416,-0.05377271771430969,-0.12641967833042145,-0.1291021704673767,0.13745637238025665,-0.27058300375938416,-0.02467799186706543,0.0953429788351059,0.13078227639198303,-0.03787405788898468,0.09380100667476654,-0.053098589181900024,0.14121486246585846,0.09857279062271118,-0.18622255325317383,-0.2515510618686676,0.12520331144332886,-0.19526317715644836,0.07239305973052979,0.10900427401065826,-0.29542800784111023,-0.23967011272907257,0.05111042782664299,-0.05025885999202728,-0.19758275151252747,0.13657915592193604,-0.22411036491394043,0.05158497765660286,0.0624694861471653,-0.10594739019870758,-0.2773255705833435,0.05712517723441124,-0.2910705804824829,0.056794267147779465,0.09209783375263214,-0.25870659947395325,-0.18250390887260437,-0.09896071255207062,0.16306503117084503,0.09473900496959686,0.12491989135742188,-0.23499253392219543,-0.2610601484775543,-0.1451554000377655,0.17116248607635498,0.11864499747753143,0.12672661244869232,0.11791230738162994,0.12894238531589508,0.13809902966022491,0.09469747543334961,-0.2060369849205017,-0.18941128253936768,0.012291401624679565,0.1562403291463852,-0.02229931205511093,0.09010949730873108,-0.1711723506450653,-0.0781649649143219,0.014009088277816772,-0.15118549764156342,-0.1755431443452835,0.10809250175952911,-0.273316353559494,-0.17920702695846558,0.127902552485466,0.13034488260746002,0.08561114966869354,-0.29542800784111023,-0.1416335552930832,0.1372571438550949,0.16062800586223602,-0.29487213492393494,0.14265041053295135,0.061711277812719345,-0.0604264959692955,-0.027717679738998413,-0.03186792880296707,0.09618546068668365,-0.1904364675283432,0.1328182965517044,-0.024314910173416138,0.10442249476909637,-0.17280009388923645,0.10608935356140137,-0.10807742178440094,0.1343211829662323,0.05204390361905098,0.15393079817295074,0.045768555253744125,0.07130247354507446,0.09380100667476654,0.12042228877544403,0.053036030381917953,0.09991754591464996,0.13496193289756775,-0.1228078305721283,-0.17911845445632935,0.0635703057050705,0.11906908452510834,0.09571695327758789,0.1464223861694336,-0.19461321830749512,0.12720009684562683,0.125400111079216,-0.11414448916912079,0.1383906453847885,-0.29277172684669495,0.07400436699390411,-0.1416947841644287,0.11864499747753143,0.06246024742722511,0.1303248405456543,-0.27058300375938416,0.13922592997550964,-0.019887328147888184,-0.04549911618232727,0.1581701785326004,-0.07484357804059982,-0.2319435179233551,0.13942545652389526,0.049210723489522934,-0.18598340451717377,0.09664641320705414,0.058451343327760696,-0.07703813910484314,0.10117200016975403,0.08260601758956909,0.08598507940769196,0.14927776157855988,-0.26018160581588745,-0.14542485773563385,-0.024090975522994995,0.1138831228017807,0.11524689197540283,-0.2510053515434265,-0.17714351415634155,-0.2702886760234833,-0.2400018721818924,0.0948682576417923,-0.0316275954246521,0.09628106653690338,-0.29542800784111023,0.12693840265274048,0.09473900496959686,-0.21228349208831787,-0.13000497221946716,-0.28263217210769653,0.16433283686637878,0.10278478264808655,0.13061997294425964,0.1648769974708557,0.12483294308185577,-0.28796154260635376,-0.2381836324930191,0.14050845801830292,0.147487074136734,-0.06035175919532776,-0.10098706185817719,0.11864499747753143,-0.29542800784111023,-0.29318591952323914,0.14317215979099274,0.1100405901670456,-0.1661776304244995,-0.1540888398885727,0.07867467403411865,-0.04120171070098877,-0.278813898563385,-0.29099857807159424,0.011790663003921509,-0.03729715943336487,0.09380100667476654,0.127836674451828,0.09589977562427521,0.011574089527130127,-0.12797832489013672,-0.2774220108985901,-0.23141063749790192,0.005372375249862671,0.13517537713050842,0.12006480991840363,0.06478261947631836,0.12966619431972504,-0.14096912741661072,-0.17272818088531494,-0.258277952671051,-0.27058300375938416,0.1658434122800827,0.17174993455410004,0.17331378161907196,-0.0718567818403244,-0.25142034888267517,-0.18641221523284912,0.1310778558254242,-0.124688521027565,0.17124108970165253,-0.29542800784111023,0.05017469450831413,0.07890355587005615,-0.15585902333259583,-0.2745259702205658,0.10234048962593079,0.10539068281650543,-0.018551677465438843,0.14046503603458405,0.031017515808343887,-0.2779369652271271,-0.01565009355545044,0.13966186344623566,0.09473900496959686,-0.10918170213699341,-0.06071573495864868,-0.16187581419944763,-0.1661997139453888,0.10828600823879242,-0.24350710213184357,-0.21574869751930237,0.06744390726089478,0.026650413870811462,0.036980073899030685,0.13089676201343536,0.12999971210956573,0.10405318439006805,-0.12107701599597931,0.16505518555641174,-0.23679958283901215,-0.16488707065582275,-0.1030680388212204,0.09380100667476654,0.049753036350011826,-0.2946765124797821,0.005323618650436401,0.12662099301815033,0.12276120483875275,0.09380100667476654,0.09626926481723785,0.0007077977061271667,0.041350092738866806,-0.0511849969625473,-0.19247978925704956,0.11864499747753143,-0.12496337294578552,-0.2373456060886383,-0.2570282816886902,-0.2074037492275238,0.10690037906169891,0.12680678069591522,0.09672556817531586,0.1199067085981369,-0.19029706716537476,0.1499815136194229,0.10485745966434479,0.0866062194108963,0.16386234760284424,0.11864499747753143,-0.047741904854774475,0.1598310023546219,-0.07355999946594238,0.11864499747753143,-0.09509187936782837,-0.09193531423807144,0.09380100667476654,0.14715564250946045,0.09371718764305115,0.14304515719413757,-0.2202347218990326,0.09380100667476654,0.03465981408953667,-0.12207609415054321,0.12091955542564392,-0.04641023278236389,0.1700012981891632,-0.02785458415746689,-0.21430900692939758,-0.28109437227249146,0.09638634324073792,0.145005464553833,-0.29542800784111023,0.09473900496959686,0.11498565971851349,-0.09560167789459229,0.09755738079547882,-0.0052023231983184814,-0.16660456359386444,-0.2772532105445862,-0.04105187952518463,-0.08631473779678345,0.15791475772857666,0.09891436994075775,-0.28256362676620483,-0.27058300375938416,0.017234377562999725,-0.2948446273803711,-0.11677084863185883,-0.043845295906066895,0.10614632070064545,0.09870553016662598,-0.27916330099105835,-0.09613218903541565,-0.08312024176120758,0.14148254692554474,0.09664584696292877,0.10364116728305817,-0.0046608541160821915,-0.12656882405281067,-0.011340171098709106,-0.2652600407600403,-0.253302663564682,-0.2786979675292969,-0.11748155951499939,-0.1819021999835968,-0.10641233623027802,-0.05726763606071472,-0.039166852831840515,-0.060305386781692505,0.17084233462810516,-0.1464565545320511,0.09317554533481598,0.0587926022708416,0.03495147451758385,-0.010347284376621246,0.16788816452026367,0.12425923347473145,0.05642443522810936,0.14678210020065308,0.10920816659927368,-0.16478027403354645,-0.27634197473526,0.06884884834289551,-0.13535287976264954,0.11666630208492279,-0.25686031579971313,0.13951744139194489,-0.28905585408210754,0.14823682606220245,0.1509285420179367,-0.2769370675086975,-0.13330355286598206,-0.21839939057826996,0.0980074405670166,0.10290840268135071,0.12074458599090576,-0.2942551374435425,0.118377685546875,0.1387370526790619,0.02922212705016136,-0.09831567108631134,0.09792119264602661,0.09870405495166779,-0.2859035134315491,-0.12150934338569641,0.15822744369506836,0.11582888662815094,0.11839212477207184,0.10077372193336487,0.11233697831630707,-0.29346367716789246,0.14472827315330505,-0.28979045152664185,-0.22249141335487366,0.10218504071235657,0.12010274827480316,-0.2545264959335327,-0.29542800784111023,0.11864499747753143,-0.1733485758304596,0.13565637171268463,-0.2712149918079376,0.12423911690711975,-0.03869074583053589,0.1209803968667984,0.06594154238700867,0.11948718130588531,0.08928532898426056,-0.18687143921852112,0.07100367546081543,-0.04901483654975891,0.02037135884165764,-0.07930111885070801,0.12857094407081604,-0.2017606794834137,0.03176568076014519,0.11300806701183319,0.07892933487892151,0.11783164739608765,0.022402670234441757,0.12199240922927856,0.006916940212249756,0.148775115609169,-0.2942448854446411,-0.28101950883865356,0.12518656253814697,-0.03997424244880676,-0.27058300375938416,-0.19156530499458313,0.10835911333560944,0.11047980189323425,-0.27058300375938416,-0.18819016218185425,-0.29249686002731323,0.11929439008235931,0.12230280041694641,0.14336620271205902,0.11765016615390778,0.12128174304962158,-0.28468072414398193,0.11864499747753143,-0.025300681591033936,-0.16270336508750916,-0.1443890780210495,0.13805818557739258,0.123769611120224,-0.26976028084754944,0.1263609528541565,0.10035943984985352,-0.21966320276260376,-0.24453268945217133,0.152986541390419,0.15886446833610535,0.12908686697483063,0.0038836225867271423,-0.04399248957633972,-0.12126205861568451,-0.22137466073036194,-0.2941456735134125,0.04216552898287773,-0.0059899091720581055],\"type\":\"scatter3d\"}],                        {\"scene\":{\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false},\"zaxis\":{\"visible\":false}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('90cbb7a1-ee05-45f4-82b3-959ec0209b9a');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f941de25-346f-4fa1-8386-82112d493793\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f941de25-346f-4fa1-8386-82112d493793\")) {                    Plotly.newPlot(                        \"f941de25-346f-4fa1-8386-82112d493793\",                        [{\"marker\":{\"color\":\"red\",\"size\":1},\"mode\":\"markers\",\"x\":[-0.1226106658577919,0.011447638273239136,0.197329580783844,-0.17580962181091309,0.049790408462285995,-0.16470883786678314,0.013498768210411072,0.030419938266277313,0.049874842166900635,-0.1464485377073288,-0.008764135651290417,0.001856047660112381,0.16160200536251068,-0.11367503553628922,0.20087184011936188,-0.1377694457769394,-0.00013384316116571426,-0.06920124590396881,-0.11708687990903854,0.0712016224861145,0.10584411025047302,-0.022785626351833344,-6.476603448390961e-05,-0.14791488647460938,0.12193208932876587,0.06920113414525986,0.13950832188129425,0.10285400599241257,-0.15642178058624268,0.05931578949093819,-0.007276969961822033,-0.15714091062545776,-0.08601223677396774,0.20228739082813263,0.035460107028484344,-0.049503568559885025,-0.1673758178949356,-0.17323322594165802,0.10552987456321716,-0.17225557565689087,0.12035295367240906,0.06169245392084122,-0.13962624967098236,-0.15083591639995575,0.19884280860424042,0.06283475458621979,-0.026424922049045563,0.0671662762761116,-0.11629106849431992,0.06157388165593147,-0.012973195873200893,-0.11275354027748108,0.14262959361076355,0.0009033074602484703,0.1342267245054245,0.0718524232506752,-0.1643008291721344,-0.15242452919483185,0.01784845069050789,-0.08875220268964767,0.07672625035047531,0.15019963681697845,0.19519712030887604,0.04771142452955246,0.08225157111883163,0.0838242843747139,-0.016209546476602554,0.19846799969673157,-0.023530609905719757,-0.07048751413822174,-0.042179856449365616,-0.03850289434194565,0.16734552383422852,0.022563859820365906,0.004390530288219452,0.047533705830574036,0.07447725534439087,-0.13445723056793213,-0.1360858678817749,0.13437941670417786,-0.09712855517864227,-0.1116824820637703,-0.10599711537361145,-0.11521609872579575,-0.13321717083454132,-0.17879125475883484,0.17784078419208527,-0.00723994430154562,-0.02849668636918068,-0.17775189876556396,0.04321318119764328,0.1274968981742859,-0.11335789412260056,-0.10201828181743622,0.09225578606128693,-0.11524506658315659,-0.14058594405651093,-0.17510871589183807,-0.12519192695617676,-0.007566140033304691,-0.10784215480089188,0.0023727603256702423,-0.15290690958499908,0.17548446357250214,0.15721766650676727,0.15061135590076447,-0.10909146070480347,0.13904953002929688,-0.17740662395954132,-0.07603581249713898,0.19785170257091522,0.14729969203472137,-0.16881521046161652,-0.11025264859199524,-0.027609024196863174,0.10226788371801376,-0.12118399888277054,-0.11870143562555313,-0.0706971287727356,0.053041186183691025,0.12102022022008896,0.0792548879981041,-0.0395207516849041,0.05362378805875778,-0.1711982637643814,-0.09694793075323105,-0.14602331817150116,-0.1787821501493454,0.0673142746090889,0.08641234040260315,-0.03168720379471779,-0.1522032469511032,0.18027178943157196,0.0009190430864691734,-0.031400345265865326,0.0913335531949997,-0.1222928985953331,-0.13583403825759888,0.17432528734207153,0.02780609205365181,0.0018655024468898773,0.061827149242162704,0.17450210452079773,-0.024123452603816986,0.19579774141311646,-0.1175985112786293,-0.15313555300235748,0.12504230439662933,0.12339963018894196,0.16998277604579926,0.06832272559404373,-0.1521511822938919,0.18764100968837738,0.1801815927028656,0.14719751477241516,-0.1565621942281723,-0.09897544234991074,0.09547356516122818,0.1767582893371582,-0.11142750829458237,0.12033028900623322,0.12202713638544083,-0.02935778722167015,-0.11780288070440292,0.07060305774211884,-0.10234091430902481,-0.16307619214057922,0.1209731251001358,-0.1457737684249878,0.002752726897597313,-0.1586301028728485,0.15915311872959137,-0.07110913842916489,-0.09895101934671402,0.1718873828649521,0.17421747744083405,0.20150193572044373,0.19233764708042145,0.1764805018901825,0.11173955351114273,0.0008463691920042038,0.04542551189661026,-0.17969077825546265,0.029114849865436554,0.18675917387008667,-0.16894353926181793,-0.018089786171913147,0.0027579190209507942,0.16442731022834778,0.16861076653003693,0.04051433503627777,-0.07579798996448517,0.1211332380771637,0.13805270195007324,0.08497679233551025,-0.10517344623804092,0.17623484134674072,-0.15800610184669495,-0.10668883472681046,0.041480615735054016,-0.05524919927120209,-0.15797622501850128,-0.1579052209854126,0.025379016995429993,0.11117337644100189,0.06347515434026718,0.1851036548614502,-0.0689934566617012,0.1845541000366211,0.19128504395484924,0.1135721206665039,-0.06067035347223282,0.11443287879228592,0.1907183676958084,0.1980983167886734,0.028321348130702972,0.042989715933799744,-0.006034753285348415,-0.14711719751358032,-0.1534264087677002,0.14245252311229706,-0.0012145116925239563,-0.1577427238225937,-0.16184701025485992,0.08721061795949936,0.1433468759059906,-0.028839867562055588,0.17873434722423553,-0.05224316567182541,-0.12851755321025848,-0.1500532627105713,0.18635757267475128,-0.14441686868667603,0.12831276655197144,0.199436217546463,0.15552344918251038,0.03903739154338837,-0.0706486627459526,0.11317674815654755,0.14220212399959564,0.05077913776040077,-0.018040858209133148,-0.14440299570560455,0.06896355003118515,0.16244348883628845,0.06177942082285881,0.05423804372549057,-0.12120208889245987,-0.04468206688761711,-0.06950134038925171,-0.027342669665813446,-0.05049243941903114,0.200534388422966,-0.12781459093093872,0.0044910479336977005,0.047438766807317734,0.1287572830915451,-0.03497990965843201,0.112284354865551,0.0033917659893631935,0.11093991249799728,0.0816693902015686,-0.016752544790506363,0.1864117532968521,0.15370358526706696,0.1841479241847992,0.1757635623216629,-0.1743958443403244,0.1353694200515747,-0.04542861506342888,0.1646236926317215,0.0994584858417511,0.15967248380184174,0.15171533823013306,0.10037005692720413,0.1989690214395523,-0.1546754688024521,-0.008284947834908962,0.14795254170894623,0.10543763637542725,0.13702285289764404,-0.12016158550977707,0.17557308077812195,0.053033292293548584,-0.15514174103736877,0.18546192348003387,-0.07241392135620117,0.17147383093833923,0.11102727055549622,0.11813616007566452,0.15046244859695435,-0.12199050933122635,-0.040688637644052505,0.06672647595405579,-0.17897367477416992,-0.07796645909547806,-0.12310250848531723,0.021836262196302414,-0.12384083122015,0.17630092799663544,0.12051466852426529,0.06860334426164627,0.17214654386043549,0.0674402117729187,-0.0589427575469017,-0.013484937138855457,-0.0954369306564331,0.023472953587770462,0.17379625141620636,0.04411635547876358,-0.15920522809028625,-0.01603059470653534,-0.019074223935604095,0.20172668993473053,0.08814419060945511,0.17597240209579468,0.014810492284595966,-0.14117148518562317,-0.09152058511972427,-0.07637378573417664,-0.15557453036308289,0.18081077933311462,0.04952661693096161,-0.051276177167892456,0.017586782574653625,0.18810684978961945,-0.06737198680639267,0.17552518844604492,0.010779234580695629,-0.17343047261238098,0.1969110071659088,-0.01885957270860672,-0.17518256604671478,0.19182591140270233,0.16110582649707794,-0.17873883247375488,0.028534989804029465,0.058974530547857285,0.11058993637561798,-0.09874072670936584,-0.1721220314502716,-0.007083593867719173,-0.04948856309056282,-0.16570782661437988,0.09612512588500977,-0.13042233884334564,0.05673796311020851,-0.17291095852851868,0.16521628201007843,-0.03093618154525757,-0.17893297970294952,-0.17609284818172455,-0.17574161291122437,0.06340358406305313,0.182180255651474,-0.17638815939426422,-0.17130222916603088,-0.14439165592193604,0.03400665521621704,0.12864075601100922,-0.028320513665676117,-0.07226381450891495,0.14478127658367157,0.1405918151140213,-0.15912571549415588,-0.03669167309999466,0.050140365958213806,0.20101211965084076,0.11782916635274887,-0.15593461692333221,0.08881714195013046,0.1400660276412964,0.19068552553653717,0.17781512439250946,0.10370755940675735,0.0337604358792305,0.03691989928483963,0.18913914263248444,0.09248371422290802,-0.019757062196731567,0.017608756199479103,0.09289717674255371,-0.02802671119570732,-0.15551505982875824,0.005180083680897951,-0.17290017008781433,-0.16306114196777344,0.08763444423675537,0.09558168798685074,-0.17481335997581482,-0.1094353049993515,0.1103564202785492,0.10878877341747284,0.19253839552402496,0.08645934611558914,-0.16177332401275635,-0.12146828323602676,0.054480765014886856,0.05405691638588905,-0.09288337081670761,0.1992003470659256,0.015655338764190674,0.01598348841071129,0.00812429841607809,0.08299725502729416,0.06386644393205643,-0.03938229754567146,0.19891680777072906,0.006137466989457607,-0.01798310875892639,-0.1735863834619522,-0.17902517318725586,0.0030035628005862236,-0.10844351351261139,0.06056678295135498,-0.0693688616156578,-0.07921324670314789,-0.180974543094635,0.05262136086821556,-0.05111932009458542,-0.10047495365142822,0.15811583399772644,-0.12435755878686905,-0.14143215119838715,-0.034529417753219604,-0.058005690574645996,-0.07856302708387375,0.19351983070373535,-0.168567955493927,-0.15253478288650513,-0.010347082279622555,0.08444914221763611,-0.008223007433116436,-0.04139985144138336,0.20039544999599457,0.0003384621813893318,-0.13514384627342224,0.0017437413334846497,0.09663960337638855,0.19473746418952942,-0.01600637659430504,0.14824163913726807,-0.17527084052562714,0.17400221526622772,0.14293211698532104,0.11100988835096359,0.11449296027421951,0.09828074276447296,-0.17742307484149933,0.18135038018226624,0.1975412517786026,-0.1051478460431099,-0.07569138705730438,0.08385742455720901,0.10789192467927933,0.05437186732888222,-0.06695730239152908,0.007884751074016094,0.005549294408410788,-0.01532695721834898,0.1247139498591423,0.16697634756565094,-0.16572698950767517,-0.06108321249485016,0.15016937255859375,0.01953936740756035,-0.051318854093551636,0.02065390720963478,0.19676563143730164,-0.06410590559244156,-0.0877784788608551,0.13593387603759766,0.19207234680652618,-0.06793095171451569,0.02588997781276703,0.00304210651665926,0.13475301861763,0.18849113583564758,-0.06258793920278549,0.07784374803304672,0.1969914585351944,0.19763919711112976,-0.16452275216579437,-0.17781773209571838,-0.04538053274154663,-0.15116070210933685,0.19657950103282928,0.12691842019557953,-0.04617198929190636,0.03328186273574829,0.17873233556747437,-0.0759131908416748,0.09973534941673279,0.1945851445198059,0.07669279724359512,0.12401578575372696,-0.08846139907836914,-0.17610757052898407,0.13816995918750763,0.1593971699476242,-0.03518916666507721,-0.09879489988088608,-0.10148724168539047,-0.13411414623260498,-0.05136800557374954,0.040880586951971054,-0.13668416440486908,-0.16060607135295868,0.19442634284496307,-0.02772173285484314,-0.10709072649478912,-0.03637494891881943,0.18371544778347015,-0.1395498365163803,0.1452447921037674,-0.10703185200691223,-0.02318965271115303,0.19683575630187988,0.17602527141571045,-0.12810401618480682,-0.0417599081993103,0.15173837542533875,-0.14805029332637787,-0.09947416186332703,0.11192193627357483,0.08188496530056,-0.03027309477329254,-0.06835481524467468,0.18292008340358734,0.0818527489900589,-0.058351755142211914,-0.15166108310222626,-0.05139080435037613,0.19680656492710114,0.16463881731033325,-0.14993859827518463,0.07241703569889069,-0.13459807634353638,0.1060090959072113,-0.05330071598291397,0.09359090775251389,0.10107411444187164,-0.17191822826862335,-0.09096518158912659,-0.05040441080927849,0.19965410232543945,-0.007997599430382252,0.056066207587718964,0.19263134896755219,0.197311133146286,-0.1752464324235916,-0.0063904887065291405,0.10479610413312912,-0.004290995188057423,0.07929040491580963,0.03250494599342346,0.09573837369680405,0.16379939019680023,-0.17667703330516815,-0.07160550355911255,0.13683660328388214,0.04693399369716644,0.17206348478794098,0.057776354253292084,-0.16030217707157135,0.19888055324554443,0.0005219001322984695,0.16850917041301727,-0.10148429125547409,0.0031660133972764015,0.06099141389131546,0.06332743167877197,-0.1741332709789276,-0.1241680309176445,0.17487947642803192,0.17237703502178192,0.0834309309720993,-0.1548927128314972,0.18114498257637024,0.19942012429237366,0.1701042503118515,-0.17917263507843018,-0.1522117555141449,-0.1504945456981659,0.2001490741968155,0.1506354659795761,0.022315556183457375,0.054189447313547134,-0.0489460788667202,0.1953413486480713,0.03862574324011803,0.06848914921283722,-0.041961733251810074,-0.17522530257701874,-0.02650465816259384,-0.07410629838705063,-0.17554739117622375,0.12596960365772247,0.010861078277230263,0.19700683653354645,-0.18088267743587494,-0.17558349668979645,0.12963443994522095,-0.06248725950717926,-0.06188011169433594,-0.027492880821228027,0.1988542228937149,0.14897960424423218,0.09088195860385895,-0.1649652123451233,-0.14633804559707642,-0.025156144052743912,0.15596525371074677,0.08293947577476501,0.016208408400416374,0.10605667531490326,-0.17538413405418396,-0.06834883987903595,-0.14017149806022644,-0.1411161720752716,-0.09236491471529007,-0.16798186302185059,0.11867301166057587,-0.0241839699447155,0.1971530169248581,0.19002889096736908,-0.14556536078453064,-0.14181683957576752,0.06442314386367798,-0.1515776664018631,-0.16449317336082458,0.1481541395187378,0.07132801413536072,-0.1456228494644165,-0.15439322590827942,0.1996811181306839,-0.05000276118516922,0.10548745840787888,0.18668271601200104,0.14158307015895844,-0.15160538256168365,-0.17443934082984924,-0.03576100990176201,0.20164696872234344,0.14864006638526917,0.20045925676822662,0.20266906917095184,-0.0032009659335017204,-0.1513555496931076,-0.03573797643184662,-0.07844077050685883,-0.06132715195417404,0.15738537907600403,-0.1607050895690918,-0.17388306558132172,-0.1429724097251892,-0.16757257282733917,0.012982252985239029,0.005820605903863907,-0.1726139336824417,0.19086800515651703,0.10291291028261185,0.09157472848892212,0.1814700961112976,-0.08005581796169281,0.07283856719732285,-0.16242928802967072,0.04546976834535599,-0.1624666005373001,-0.10349305719137192,-0.11171086132526398,0.040074266493320465,0.06857281178236008,-0.02880990132689476,0.06077824905514717,-0.14845342934131622,-0.11966220289468765,-0.1295318305492401,0.013539689593017101,0.15320321917533875,-0.07313411682844162,0.19566676020622253,-0.1571795642375946,0.11837805807590485,-0.1683637648820877,-0.1513916403055191,0.16786082088947296,-0.14878153800964355,0.1795320361852646,-0.004264459945261478,0.13252976536750793,-0.07828312367200851,0.1591527909040451,-0.1738799661397934,0.029147878289222717,-0.0962650403380394,-0.045812103897333145,0.13419470191001892,0.16998684406280518,0.048159338533878326,0.16418521106243134,0.12479061633348465,-0.17712973058223724,0.052767664194107056,-0.16236886382102966,-0.016632355749607086,-0.06360983103513718,0.19650594890117645,0.18917138874530792,0.17939311265945435,0.1847519874572754,-0.16775117814540863,0.025257673114538193,-0.1538579761981964,-0.06224638968706131,0.15894286334514618,-0.00918483454734087,-0.17454388737678528,0.05013043433427811,0.12753215432167053,-0.0734000876545906,-0.15583308041095734,-0.0409119613468647,0.1748620867729187,-0.07298532873392105,0.07306413352489471,-0.14454317092895508,-0.16236509382724762,0.008131278678774834,0.022718004882335663,0.18591639399528503,0.0665891021490097,0.00866864062845707,0.09983563423156738,-0.06287232041358948,-0.1755751222372055,-0.061333246529102325,-0.02083422616124153,-0.08294600248336792,-0.10718227922916412,-0.16117921471595764,0.20011267066001892,0.1528325080871582,-0.013997458852827549,-0.09080386161804199,-0.06481647491455078,0.06062925234436989,-0.16589592397212982,-0.11921224743127823,0.15559114515781403,-0.17937812209129333,0.09747200459241867,0.1833103448152542,0.20368562638759613,0.15134771168231964,-0.04355168715119362,-0.13188154995441437,-0.04991726204752922,0.1498595029115677,0.20031970739364624,0.16751180589199066,0.04237786680459976,-0.16050240397453308,-0.14390724897384644,0.05691744014620781,0.1737745851278305,-0.031224612146615982,0.1574947088956833,0.07554026693105698,-0.010683502070605755,0.1946835219860077,0.17899471521377563,0.0013379808515310287,0.08872300386428833,-0.1205633357167244,-0.13823071122169495,-0.1755591332912445,0.19944487512111664,-0.05834690481424332,-0.11736763268709183,0.11971390247344971,-0.17153111100196838,-0.09932821244001389,0.1360371857881546,-0.09604611247777939,0.12786291539669037,-0.013984263874590397,-0.007301456294953823,-0.06585996598005295,0.17338813841342926,0.10638616234064102,-0.09322672337293625,-0.13680832087993622,-0.1283678114414215,-0.13839420676231384,-0.15920184552669525,0.12779861688613892,-0.07864445447921753,0.06280657649040222,-0.17550143599510193,-0.14536283910274506,0.19122891128063202,-0.14634373784065247,0.04451896622776985,-0.17897257208824158,0.17348712682724,0.1051265075802803,-0.08559425920248032,0.01190413162112236,-0.179292693734169,0.07816615700721741,0.058548007160425186,-0.009347443468868732,-0.17783048748970032,-0.14752840995788574,0.12422779202461243,-0.04879220202565193,-0.17424309253692627,-0.13692063093185425,-0.16451698541641235,0.018199091777205467,0.18460266292095184,0.11572752147912979,0.19225475192070007,-0.1501934975385666,0.10135629028081894,-0.1716877818107605,0.19586428999900818,0.1717253029346466,0.18043208122253418,-0.17544540762901306,-0.05167585611343384,0.15869095921516418,0.14062747359275818,-0.03708401694893837,-0.10544189065694809,0.09742080420255661,-0.09095809608697891,-0.17381170392036438,-0.09197468310594559,-0.12990830838680267,-0.16637301445007324,-0.14772310853004456,-0.09462063014507294,-0.14949147403240204,-0.0436733104288578,0.011365038342773914,0.002343198284506798,0.12046858668327332,-0.1441497504711151,-0.002110588364303112,-0.14388728141784668,-0.020743638277053833,0.011992956511676311,-0.0346161425113678,0.11855137348175049,-0.11060193181037903,0.18327844142913818,0.1513005793094635,0.06861499696969986,-0.15654335916042328,0.07349841296672821,-0.17697970569133759,0.045475251972675323,0.004931336268782616,-0.03967274725437164,-0.16059228777885437,0.034660935401916504,-0.16721250116825104,0.05319151654839516,-0.004789439029991627,0.04825612157583237,-0.15873083472251892,0.08155683428049088,-0.1558355987071991,-0.15373530983924866,0.09980310499668121,-0.10247556865215302,-0.16127225756645203,0.1573137491941452,0.09011980146169662,-0.1374417543411255,-0.05740521848201752,0.16382525861263275,-0.12331954389810562,-0.16118881106376648,0.17086373269557953,-0.08688540756702423,-0.009244726039469242,0.06523452699184418,-0.1761329472064972,-0.08851597458124161,-0.1761203110218048,-0.16935302317142487,0.09114743024110794,-0.16890059411525726,-0.14783647656440735,-0.013919918797910213,0.05837220326066017,0.1251833438873291,0.1424911469221115,0.04472433030605316,0.12787897884845734,0.09339194744825363,0.19029870629310608,-0.1733662337064743,0.20241600275039673,-0.06899623572826385,-0.15029728412628174,0.1997305005788803,-0.11159419268369675,0.15957850217819214,-0.053438715636730194,-0.13378460705280304,0.019764605909585953,0.015131926164031029,-0.17567525804042816,0.10967430472373962,0.08499713987112045,-0.11020896583795547,0.18355408310890198,-0.030601482838392258,-0.014518273063004017,0.1509917974472046,-0.17794257402420044,-0.1166333481669426,-0.07630994915962219,-0.1489022672176361,-0.11519696563482285,0.10156270861625671,-0.08628357946872711,-0.10462363064289093,0.1266048401594162,-0.14407172799110413,-0.06118316948413849,0.1859465390443802,0.03860004246234894,-0.01422008778899908,-0.047062695026397705,0.10860169678926468,0.20066410303115845,-0.08550728857517242,0.0477876216173172,-0.05785314738750458,0.039086662232875824,-0.16179925203323364,0.1084873303771019,0.13248243927955627,0.0005558636039495468,0.12891894578933716,0.172816202044487,0.18613572418689728,-0.1760966032743454,-0.17303983867168427,-0.12015213817358017,-0.038002658635377884,-0.08721240609884262,-0.1567191481590271,0.13600677251815796,0.00825686939060688,-0.06416863203048706,-0.0454070009291172,-0.1621595174074173,-0.09453468769788742,-0.04208776354789734,0.0018870346248149872,-0.021294206380844116,0.18119525909423828,0.202621191740036,-0.042569998651742935,-0.1228320524096489,-0.056908637285232544,-0.026817090809345245,0.022118857130408287,0.15616120398044586,0.18568557500839233,-0.1751016527414322,-0.09469866752624512,-0.09725547581911087,0.007416123058646917,-0.09682099521160126,0.03998567909002304,-0.07847130298614502,0.17265908420085907,0.07804805785417557,0.06577786803245544,-0.07016907632350922,0.0972881019115448,0.13294874131679535,0.13195230066776276,0.08764636516571045,-0.16053053736686707,-0.16937009990215302,-0.16835333406925201,0.04600178450345993,0.020820746198296547,-0.1753217577934265,-0.09053753316402435,-0.048648398369550705,-0.04028579965233803,-0.17904487252235413,-0.17489074170589447,-0.17748987674713135,-0.0034624133259058,-0.0242728590965271,-0.122127003967762,0.12514744699001312,0.026359211653470993,0.19055615365505219,-0.14722944796085358,0.11543666571378708,-0.11323536187410355,0.043560851365327835,-0.1601289063692093,0.18322229385375977,0.20041389763355255,0.06434106081724167,0.1571250706911087,0.08463799208402634,0.10181943327188492,-0.0516209751367569,-0.13389600813388824,0.17505355179309845,0.04886166378855705,0.1771714985370636,0.1313847005367279,0.18768605589866638,0.10674571990966797,0.20156237483024597,0.16388821601867676,-0.16248343884944916,0.19086746871471405,-0.1561518758535385,0.19020576775074005,0.12608569860458374,-0.16616950929164886,0.03691978380084038,-0.06461963057518005,-0.09110607951879501,0.16722792387008667,-0.10052094608545303,-0.17327997088432312,0.039987701922655106,0.02586299553513527,-0.15185989439487457,0.017961330711841583,0.1730949729681015,-0.15187856554985046,0.06391867250204086,-0.13809072971343994,-0.1741907298564911,-0.007899307645857334,-0.1220461055636406,-0.12394765764474869,0.17239272594451904,0.16938164830207825,0.1260737031698227,-0.08053271472454071,0.18658842146396637,-0.09144948422908783,0.1875346153974533,-0.1524624079465866,0.007679743692278862,0.013104880228638649,0.1015145480632782,0.12023074179887772,0.06876353174448013,0.14646197855472565,-0.13666416704654694,0.19458556175231934,0.19289672374725342,0.12420527637004852,-0.0457070916891098,-0.06140153110027313,-0.0027294540777802467,0.12344806641340256,-0.11700353771448135,0.17468607425689697,0.19640995562076569,0.17243173718452454,-0.059501782059669495,0.008994309231638908,0.17899487912654877,0.15152472257614136,-0.0836397111415863,-0.06302652508020401,-0.16255630552768707,-0.179337278008461,-0.02675176039338112,0.07449263334274292,-0.03537442907691002,0.060530439019203186,-0.178036168217659,0.20077772438526154,0.17044280469417572,0.07699950039386749,-0.03459971398115158,0.006136447656899691,-0.1426476389169693,0.06006869673728943,0.0918494462966919,0.01545027270913124,-0.08896638453006744,0.07949396967887878,0.1694898009300232,-0.0935455858707428,0.12611480057239532,-0.15120410919189453,-0.17059651017189026,0.1767209768295288,0.19976933300495148,-0.1196598932147026,0.016302440315485,0.013113414868712425,-0.03862980753183365,-0.07775501906871796,0.002258780412375927,0.18263447284698486,0.09960388392210007,-0.17693673074245453,0.048967305570840836,-0.01545496191829443,0.13898363709449768,0.19963982701301575,-0.12174739688634872,0.01562831923365593,0.17681482434272766,-0.05197995901107788,0.19828206300735474,-0.17171725630760193,0.007816766388714314,-0.02064971625804901,0.0715106949210167,-0.1462734490633011,0.16715741157531738,-0.13741526007652283,0.19828763604164124,0.12184378504753113,-0.13957566022872925,0.028118450194597244,-0.13393554091453552,0.06563536822795868,0.03189451992511749,0.10283966362476349,-0.1755877584218979,0.1697814017534256,0.05306330323219299,-0.04962275177240372,0.014988286420702934,0.057505980134010315,-0.17665712535381317,-0.051214419305324554,0.16473503410816193,-0.15134942531585693,-0.15698738396167755,0.06507288664579391,0.08399691432714462,-0.022126860916614532,0.13262224197387695,0.19875794649124146,0.01748756319284439,-0.17868192493915558,0.1280769556760788,0.13508401811122894,-0.1596655547618866,0.04178853705525398,-0.16889549791812897,-0.1410229206085205,-0.14693771302700043,0.03549565374851227,-0.13506470620632172,-0.12473436444997787,-0.07993215322494507,-0.0161406509578228,0.1885373741388321,0.12139230966567993,-0.1424676924943924,-0.1597016304731369,-0.07451511174440384,0.059952907264232635,-0.054539382457733154,0.08642880618572235,0.13984709978103638,-0.09560981392860413,-0.14950676262378693,-0.07658298313617706,0.13614368438720703,0.15211358666419983,-0.06025063991546631,-0.10908830910921097,0.13550496101379395,0.19857622683048248,0.13672398030757904,0.1539684385061264,0.17606838047504425,0.19908204674720764,0.07846037298440933,0.17691266536712646,0.14770159125328064,0.09881913661956787,0.08828116953372955,0.013427427969872952,-0.003951436839997768,0.1170363649725914,0.04491588845849037,0.060339272022247314,0.1318681687116623,-0.1436922550201416,0.15192784368991852,-0.03038278967142105,0.08094853162765503,-0.16391146183013916,-0.12682108581066132,-0.1634300947189331,0.17208124697208405,0.1304553896188736,-0.11149711161851883,-0.10999434441328049,0.17469348013401031,-0.16753622889518738,0.10068103671073914,0.16483242809772491,0.18230414390563965,0.19141878187656403,-0.16707441210746765,0.1831531673669815,0.07402179390192032,0.16805262863636017,0.17190243303775787,0.04961799830198288,0.15437206625938416,0.1765328347682953,0.13875746726989746,-0.015049631707370281,-0.13840650022029877,0.20090000331401825,0.14866003394126892,0.07713861763477325,-0.1532094031572342,0.1528075635433197,-0.11657651513814926,-0.05973265320062637,0.08573799580335617,0.19896601140499115,0.11782287806272507,0.021398309618234634,0.0001753130927681923,0.07798927277326584,0.11432074755430222,-0.12336335331201553,-0.15420855581760406,-0.14624232053756714,-0.09830491244792938,0.17822881042957306,0.09995380789041519,-0.06528860330581665,0.19911351799964905,0.014488501474261284,-0.14288268983364105,-0.17069441080093384,-0.1625591218471527,-0.04203879088163376,-0.020779062062501907,-0.11290856450796127,-0.17051121592521667,-0.16374953091144562,-0.04967320337891579,-0.17326714098453522,-0.10963679850101471,-0.16908524930477142,-0.10218130052089691,-0.14969372749328613,-0.058007679879665375,0.037453632801771164,-0.11378776282072067,0.056533828377723694,0.06844156235456467,0.20078647136688232,0.1854303479194641,0.01905054599046707,0.17639252543449402,0.16964343190193176,-0.08615066111087799,0.1531013399362564,-0.09797760099172592,0.014099260792136192,0.12308970838785172,0.14775720238685608,-0.16381052136421204,-0.14242106676101685,-0.17984260618686676,0.1821962147951126,-0.0820838138461113,0.19660553336143494,-0.02150079980492592,-0.1765090376138687,0.058979615569114685,0.1555873155593872,0.18799924850463867,0.17727483808994293,0.0903756394982338,0.1797732412815094,-0.1306435763835907,0.14890550076961517,-0.01490220706909895,-0.10394992679357529,0.17086642980575562,0.020477306097745895,-0.024401631206274033,0.1261633038520813,-0.12754596769809723,0.16451533138751984,-0.17559874057769775,0.07353909313678741,0.20243553817272186,-0.06754344701766968,-0.10070694983005524,0.17068961262702942,0.17912504076957703,-0.17774991691112518,-0.1492675244808197,0.029058631509542465,0.19680659472942352,-0.11297962069511414,-0.06342103332281113,-0.14711320400238037,-0.14453741908073425,-0.0019599078223109245,-0.09868527948856354,-0.040524568408727646,-0.15599441528320312,0.12694866955280304,0.06717413663864136,-0.17625854909420013,0.056169651448726654,0.09404321014881134,0.02575521543622017,-0.0868445485830307,0.07296206057071686,0.18644459545612335,-0.00413344893604517,0.02860887348651886,-0.1624552607536316,0.02249527908861637,-0.14812757074832916,-0.02384888008236885,0.10547402501106262,0.1688268780708313,-0.11081834882497787,-0.15118323266506195,-0.1778142899274826,0.11884985119104385,0.09847096353769302,-0.17140650749206543,-0.13840769231319427,-0.17621104419231415,-0.16650214791297913,0.13948439061641693,0.056170396506786346,0.006035823840647936,-0.028968479484319687,0.19519539177417755,-0.13227686285972595,-0.08781827986240387,-0.05054676532745361,-0.09524444490671158,-0.03173078969120979,-0.1252427101135254,-0.11383602768182755,0.1518307328224182,0.028623126447200775,-0.1517847180366516,-0.031321264803409576,-0.1429460048675537,-0.01563243195414543,0.15621939301490784,-0.1690499186515808,-0.1530032902956009,-0.06163445860147476,-0.06466329097747803,0.05455894023180008,-0.14214196801185608,0.12243238091468811,0.1991569995880127,-0.14819729328155518,0.03828945383429527,0.14669543504714966,0.19946140050888062,0.06752245128154755,-0.050998784601688385,0.09868579357862473,0.11365724354982376,0.19220341742038727,0.19800011813640594,-0.17530681192874908,0.016954749822616577,-0.023976199328899384,-0.0008574994280934334,-0.17026187479496002,-0.11817564815282822,-0.0430559366941452,-0.1307181715965271,-0.14547507464885712,0.08317580819129944,-0.17620734870433807,0.16931648552417755,-0.03402448818087578,0.10249704122543335,0.18361850082874298,-0.07072938233613968,0.10961014777421951,0.07378368079662323,0.028346583247184753,-0.020714398473501205,0.1296938806772232,0.10123445838689804,0.2032555192708969,-0.0687338337302208,-0.1300804316997528,-0.17548327147960663,0.1599433720111847,0.0015667257830500603,0.19875003397464752,0.1816895604133606,0.025006458163261414,-0.13914591073989868,0.07840423285961151,-0.011557727120816708,0.09196273982524872,0.1849476397037506,0.18227609992027283,0.06187231466174126,0.1935192197561264,-0.06999035179615021,0.1722722053527832,0.19826582074165344,0.05745229125022888,0.19634243845939636,0.08732160925865173,-0.15150532126426697,0.05301961675286293,-0.11782311648130417,0.20165127515792847,-0.14542213082313538,0.19355368614196777,-0.08085396885871887,-0.14106670022010803,-0.1145523265004158,-0.1420927494764328,0.07027073204517365,0.12329364567995071,-0.17808517813682556,0.09674519300460815,-0.11153828352689743,-0.11698824912309647,-0.14352434873580933,-0.10227350145578384,0.1501593440771103,0.1976173371076584,-0.10335464775562286,0.13543032109737396,0.16709938645362854,-0.11804120987653732,0.18993820250034332,-0.014408565126359463,0.12735290825366974,-0.09233489632606506,0.007449728902429342,0.20231559872627258,-0.10555557906627655,-0.004807711578905582,-0.11822942644357681,-0.16633211076259613,-0.06678254157304764,-0.17624887824058533,-0.15162181854248047,0.07872174680233002,0.010486848652362823,-0.09306840598583221,0.09642663598060608,0.10274684429168701,-0.1704748421907425,0.020197588950395584,0.08059051632881165,0.15894505381584167,0.15437377989292145,-0.12777066230773926,0.04307848960161209,0.12463664263486862,0.0010722000151872635,0.13237714767456055,-0.07216879725456238,0.009718877263367176,0.13212642073631287,0.018345410004258156,0.19867384433746338,-0.1106073334813118,-0.0869762971997261,0.14929507672786713,0.1948898732662201,0.15606805682182312,0.04520420357584953,-0.023774772882461548,0.1257639229297638,0.12990400195121765,-0.1003129705786705,0.08548608422279358,0.14347539842128754,-0.1769268661737442,-0.10454484075307846,0.1444118618965149,-0.1457574963569641,-0.16244682669639587,-0.1770351082086563,-0.1369425505399704,-0.1739075779914856,-0.16260015964508057,-0.10495131462812424,0.09222813695669174,-0.16599829494953156,0.16514922678470612,0.10239414125680923,-0.16478858888149261,0.1788601279258728,-0.15282462537288666,-0.10171115398406982,-0.17897042632102966,0.03349030762910843,-0.11443986743688583,-0.01202180702239275,-0.1458905190229416,-0.10499048233032227,-0.1683627814054489,-0.09117499738931656,-0.06318069994449615,0.1875690370798111,-0.15627804398536682,0.11946015805006027,-0.001999080181121826,-0.15069182217121124,0.19755291938781738,0.07672590017318726,-0.17551428079605103,-0.02797621488571167,0.07665740698575974,0.17858289182186127,-0.05356963723897934,-0.157185360789299,0.19798406958580017,-0.028725937008857727,-0.09926290065050125,0.09469351172447205,-0.0024404870346188545,0.10707078874111176,0.10232141613960266,-6.960052996873856e-05,-0.06159468740224838,0.03368110582232475,-0.15086336433887482,-0.14218547940254211,-0.17085526883602142,0.1810624897480011,0.06924403458833694,-0.09547311812639236,-0.17975562810897827,0.13954396545886993,0.0449553020298481,-0.16167809069156647,-0.01973838359117508,-0.15000438690185547,-0.07476095110177994,0.09308747947216034,0.06989709287881851,0.01763719692826271,0.05546654015779495,0.0862049087882042,-0.15443716943264008,0.1932114213705063,-0.0045218029990792274,-0.009945235215127468,-0.14935776591300964,-0.163791686296463,0.16984723508358002,-0.11198306828737259,-0.1486334204673767,0.06943649798631668,0.1716087907552719,0.16500838100910187,-0.08693899214267731,0.025011185556650162,0.09531289339065552,-0.04995916038751602,-0.15027785301208496,-0.04012846201658249,-0.15783047676086426,-0.06417863070964813,-0.009268845431506634,0.18606755137443542,-0.0921454057097435,0.16214138269424438,0.12719124555587769,-0.1486632525920868,0.15948927402496338,-0.11955256015062332,-0.15978692471981049,0.12615229189395905,0.10490317642688751,0.006542245391756296,-0.10410104691982269,-0.07226649671792984,-0.13132253289222717,0.15978336334228516,-0.1555587500333786,0.038659077137708664,0.11278993636369705,0.1813313364982605,-0.15921296179294586,-0.08229618519544601,0.067190982401371,-0.012190324254333973,-0.15324395895004272,0.16909343004226685,0.17940379679203033,0.07948602735996246,-0.13419373333454132,-0.15325412154197693,0.19722820818424225,-0.15526945888996124,-0.09342175722122192,0.2041207253932953,0.1800418645143509,0.03517499193549156,-0.08972208201885223,-0.10656166821718216,-0.12880288064479828,0.18523617088794708,-0.006834031082689762,-0.0028810594230890274,-0.10608019679784775,-0.04848397150635719,0.16252051293849945,0.11041595041751862,-0.00027291569858789444,-0.08062970638275146,0.15643267333507538,0.17616459727287292,0.19284909963607788,0.053506698459386826,0.03701247274875641,0.08676111698150635,0.06981481611728668,-0.12609590590000153,-0.16104669868946075,0.1045861542224884,0.06834165006875992,-0.17916171252727509,-0.053001679480075836,-0.15085844695568085,-0.025327075272798538,-0.1499701738357544,0.10826434940099716,0.1740252822637558,-0.11895044893026352,-0.12018582969903946,-0.15562860667705536,-0.049356285482645035,0.1976451426744461,-0.16876079142093658,-0.004929724149405956,-0.11011815816164017,-0.020476292818784714,-0.034338679164648056,-0.11681332439184189,0.008331714197993279,-0.175641730427742,0.1498899906873703,0.00120489951223135,-0.09152601659297943,-0.13288380205631256,0.1677127182483673,0.07594459503889084,-0.151826873421669,0.041012633591890335,-0.16271483898162842,0.0428362675011158,-0.17548233270645142,0.15815861523151398,0.0996842235326767,0.19057568907737732,-0.05175621062517166,0.19072458148002625,-0.12513995170593262,-0.07119527459144592,0.011140106245875359,-0.15874971449375153,0.19661013782024384,0.1311725676059723,-0.07557403296232224,-0.11381498724222183,0.1507190763950348,-0.08154145628213882,-0.1027211993932724,-0.17989982664585114,0.0015142997726798058,0.04851514846086502,0.19817689061164856,0.07996197789907455,-0.17153404653072357,-0.17424090206623077,0.11939734220504761,0.0752839520573616,-0.0769800990819931,-0.009592373855412006,0.04058506339788437,-0.15842196345329285,0.0802048072218895,-0.09275023639202118,-0.166164368391037,-0.05937764793634415,0.17304085195064545,0.005892535205930471,0.00211926456540823,0.12521882355213165,0.19117236137390137,0.17474792897701263,0.17259001731872559,-0.05005933344364166,-0.0217585489153862,0.1103883758187294,0.0540463924407959,0.19081301987171173,-0.17735669016838074,0.009193431586027145,-0.16185492277145386,0.16689875721931458,0.18038910627365112,-0.022128432989120483,0.14556868374347687,0.02971896529197693,0.11195576936006546,-0.1285644769668579,0.02659796178340912,0.20243240892887115,0.19681644439697266,-0.1173359826207161,0.1272226721048355,-0.17164188623428345,-0.0476827509701252,0.08079469949007034,0.1736915111541748,0.08779255300760269,-0.14207279682159424,-0.11437626928091049,-0.08230496942996979,-0.16524739563465118,-0.17543400824069977,-0.0616561621427536,-0.16886423528194427,-0.1593361645936966,0.1932690590620041,-0.13606704771518707,-0.1370055228471756,0.06139104813337326,-0.15486523509025574,-0.15949934720993042,0.009444194845855236,-0.006667827256023884,-0.1077554002404213,-0.08190407603979111,-0.17920178174972534,-0.08995422720909119,-0.13794289529323578,0.15937577188014984,0.15930266678333282,-0.00561013538390398,-0.17275336384773254,-0.04407386854290962,0.03153448924422264,-0.04767436534166336,0.1933290660381317,-0.16844117641448975,0.004575911909341812,-0.14900429546833038,0.07990998029708862,-0.04603949189186096,0.003261944279074669,-0.17502331733703613,-0.15200331807136536,-0.043699659407138824,0.17116984724998474,0.0465458445250988,0.09485341608524323,-0.150260791182518,-0.0335746705532074,0.13921737670898438,0.1575622707605362,0.17216715216636658,0.1632527858018875,0.07734498381614685,0.07094308733940125,0.03977092728018761,-0.17601633071899414,0.15203672647476196,-0.13851846754550934,0.040834683924913406,0.09277891367673874,0.01283850148320198,-0.08886509388685226,0.07103527337312698,-0.10873211175203323,0.18314287066459656,-0.15522974729537964,0.06763442605733871,0.033866751939058304,-0.09817609190940857,-0.13732007145881653,0.20123009383678436,0.17786389589309692,0.11920901387929916,0.09025975316762924,0.17742647230625153,-0.17671553790569305,0.10858163982629776,-0.0870504230260849,0.07861141860485077,0.18674951791763306,0.05495796725153923,0.13729266822338104,0.07872884720563889,0.17635348439216614,0.19577644765377045,0.20273789763450623,0.19411055743694305,-0.06867523491382599,-0.11619294434785843,-0.11493165045976639,0.06384926289319992,-0.16425231099128723,0.1746857762336731,0.1515132039785385,-0.05703200399875641,-0.09498094022274017,0.19464047253131866,0.15806591510772705,0.1485031396150589,0.20220786333084106,-0.07991008460521698,-0.006347152404487133,0.04088706523180008,-0.0613866001367569,-0.12704981863498688,-0.15090039372444153,0.11211950331926346,-0.014494807459414005,0.08761817961931229,-0.09894454479217529,0.17523397505283356,0.12309837341308594,0.04504049941897392,0.0755632147192955,-0.1456492841243744,-0.01935238391160965,-0.142042875289917,-0.17819775640964508,-0.14614173769950867,-0.13752509653568268,-0.006027570925652981,0.08748389780521393,-0.17850615084171295,-0.10074371099472046,-0.0758572667837143,0.1916467696428299,-0.1289857178926468,0.015567444264888763,0.17510680854320526,0.16624942421913147,0.12127528339624405,-0.1707678735256195,0.0689750537276268,-0.03499516844749451,-0.0051643131300807,-0.04139472916722298,-0.01096444670110941,-0.11090869456529617,0.01692952960729599,0.16752898693084717,0.1303805112838745,0.031671471893787384,-0.015743549913167953,-0.1578066498041153,0.13917964696884155,0.17672264575958252,0.17000074684619904,-0.0583343431353569,-0.17619077861309052,0.17567913234233856,0.08773636817932129,-0.07047747075557709,0.17414847016334534,0.0501055046916008,-0.1053030863404274,0.028710942715406418,0.10769399255514145,-0.018871400505304337,0.10880335420370102,-0.14531929790973663,0.0777922049164772,0.15908460319042206,0.1912374049425125,0.11735878884792328,0.10892967134714127,0.008597777225077152,0.07557718455791473,0.17809990048408508,-0.014001608826220036,-0.054640993475914,-0.02305947244167328,-0.11946874111890793,-0.1505538374185562,-0.07900752872228622,0.17259782552719116,-0.048308633267879486,-0.037136197090148926,0.05890392139554024,0.09848923981189728,-0.1767224669456482,0.1871815025806427,0.15082789957523346,-0.047800928354263306,0.03063322976231575,-0.10017946362495422,0.035486429929733276,0.04126562550663948,0.18631140887737274,-0.1136835440993309,-6.997212767601013e-05,0.04149460792541504,-0.1254345178604126,0.053050849586725235,-0.002966996282339096,-0.07005596905946732,-0.17490242421627045,0.11517558246850967,0.03282344341278076,-0.14035041630268097,0.16115616261959076,0.1614101231098175,0.022992338985204697,-0.1718491017818451,-0.1325986236333847,0.18632756173610687,-0.09551208466291428,0.13540057837963104,0.09036286175251007,-0.044455189257860184,0.06234343722462654,0.1925126612186432,-0.11859453469514847,0.19281822443008423,0.1261320412158966,0.1844283938407898,0.16748929023742676,0.06058146059513092,-0.17080365121364594,0.025677964091300964,0.16790436208248138,-0.1195652112364769,0.057152677327394485,-0.17699392139911652,0.1941819041967392,-0.1783907264471054,0.1872042864561081,0.1563872992992401,-0.08236522227525711,-0.05187477171421051,0.19818279147148132,0.18147167563438416,-0.08634745329618454,0.148939311504364,-0.08601043373346329,-0.10521648824214935,-0.02564849704504013,-0.14082516729831696,-0.12599943578243256,-0.036662377417087555,-0.15261028707027435,-0.16244329512119293,0.03303557634353638,0.09427381306886673,-0.1182388886809349,-0.09139920771121979,0.1236615777015686,-0.08885890990495682,-0.16062496602535248,-0.15178915858268738,-0.17130303382873535,-0.011475431732833385,-0.09148333966732025,0.027048926800489426,0.17551691830158234,-0.006551194004714489,0.13470765948295593,0.1970093846321106,0.01307632401585579,-0.09020931273698807,0.06630208343267441,0.12387265264987946,0.2006174921989441,0.1632852852344513,-0.12456781417131424,-0.1374295949935913,-0.035912830382585526,0.18896813690662384,-0.15132702887058258,-0.07559999823570251,0.17169584333896637,-0.17415498197078705,-0.16845378279685974,0.022567905485630035,0.009792466647922993,-0.16446241736412048,0.053606610745191574,0.17466072738170624,0.1226293072104454,0.17932969331741333,-0.026892706751823425,0.01741059310734272,0.10775599628686905,-0.17138320207595825,-0.08182985335588455,-0.1493404358625412,-0.12438604980707169,-0.02067829668521881,0.19475461542606354,-0.17343884706497192,-0.020469844341278076,0.04993150010704994,0.19886605441570282,0.0966874212026596,0.04840054735541344,0.15150274336338043,-0.05168069899082184,0.1496744155883789,0.08458214998245239,0.1389034539461136,-0.08887914568185806,-0.07823113352060318,-0.11806025356054306],\"y\":[-0.057597752660512924,-0.05164617672562599,-0.06793075054883957,-0.27980536222457886,-0.025965377688407898,-0.040667321532964706,-0.03574186563491821,-0.07344979792833328,-0.011184102855622768,-0.13333304226398468,-0.006876177154481411,0.04878338798880577,-0.010315953753888607,-0.047867342829704285,-0.19369958341121674,0.17456434667110443,0.3710618019104004,0.04268304631114006,-0.04534360021352768,0.08509381115436554,-0.04326970875263214,0.026776093989610672,-0.06729834526777267,-0.006118657998740673,-0.04919012635946274,-0.04168440029025078,-0.0066129909828305244,-0.04681664705276489,-0.39413905143737793,-0.07243827730417252,-0.055445197969675064,-0.04662749916315079,-0.04940348118543625,0.38948938250541687,-0.06099193915724754,0.03420240804553032,-0.05859851464629173,-0.07258053869009018,-0.07246112078428268,-0.06887774914503098,-0.07532420754432678,-0.03183630853891373,-0.0720442607998848,-0.03172890841960907,0.4075317978858948,0.2572717070579529,-0.006656252779066563,0.16286106407642365,-0.044765621423721313,-0.07583469897508621,-0.07360086590051651,0.257625937461853,-0.006450180895626545,-0.03401549160480499,0.013630121015012264,-0.04819810763001442,-0.06036313995718956,-0.3233817219734192,0.16035626828670502,-0.048182643949985504,-0.03393660858273506,-0.04449576139450073,-0.09958050400018692,-0.03387235477566719,-0.07541404664516449,0.3631918728351593,-0.008184850215911865,0.03201409429311752,0.05138874053955078,-0.0735245943069458,-0.007031841203570366,-0.04799038916826248,-0.06151951476931572,0.044046368449926376,-0.04719929024577141,-0.008280453272163868,0.043456461280584335,-0.03479919210076332,-0.050047289580106735,-0.008564949035644531,-0.07816880941390991,-0.00831941794604063,0.2792814075946808,-0.04509005323052406,-0.07388462126255035,-0.3327028155326843,-0.2759068012237549,-0.050023581832647324,-0.04692183434963226,0.20297633111476898,-0.04734393209218979,-0.04616415500640869,-0.07516659051179886,-0.04475608095526695,-0.054921284317970276,-0.005443284288048744,-0.056154582649469376,0.23144792020320892,-0.05184100195765495,-0.03889426216483116,0.3195819556713104,-0.005375944077968597,-0.0054704369977116585,-0.3825239837169647,-0.049976740032434464,-0.05022948980331421,-0.01692664995789528,0.31306958198547363,0.29919612407684326,-0.03972386196255684,-0.038971249014139175,0.26552438735961914,-0.04687480628490448,0.08396473526954651,-0.04496796429157257,-0.039108652621507645,-0.052139315754175186,-0.07266809046268463,0.16127115488052368,0.3840106725692749,-0.07752685248851776,-0.053631898015737534,0.28648942708969116,-0.0644112378358841,-0.08876042813062668,0.37651580572128296,-0.07457767426967621,-0.2277679443359375,-0.03414836898446083,0.18963518738746643,-0.007162337191402912,-0.21459603309631348,-0.07105235755443573,-0.046685926616191864,0.3467530906200409,0.19080525636672974,-0.008739491924643517,-0.04707345739006996,-0.048931293189525604,-0.04707116633653641,0.3944656550884247,0.07330574840307236,0.13095980882644653,-0.045262690633535385,-0.03312209993600845,-0.07241987437009811,-0.05089527741074562,-0.075364850461483,-0.05602842569351196,-0.0857200026512146,-0.006411182694137096,-0.2604742646217346,-0.34706196188926697,-0.05228189751505852,-0.07331835478544235,-0.05079607665538788,0.21297688782215118,0.2542398273944855,-0.006358537822961807,-0.07269749045372009,0.18350966274738312,-0.006573472172021866,-0.01082765031605959,-0.04699842259287834,-0.07279561460018158,-0.04460323229432106,-0.4055420756340027,-0.04845283180475235,-0.05170713737607002,-0.07309228926897049,-0.045584045350551605,-0.049608487635850906,-0.07174040377140045,-0.007326182909309864,-0.11126375198364258,-0.07373856753110886,-0.05480854585766792,0.35813504457473755,-0.009091220796108246,0.260683536529541,0.03571532666683197,0.050004083663225174,-0.025805683806538582,-0.03405192121863365,0.16134175658226013,-0.05973490700125694,-0.042473383247852325,-0.04495501145720482,-0.007957952097058296,-0.07060904800891876,-0.037178102880716324,-0.048738036304712296,-0.0499500036239624,-0.008072910830378532,0.2626974880695343,0.1860644370317459,-0.139685720205307,-0.049034420400857925,-0.010827519930899143,0.010430959053337574,-0.07656046003103256,-0.04385547339916229,0.3603641390800476,-0.008387413807213306,-0.007025282829999924,0.26298773288726807,0.0520363450050354,-0.007609819993376732,-0.27830833196640015,-0.37999022006988525,-0.07289756089448929,0.07237415760755539,-0.004732775501906872,0.23534443974494934,-0.22326573729515076,0.3757901191711426,0.0797300636768341,-0.0379064604640007,-0.024156196042895317,-0.00757807120680809,-0.0447847843170166,-0.07149402052164078,-0.06861700117588043,0.298537939786911,-0.00708802230656147,-0.014742177911102772,-0.006437012925744057,0.4147198796272278,-0.04603055492043495,0.07009866088628769,0.31607019901275635,-0.04720164090394974,-0.03640071302652359,-0.009726152755320072,-0.0539165735244751,-0.016865424811840057,0.3044130802154541,0.037565961480140686,0.41107994318008423,-0.031745024025440216,-0.057278599590063095,0.03142521530389786,0.2511928677558899,0.23351514339447021,-0.04450322687625885,-0.05062553659081459,-0.07682056725025177,0.12689559161663055,-0.04661655053496361,0.026978569105267525,-0.07444356381893158,-0.021139776334166527,-0.014726952649652958,-0.07263670116662979,-0.04552942514419556,-0.04729044809937477,-0.046413496136665344,0.07076764106750488,-0.006952420808374882,-0.047768689692020416,-0.0078893406316638,0.05406228080391884,0.23183195292949677,-0.030548568814992905,0.24236363172531128,0.27061590552330017,0.36801376938819885,-0.05634414777159691,-0.07105112820863724,0.24084626138210297,-0.00791480764746666,-0.0459805466234684,-0.04339287057518959,-0.006494659930467606,0.3454475700855255,-0.4013095796108246,-0.14654769003391266,-0.04734080657362938,0.360629677772522,-0.05310872197151184,-0.008908902294933796,-0.07497101277112961,-0.19094109535217285,0.12189067900180817,-0.07592102885246277,0.3646927773952484,-0.07333368808031082,-0.044876161962747574,-0.06769891828298569,-0.06778623908758163,-0.07281658053398132,0.13866068422794342,0.013542352244257927,-0.00602967943996191,-0.04566286504268646,-0.07289472222328186,-0.07327714562416077,0.4050523042678833,0.11777905374765396,-0.34795650839805603,-0.07506446540355682,-0.07561592757701874,-0.357215940952301,-0.044561099261045456,0.3612852096557617,-0.03603257238864899,-0.020013919100165367,0.34092819690704346,0.05897562950849533,-0.07202734798192978,-0.050382111221551895,-0.008282607421278954,-0.04361835494637489,-0.255027174949646,-0.021965041756629944,-0.04279930889606476,-0.07487481832504272,-0.042846355587244034,-0.04524001106619835,-0.006319562904536724,-0.1568785160779953,-0.01985081657767296,-0.048496123403310776,-0.05276952311396599,0.3969001770019531,-0.06675243377685547,-0.04233488440513611,-0.014310256578028202,-0.07426229864358902,-0.04361584410071373,-0.04027774557471275,0.04940081387758255,0.39221659302711487,-0.0202613715082407,0.36932241916656494,-0.29379183053970337,-0.04938741773366928,-0.04350554570555687,-0.07646747678518295,-0.0450667068362236,-0.3920634090900421,-0.05020405352115631,0.2017713040113449,-0.1104670837521553,0.04869924485683441,0.16378650069236755,0.2249252051115036,-0.2828589975833893,-0.039608847349882126,0.3954276144504547,-0.035744551569223404,-0.05216865614056587,0.0750412791967392,-0.012195962481200695,0.1434967815876007,-0.06814558058977127,-0.006378684192895889,-0.15440143644809723,-0.07421021908521652,-0.05331968516111374,-0.05359430983662605,-0.0727933868765831,0.0031466204673051834,-0.046705011278390884,-0.19664867222309113,-0.04995711147785187,0.30467283725738525,-0.04824428632855415,0.0980200469493866,-0.015141730196774006,-0.04819757491350174,-0.009542539715766907,-0.005478329956531525,0.3959777355194092,-0.008004192262887955,0.15961091220378876,0.3118068277835846,-0.17265239357948303,0.37925124168395996,-0.0510302372276783,-0.007509293034672737,-0.07686098664999008,0.08849238604307175,0.36642083525657654,-0.07265789061784744,-0.06938107311725616,-0.07647097855806351,-0.06494901329278946,0.04108066484332085,-0.1605188250541687,-0.007611907087266445,-0.07454562187194824,-0.05151180550456047,0.19759833812713623,0.05949738249182701,-0.3892136514186859,0.3216738998889923,-0.0519377775490284,0.09922848641872406,-0.049519650638103485,-0.3390626311302185,0.1336968094110489,0.3630330264568329,-0.005945372395217419,-0.07499773800373077,0.26172590255737305,-0.0068311383947730064,-0.06994438171386719,-0.04640302062034607,-0.03949199616909027,-0.39293235540390015,-0.07001437246799469,-0.008386624976992607,-0.026398101821541786,-0.03938262537121773,-0.029334668070077896,-0.02669662795960903,-0.06944155693054199,-0.009047292172908783,0.33248522877693176,0.2611505389213562,-0.041933268308639526,0.28485921025276184,-0.051590461283922195,-0.007071192376315594,-0.0752084031701088,-0.07307591289281845,-0.2992091476917267,0.30375194549560547,-0.30790454149246216,-0.07282475382089615,-0.06326255947351456,0.059247128665447235,0.4233696460723877,-0.01996566727757454,0.027615340426564217,0.23686453700065613,-0.00856927689164877,-0.009235907346010208,0.020924698561429977,-0.05001327022910118,-0.07502934336662292,-0.17260465025901794,-0.049961987882852554,-0.023469651117920876,-0.008284728042781353,0.06908704340457916,-0.03806895762681961,-0.07311743497848511,-0.037323616445064545,-0.052458059042692184,0.1673358529806137,-0.048040617257356644,-0.04250757023692131,-0.006980896927416325,-0.04521569237112999,-0.017600977793335915,-0.0064645204693078995,-0.00805737730115652,-0.04650823771953583,-0.04667700082063675,0.30762484669685364,-0.05360576882958412,-0.07590670883655548,0.14363352954387665,-0.07182852178812027,-0.025037769228219986,-0.008625995367765427,-0.051978033035993576,0.15931732952594757,0.06806600093841553,-0.014541187323629856,-0.08557992428541183,-0.07370275259017944,0.38977107405662537,-0.05007730796933174,0.07409784197807312,-0.07508555799722672,-0.00658505130559206,-0.003739116946235299,-0.0747656524181366,-0.062364231795072556,-0.16121169924736023,0.14560474455356598,-0.04176352918148041,-0.07294245064258575,-0.03948597237467766,0.008407335728406906,-0.012752177193760872,-0.04599800333380699,-0.10650032758712769,-0.04800460487604141,-0.07350167632102966,-0.40526747703552246,-0.007388300262391567,-0.07374100387096405,-0.05419991537928581,-0.2924439013004303,0.0437871590256691,-0.051482584327459335,-0.07742567360401154,-0.041477348655462265,-0.07119757682085037,0.18321241438388824,-0.07540204375982285,0.17949871718883514,-0.07391153275966644,0.014317946508526802,0.3806440830230713,-0.06206672266125679,-0.07334093004465103,-0.03978950157761574,0.2753216624259949,-0.07629183679819107,-0.006517430767416954,-0.042725589126348495,-0.010766218416392803,-0.03872404247522354,-0.33235979080200195,-0.006556962616741657,-0.0043943049386143684,-0.008001669310033321,-0.041744112968444824,0.12943613529205322,0.4056011736392975,0.32560497522354126,-0.07403064519166946,-0.008810398168861866,0.3834024965763092,-0.07426244020462036,-0.015369092114269733,-0.1430738866329193,-0.07348395884037018,-0.032892096787691116,-0.05062656104564667,-0.12367337197065353,-0.03847230598330498,-0.006345490925014019,0.013764849863946438,0.36853355169296265,-0.07262656837701797,-0.045409370213747025,-0.2221803069114685,-0.008037043735384941,0.12962709367275238,-0.03914637863636017,0.04357428103685379,-0.05470747873187065,-0.02363247610628605,0.3136916160583496,-0.11069604754447937,-0.04601017385721207,-0.005445571616292,-0.049250856041908264,-0.009795268066227436,0.08736054599285126,-0.04361335188150406,0.11355656385421753,0.07681605964899063,0.0779849961400032,-0.04341678321361542,-0.07486286014318466,-0.1007714495062828,0.27993276715278625,-0.0460723377764225,-0.23661141097545624,0.2042834609746933,-0.13679330050945282,-0.07093960791826248,0.19041883945465088,0.28207454085350037,0.05718937888741493,-0.19898271560668945,-0.045113977044820786,-0.15354372560977936,-0.2950272262096405,-0.008385762572288513,-0.2881026566028595,-0.2798214256763458,-0.014868914149701595,-0.04803889989852905,-0.05922657996416092,0.260460764169693,-0.04641486704349518,-0.3886711597442627,-0.057194627821445465,-0.07312178611755371,-0.07459267228841782,-0.007772919721901417,-0.06851910054683685,-0.007677377201616764,-0.008293126709759235,-0.010340689681470394,0.3533591330051422,0.35245758295059204,-0.006214172579348087,0.33627524971961975,-0.05125879496335983,-0.007565883919596672,-0.04745718464255333,-0.023459751158952713,-0.01720387674868107,0.2219209223985672,-0.04676748067140579,-0.05576154217123985,0.2930784821510315,-0.036794692277908325,-0.005816206336021423,-0.04419942572712898,0.1649806648492813,-0.04323900118470192,-0.012243947945535183,-0.007098094560205936,0.02575196884572506,-0.07378597557544708,-0.023016002029180527,-0.05672760307788849,-0.04751986637711525,0.17948348820209503,-0.044225871562957764,0.07922480255365372,-0.036931347101926804,0.3224518895149231,-0.07242073118686676,0.28231048583984375,-0.0660579726099968,-0.07210605591535568,0.07293520122766495,-0.008479507640004158,-0.006936514750123024,-0.3957267701625824,0.2877723276615143,0.32860666513442993,-0.008412202820181847,-0.2068955898284912,-0.06436488777399063,-0.006012695841491222,-0.017944563180208206,0.2045106142759323,0.32445859909057617,-0.04961739480495453,-0.16673947870731354,0.24360492825508118,0.145418182015419,-0.006347229704260826,-0.3862709403038025,0.20025679469108582,-0.038670603185892105,-0.1212732270359993,0.014715624041855335,0.38003525137901306,-0.015478977002203465,-0.07258960604667664,-0.05421481281518936,-0.17061161994934082,0.0940830409526825,-0.0731680616736412,0.3662635385990143,-0.007494294084608555,-0.1198374405503273,-0.3293834328651428,-0.010190214030444622,-0.0067771682515740395,0.4117127060890198,-0.07224030047655106,-0.046473730355501175,-0.3407667279243469,-0.011352229863405228,0.3448902368545532,0.20709185302257538,-0.04805610328912735,-0.04683354124426842,-0.05035053566098213,-0.043946996331214905,-0.06053717061877251,-0.13338136672973633,-0.045162633061409,-0.050633274018764496,-0.07423269748687744,-0.05304857715964317,0.07171206176280975,-0.014350092969834805,-0.05142660439014435,0.3353038430213928,-0.05937753990292549,-0.19218841195106506,0.15144222974777222,-0.0533379502594471,-0.023685596883296967,-0.045828595757484436,0.31698644161224365,0.36713093519210815,-0.049478720873594284,0.011019167490303516,0.0401468500494957,0.2999405860900879,-0.044439155608415604,-0.07270561158657074,-0.07194401323795319,-0.07372437417507172,-0.04305431619286537,-0.007289862260222435,-0.23931816220283508,0.10870535671710968,-0.21190591156482697,-0.0678984522819519,-0.043300844728946686,-0.028763972222805023,-0.04536712169647217,-0.23235605657100677,-0.33722126483917236,-0.3205990493297577,0.08355589210987091,0.3080209493637085,0.13006290793418884,-0.06857029348611832,0.34671637415885925,-0.03382989019155502,-0.07633551955223083,-0.047478772699832916,-0.07314640283584595,0.22875115275382996,-0.007755966857075691,-0.045693833380937576,-0.04202789440751076,-0.04374938830733299,0.15062090754508972,-0.07972808927297592,-0.07462650537490845,-0.007603779435157776,-0.04969458281993866,0.07553234696388245,-0.05330818146467209,-0.03821121156215668,0.31718453764915466,-0.1289428472518921,0.050103843212127686,-0.030967392027378082,-0.05288958549499512,-0.04852956905961037,-0.008464038372039795,-0.035626061260700226,-0.006500863470137119,0.28131982684135437,0.35055187344551086,-0.0736083835363388,-0.005353426560759544,0.41253462433815,0.2362278550863266,-0.021184813231229782,-0.3068265914916992,-0.025620363652706146,0.07690051198005676,-0.0019318978302180767,0.17563046514987946,-0.07515444606542587,-0.006230443716049194,-0.07277431339025497,0.10142511874437332,-0.1677992045879364,0.3920409679412842,-0.005943428725004196,0.2071228325366974,-0.05082423612475395,-0.07662791758775711,0.12632501125335693,0.23171676695346832,-0.0056245336309075356,0.13458938896656036,-0.05378459766507149,0.35852691531181335,-0.24838504195213318,0.3430137634277344,0.0903116911649704,-0.005963911302387714,-0.01064715813845396,-0.1671593189239502,-0.30191290378570557,-0.07094696909189224,-0.07327082008123398,-0.05239267274737358,-0.031001057475805283,0.3422244191169739,-0.007608336396515369,-0.039259444922208786,0.13318321108818054,0.30242738127708435,-0.006966323591768742,-0.07301487028598785,-0.13731354475021362,-0.007236757315695286,-0.007437204942107201,-0.031356021761894226,-0.008851589635014534,-0.006555578671395779,-0.01050548255443573,-0.04502948746085167,0.0864332765340805,0.3859395682811737,-0.4007096588611603,-0.050294339656829834,-0.005983303301036358,-0.008424417115747929,0.16419342160224915,-0.09505477547645569,-0.31654393672943115,-0.05091604217886925,-0.04690450057387352,-0.006511695683002472,0.20447568595409393,-0.04393351823091507,-0.05349813774228096,-0.0612608902156353,-0.06596275418996811,-0.07398710399866104,-0.04299939051270485,0.404738187789917,-0.04313763231039047,-0.008891834877431393,-0.02415972575545311,0.05521061643958092,-0.07619696110486984,-0.021077118813991547,-0.20075304806232452,0.24967829883098602,0.05634131282567978,-0.07280467450618744,-0.16666212677955627,-0.05466347560286522,-0.05382872372865677,-0.07038542628288269,-0.07358977198600769,-0.049356892704963684,-0.04437324032187462,-0.044492416083812714,-0.070358507335186,-0.03092005103826523,0.2271241843700409,-0.05012693628668785,-0.04561822488903999,-0.0067726001143455505,-0.030255910009145737,0.023249583318829536,0.07641619443893433,0.3008161783218384,-0.053329408168792725,-0.04906115680932999,-0.07639273256063461,-0.030764736235141754,-0.04811456799507141,-0.049352649599313736,-0.17720939218997955,-0.005633578635752201,0.038502585142850876,0.3562586307525635,-0.07297555357217789,0.3194647431373596,-0.07002579420804977,0.3891860246658325,-0.03776261582970619,-0.36183983087539673,-0.04512478783726692,-0.06240158900618553,0.11989960074424744,-0.014342890121042728,-0.00835058931261301,-0.07464571297168732,-0.05138863995671272,-0.0661126971244812,-0.013388033956289291,-0.01845213584601879,-0.04842674359679222,0.027495896443724632,0.3598676025867462,-0.07217121869325638,-0.15870028734207153,-0.07119736075401306,0.022920211777091026,-0.14225763082504272,-0.007837303914129734,-0.05090806633234024,-0.06950338184833527,0.39824724197387695,-0.07315303385257721,-0.0652998685836792,-0.18457135558128357,-0.07017993927001953,0.17866216599941254,-0.005178301595151424,-0.04740181192755699,-0.050472941249608994,-0.07634381949901581,0.2579291760921478,0.06894544512033463,0.1872657835483551,-0.028903117403388023,0.10000757873058319,-0.07398765534162521,-0.05026935413479805,-0.03101883828639984,0.035214632749557495,-0.07876814901828766,0.008467275649309158,-0.00887542124837637,-0.04892341420054436,0.4126957058906555,-0.26609423756599426,-0.04970616474747658,-0.061126384884119034,0.07280848920345306,0.17308828234672546,0.3992207646369934,-0.04917781054973602,-0.04489601030945778,-0.04494525119662285,0.3023163974285126,-0.06797857582569122,-0.04587944597005844,-0.057273298501968384,-0.046050895005464554,-0.11179545521736145,-0.04419264569878578,0.35731226205825806,-0.03544635698199272,-0.3065033555030823,-0.07329367101192474,-0.004788893274962902,0.04687899351119995,0.3551560938358307,-0.007966903038322926,0.36023736000061035,-0.044426079839468,0.28358474373817444,-0.07144981622695923,0.1960204839706421,-0.051518913358449936,-0.047579143196344376,-0.04385177791118622,0.2629557251930237,-0.04640617594122887,-0.06827720999717712,-0.051023613661527634,-0.048944320529699326,-0.07412083446979523,-0.04980151727795601,-0.28066903352737427,0.05595284700393677,0.23347552120685577,0.20534494519233704,-0.05215552821755409,-0.05589514225721359,-0.02046271041035652,-0.016718171536922455,-0.26964348554611206,0.03220454975962639,0.23845604062080383,-0.050923410803079605,-0.04720291495323181,-0.0436648391187191,-0.04733048006892204,-0.0722493901848793,0.3283107876777649,-0.055808983743190765,0.2985701560974121,-0.016473332419991493,-0.0471673458814621,0.15341831743717194,-0.04782550781965256,-0.20467104017734528,0.16905294358730316,0.3157530426979065,-0.04426277428865433,-0.06111336871981621,-0.04472211375832558,-0.007402786053717136,-0.0974641665816307,-0.328732967376709,0.1748841255903244,0.09176384657621384,-0.04046899825334549,-0.04334898293018341,-0.0727727860212326,-0.05923354625701904,-0.07553624361753464,-0.0454082153737545,0.2076418399810791,-0.045494839549064636,0.2560325562953949,-0.04420176148414612,-0.026705065742135048,-0.07206902652978897,0.27179771661758423,-0.1917508989572525,0.09530172497034073,-0.07693925499916077,-0.049023326486349106,-0.34196534752845764,-0.041890233755111694,0.11716733872890472,-0.06964129954576492,-0.40642011165618896,-0.2165931910276413,-0.06059860810637474,-0.055540408939123154,0.26653775572776794,-0.03909248113632202,-0.009417608380317688,0.34026646614074707,-0.07185433059930801,-0.07237286865711212,-0.07521746307611465,0.253208726644516,0.2416456788778305,-0.07990517467260361,0.050139281898736954,0.07340063154697418,-0.06253059208393097,-0.07301265001296997,0.08856820315122604,-0.027682384476065636,0.4025140702724457,-0.04675556719303131,-0.2600795030593872,-0.01071219053119421,-0.3963240385055542,-0.07648555934429169,-0.019139952957630157,-0.046645574271678925,0.2191419005393982,-0.011437997221946716,0.33098021149635315,-0.24118025600910187,-0.04992220178246498,-0.008702178485691547,0.2214478999376297,0.02828003279864788,-0.04491965100169182,-0.05369735509157181,0.13420391082763672,-0.07414404302835464,-0.056839775294065475,0.40988755226135254,-0.048146311193704605,-0.0059816874563694,0.05953844264149666,0.02119104191660881,0.3295365869998932,-0.04770258441567421,-0.036512624472379684,-0.05226993188261986,-0.04230418801307678,0.058348335325717926,0.023873193189501762,-0.050377100706100464,-0.06637972593307495,-0.007692843675613403,0.15315468609333038,-0.0684838593006134,-0.1626751720905304,-0.06980708241462708,0.12116425484418869,-0.3691851794719696,0.10731293261051178,0.28731152415275574,-0.06729086488485336,0.29371634125709534,-0.045097190886735916,-0.04859662055969238,-0.06750357896089554,0.26147907972335815,-0.06078736111521721,-0.05005829408764839,-0.048661328852176666,-0.043471746146678925,0.4022824168205261,-0.04708687588572502,0.08272946625947952,0.30538517236709595,-0.05387871712446213,-0.26230695843696594,-0.07612141966819763,0.20111125707626343,-0.07138988375663757,-0.030251916497945786,-0.007933567278087139,0.14185775816440582,-0.05774713680148125,0.3219958245754242,-0.04216542840003967,-0.06902740895748138,0.06955220550298691,-0.07601471245288849,-0.03188334405422211,-0.1851993054151535,-0.04835767671465874,0.36732229590415955,0.3579879105091095,-0.07490281760692596,-0.04177992790937424,-0.03916549310088158,-0.005593515001237392,0.17229421436786652,0.3040628135204315,-0.0614902563393116,-0.008470804430544376,-0.07396817952394485,-0.07225240021944046,0.404911607503891,0.05994855612516403,-0.08746162056922913,-0.03447515890002251,0.29834645986557007,-0.045614007860422134,0.25232571363449097,-0.007201955653727055,-0.07344841957092285,-0.04391912743449211,-0.07446708530187607,-0.007245977409183979,-0.16416484117507935,-0.04733122140169144,0.20194993913173676,-0.05328642576932907,-0.2450837790966034,-0.07266541570425034,0.10859856009483337,-0.05008608102798462,-0.04454641789197922,-0.12454415112733841,-0.08764197677373886,-0.005955152213573456,-0.027603045105934143,-0.05027327314019203,0.39317646622657776,-0.07472100108861923,-0.07372955977916718,-0.14038391411304474,-0.05330498889088631,-0.04575996845960617,-0.07027774304151535,0.36619487404823303,-0.01270364597439766,-0.046981312334537506,0.15822173655033112,0.32172444462776184,0.3721783757209778,0.2175445258617401,0.04950788989663124,-0.00469150859862566,-0.07079198956489563,-0.06038856506347656,-0.07201539725065231,-0.07583624124526978,-0.06113942340016365,-0.1487353891134262,-0.07788444310426712,-0.010508915409445763,-0.008300038985908031,0.09200359880924225,-0.14736387133598328,-0.035358231514692307,-0.020680991932749748,-0.008188772946596146,-0.004941818304359913,-0.009304110892117023,-0.07288216054439545,-0.024921869859099388,0.05586725473403931,-0.056270353496074677,-0.0450383722782135,-0.07362285256385803,0.2438187301158905,-0.0499967597424984,0.3270752429962158,-0.0564558170735836,0.3386249840259552,-0.05002456158399582,-0.2598523795604706,-0.007177654653787613,-0.006498613394796848,-0.07293003797531128,0.2939246594905853,-0.056126918643713,-0.04930941015481949,-0.039881493896245956,-0.07566431909799576,0.07529692351818085,-0.010906664654612541,0.30340152978897095,0.11780910193920135,0.09888370335102081,0.28232523798942566,-0.00729794055223465,-0.052880968898534775,-0.26603445410728455,-0.2266334742307663,0.21055684983730316,-0.2002304047346115,0.17588813602924347,-0.07343322783708572,-0.04632195457816124,-0.05913637578487396,-0.05304493382573128,0.2754853367805481,-0.010739262215793133,-0.017568059265613556,0.0003397916443645954,0.047422196716070175,0.06358660012483597,0.2780507504940033,-0.04686393216252327,-0.39588767290115356,0.16934733092784882,-0.015319407917559147,0.019404584541916847,-0.07437150180339813,-0.05022282525897026,-0.07160469144582748,-0.07204371690750122,0.41734614968299866,-0.04496204853057861,-0.01151704415678978,-0.04456189274787903,-0.104654461145401,0.012301335111260414,-0.03370168060064316,-0.005570599809288979,-0.050399456173181534,-0.22632120549678802,-0.03312757983803749,0.0626184344291687,-0.07436411827802658,-0.05155710130929947,0.4022485911846161,-0.040606915950775146,-0.4017954170703888,-0.07314515858888626,-0.007175792008638382,-0.24798673391342163,-0.0069388048723340034,0.2543967664241791,-0.0072952210903167725,-0.03154301643371582,-0.27794450521469116,-0.0325237475335598,0.08218639343976974,0.40938594937324524,-0.0393691249191761,-0.04553025960922241,0.027570370584726334,-0.26887285709381104,-0.12110991030931473,0.1202288568019867,-0.05343010649085045,-0.008740552701056004,-0.01107970904558897,-0.03534182533621788,-0.045118384063243866,0.36925870180130005,-0.23891788721084595,-0.07275786995887756,0.3844870924949646,0.2085554152727127,-0.0066996412351727486,-0.15925592184066772,-0.2685999870300293,-0.0759856253862381,-0.3154987692832947,0.20880602300167084,-0.14447417855262756,-0.0738210454583168,-0.048402294516563416,-0.030335761606693268,0.346451073884964,0.29257237911224365,0.13024045526981354,-0.05214820057153702,-0.021072838455438614,-0.2042929232120514,-0.045927952975034714,0.372339129447937,0.21112976968288422,-0.035378482192754745,-0.052103109657764435,-0.049858465790748596,-0.04583099111914635,-0.05428960919380188,-0.05751568078994751,0.3485153019428253,0.09501033276319504,-0.04056235030293465,-0.07468580454587936,-0.04806200787425041,0.06077740341424942,-0.04065456986427307,-0.035342227667570114,-0.04589681327342987,-0.00751020573079586,-0.3203219473361969,-0.042393170297145844,-0.07435006648302078,-0.13343234360218048,-0.0749054104089737,0.06727788597345352,-0.004433741793036461,-0.0742097720503807,-0.04184933751821518,-0.07416083663702011,-0.005839978344738483,-0.006236765533685684,0.325732946395874,-0.07257431000471115,-0.22147497534751892,0.13215504586696625,0.360055536031723,-0.04128047451376915,-0.03635087609291077,0.06620535999536514,0.011431226506829262,0.003037826158106327,0.23885363340377808,-0.008099770173430443,-0.04546099156141281,-0.04615865275263786,0.20505017042160034,0.08079934120178223,0.042127933353185654,-0.04953654482960701,-0.04766473174095154,-0.007179507985711098,-0.07216323167085648,-0.006985265761613846,0.29178258776664734,0.03951876237988472,-0.07343661785125732,-0.04046088084578514,-0.044704362750053406,0.22673359513282776,-0.008470263332128525,-0.052072588354349136,0.2594725489616394,-0.00573921762406826,-0.05608716979622841,-0.044717296957969666,-0.30176591873168945,-0.07728387415409088,0.29420480132102966,-0.08166074752807617,-0.07304564118385315,-0.20798245072364807,0.033040162175893784,-0.04925289377570152,0.36092260479927063,-0.371625691652298,0.1258310228586197,-0.10266275703907013,0.060989804565906525,-0.020718444138765335,-0.0068126097321510315,-0.008130277507007122,-0.042449600994586945,-0.2102372944355011,0.24216388165950775,-0.006313697434961796,-0.07213244587182999,-0.050929922610521317,-0.038752779364585876,-0.008190399035811424,-0.07058706879615784,-0.07359663397073746,-0.038422469049692154,0.014840584248304367,0.3441545367240906,-0.14578741788864136,-0.00725332647562027,-0.07334014028310776,-0.29664888978004456,-0.1388828009366989,-0.014498797245323658,-0.005526884458959103,0.2715603709220886,-0.07526110112667084,-0.07476485520601273,0.25672900676727295,0.08611693978309631,-0.00559612549841404,-0.06043560430407524,-0.03744104132056236,-0.07230612635612488,-0.05376231297850609,-0.0728486105799675,0.2432747781276703,-0.028793122619390488,-0.32168450951576233,-0.054390519857406616,0.17384259402751923,-0.04928092285990715,-0.04770604893565178,-0.011159050278365612,-0.006556747481226921,0.14728064835071564,0.1142892986536026,-0.04857724532485008,-0.009020698256790638,-0.03611011058092117,-0.3140404224395752,0.15184730291366577,0.14850099384784698,0.32738012075424194,-0.00523512065410614,-0.052418261766433716,0.13602206110954285,0.2450169175863266,-0.0734124481678009,-0.05416274070739746,-0.05420079454779625,-0.06644822657108307,-0.07385285943746567,-0.007861514575779438,-0.3275591731071472,-0.059837084263563156,-0.0343766026198864,-0.04065307229757309,0.18699683248996735,-0.042254846543073654,-0.07396908849477768,-0.0068930359557271,-0.011125974357128143,-0.04628188535571098,-0.21647778153419495,-0.04917270317673683,0.09863578528165817,-0.07147818803787231,0.3719419836997986,-0.35373830795288086,-0.3440578877925873,0.22804509103298187,-0.050014518201351166,-0.04543738067150116,0.319053590297699,0.22677333652973175,-0.0728362649679184,-0.22390775382518768,-0.006474518217146397,-0.013073029927909374,-0.008782737888395786,0.3152013123035431,-0.07432268559932709,-0.011571811512112617,0.1849329024553299,-0.007273553870618343,-0.03298468515276909,-0.008914844132959843,-0.035740483552217484,0.04675399139523506,-0.04162996634840965,-0.07427290081977844,-0.041235215961933136,-0.0401134230196476,-0.07335008680820465,-0.06714236736297607,-0.04753337800502777,-0.07201019674539566,-0.39169299602508545,-0.036308541893959045,-0.03861161321401596,0.23605646193027496,-0.07420218735933304,-0.036311302334070206,-0.07269202917814255,-0.049007438123226166,0.13847088813781738,-0.39276036620140076,-0.04292306676506996,-0.18581433594226837,-0.0753263533115387,-0.04622332379221916,-0.0051850369200110435,-0.048060961067676544,-0.046991948038339615,-0.007372904568910599,-0.081512451171875,-0.04614032804965973,-0.005743380635976791,-0.051510948687791824,-0.07548347860574722,-0.00616834033280611,-0.051446497440338135,-0.04720422625541687,-0.03703081235289574,-0.04029051586985588,-0.07036369293928146,-0.04888905957341194,-0.003971392288804054,0.030697299167513847,-0.04660128429532051,-0.07429486513137817,-0.07355553656816483,-0.06327330321073532,-0.009870517998933792,-0.04611080139875412,-0.057338688522577286,-0.007579704746603966,-0.07344576716423035,-0.0518498569726944,-0.041054584085941315,0.379727303981781,0.36688232421875,-0.07034815102815628,0.24924436211585999,-0.05316510424017906,-0.07231081277132034,-0.2474043071269989,-0.19822323322296143,-0.05314754694700241,-0.0695711225271225,-0.052008818835020065,-0.04834655672311783,-0.05062532424926758,-0.07504508644342422,-0.051787152886390686,-0.006453740410506725,-0.037476811558008194,-0.11543653905391693,-0.041597168892621994,-0.006879112683236599,-0.028173737227916718,0.35890820622444153,0.4053097665309906,-0.005716750398278236,0.05537796765565872,-0.03850272297859192,-0.028522638604044914,-0.05783439427614212,0.15100376307964325,-0.016648326069116592,0.11303933709859848,0.051357172429561615,-0.07285484671592712,-0.05036618560552597,-0.07346131652593613,-0.07294020056724548,0.3847389817237854,-0.07478825002908707,-0.04407728835940361,-0.3574017286300659,-0.0747915580868721,-0.22080904245376587,-0.04939066246151924,-0.05241110175848007,-0.0074394820258021355,-0.06309143453836441,-0.0073336828500032425,-0.005539224483072758,-0.008384830318391323,0.2747287154197693,0.31626075506210327,-0.04532179236412048,-0.04098937660455704,0.31563571095466614,0.12968765199184418,-0.33191683888435364,0.02278641052544117,0.34770020842552185,-0.28839898109436035,0.3684387505054474,-0.016970975324511528,-0.038576047867536545,-0.04706509783864021,-0.011577465571463108,-0.03659597411751747,0.08527833223342896,-0.014863182790577412,-0.04738016054034233,-0.0737270787358284,-0.044786568731069565,0.07124808430671692,-0.1378660351037979,0.189075767993927,-0.07255394756793976,-0.24421045184135437,0.35657262802124023,-0.07469591498374939,-0.062411997467279434,-0.033220574259757996,-0.05736289545893669,-0.06210203841328621,0.2589314579963684,0.10335590690374374,-0.004965388216078281,0.31136491894721985,-0.06525732576847076,0.13057130575180054,-0.07098444551229477,-0.14723123610019684,-0.0455268993973732,0.3330211341381073,-0.059834692627191544,-0.009771573357284069,-0.044758182018995285,0.27854177355766296,-0.07402662932872772,-0.07288716733455658,0.04775471240282059,-0.3843494951725006,-0.07068020105361938,0.39361143112182617,-0.043553922325372696,-0.07344095408916473,-0.05542612820863724,-0.04705444723367691,-0.05022440105676651,-0.28953421115875244,-0.03947616368532181,-0.035463809967041016,-0.13505308330059052,0.3456002175807953,-0.07274730503559113,-0.04375819116830826,-0.04348225146532059,-0.3562433123588562,-0.05377722159028053,-0.20885857939720154,0.25259095430374146,-0.046683475375175476,-0.36652886867523193,-0.2994169592857361,-0.0018117842264473438,-0.051645807921886444,0.38256967067718506,-0.040566585958004,-0.07191828638315201,-0.0070562222972512245,-0.05096834525465965,-0.005995952524244785,-0.08923545479774475,-0.04744957759976387,0.2433939278125763,0.26208922266960144,0.15909074246883392,-0.07295797765254974,-0.022185908630490303,0.3823966383934021,-0.045106589794158936,-0.04695003852248192,-0.0778774842619896,-0.03603566810488701,-0.04649648442864418,-0.00845371838659048,-0.0077797649428248405,-0.004147462546825409,-0.008304926566779613,-0.13458351790905,0.01983259804546833,-0.07251168042421341,0.2585894465446472,0.20743663609027863,-0.07504551112651825,-0.04420904442667961,-0.1405484676361084,-0.045629121363162994,-0.07307907938957214,-0.046144887804985046,0.06598424911499023,-0.3867536783218384,-0.04531708359718323,-0.0995098352432251,0.33919766545295715,-0.07235852628946304,-0.013027237728238106,-0.06434718519449234,-0.05028136819601059,-0.04211312159895897,-0.010574140585958958,-0.018560482189059258,-0.04732996225357056,-0.006647912785410881,0.07930158078670502,-0.017858734354376793,0.07906249165534973,0.2529275119304657,0.042870745062828064,-0.07324997335672379,-0.054085854440927505,-0.04755425080657005,-0.038002487272024155,-0.07385554164648056,-0.013791058212518692,-0.1266486495733261,-0.06390036642551422,-0.047974783927202225,-0.03506249934434891,-0.05779658630490303,0.27926668524742126,0.05859958380460739,-0.21429595351219177,-0.05747085437178612,-0.01029147021472454,0.0887027159333229,-0.055106114596128464,0.17585524916648865,-0.07359983026981354,-0.19529932737350464,-0.07612299174070358,-0.006363558582961559,-0.26512977480888367,-0.07462643831968307,0.08310101926326752,-0.04477278143167496,-0.03971361741423607,-0.0433800183236599,-0.017933443188667297,-0.04968440532684326,-0.011655119247734547,-0.07506836205720901,0.21211934089660645,-0.07381460070610046,-0.03359462320804596,-0.0055439285933971405,0.28920960426330566,-0.030748486518859863,-0.07399318367242813,-0.02176576480269432,-0.3920435607433319,-0.2418273389339447,-0.39361926913261414,-0.0741482749581337,-0.0744844377040863,-0.07363390177488327,0.1481805443763733,-0.049076806753873825,0.10241074115037918,-0.006314857862889767,-0.35286203026771545,0.3641420602798462,-0.054824069142341614,-0.07385120540857315,-0.006441636011004448,0.3354668617248535,-0.06968925893306732,-0.041113611310720444,-0.008856081403791904,0.3559902012348175,-0.21874253451824188,-0.07369368523359299,-0.07643146812915802,-0.050529539585113525,-0.0749814510345459,-0.033464957028627396,0.2790956199169159,-0.07270971685647964,-0.04582776129245758,-0.03909798339009285,-0.009519251063466072,-0.03751291334629059,-0.033874496817588806,0.3468494713306427,-0.006904895417392254,-0.18276605010032654,-0.031985413283109665,-0.07025937736034393,-0.07140152901411057,0.008577040396630764,-0.0700697973370552,-0.01370503194630146,0.18902654945850372,0.26651886105537415,0.2622506618499756,-0.07536239922046661,-0.21969573199748993,0.05841510742902756,-0.04695016145706177,-0.04958271607756615,-0.07526661455631256,0.12706296145915985,-0.2265675663948059,0.3198409378528595,-0.049772344529628754,-0.008985709398984909,0.3947163224220276,0.3979024291038513,-0.07274126261472702,-0.1598159819841385,0.14468739926815033,-0.0538286454975605,-0.06929251551628113,-0.06455141305923462,-0.025411004200577736,-0.07378983497619629,0.33954793214797974,-0.04813150689005852,-0.05380220338702202,0.07654336094856262,-0.025633634999394417,-0.07283184677362442,-0.07165598124265671,-0.30321457982063293,-0.039404839277267456,-0.04618950933218002,0.21228612959384918,-0.0742870569229126,0.24035723507404327,-0.07228567451238632,-0.008571140468120575,-0.049373164772987366,-0.0061377305537462234,-0.052168965339660645,0.27493980526924133,-0.004405134357511997,-0.014424105174839497,0.3112484812736511,-0.05648631602525711,-0.0054892608895897865,-0.044566743075847626,0.35486307740211487,-0.006027038209140301,-0.042390428483486176,-0.3090837299823761,0.2928675711154938,0.1864023059606552,-0.32633116841316223,-0.05081530287861824,0.18241094052791595,0.22753138840198517,0.20718277990818024,-0.03238971531391144,-0.012008977122604847,-0.03134492039680481,0.19754771888256073,-0.02430565282702446,-0.07309365272521973,0.34140539169311523,-0.10680369287729263,0.01621256209909916,-0.06957849860191345,-0.07199980318546295,-0.05329682305455208,-0.03937883302569389,-0.39346495270729065,-0.051217589527368546,0.08772871643304825,-0.006739619188010693,0.15750019252300262,-0.012967327609658241,-0.04790649190545082,0.0036454149521887302,0.14325354993343353,-0.023115435615181923,-0.011217215098440647,-0.007296167314052582,-0.057288624346256256,-0.07199380546808243,-0.008690817281603813,-0.042972587049007416,-0.03706314042210579,-0.006116744130849838,0.19749748706817627,0.31593766808509827,-0.07423266023397446,-0.04041653126478195,0.024768825620412827,0.033837638795375824,-0.07232995331287384,-0.22751116752624512,-0.12449187785387039,-0.04716261103749275,-0.04733477160334587,-0.04585712030529976,-0.03346753492951393,0.011415664106607437,-0.04479597508907318,0.3515743017196655,-0.049278076738119125,0.15312282741069794,0.13420619070529938,0.17036819458007812,-0.008129173889756203,-0.28691691160202026,-0.01680712215602398,-0.06200693920254707,-0.006699671037495136,0.383143812417984,-0.07728304713964462,0.2540122866630554,0.4029007852077484,-0.06669716536998749,0.13125430047512054,-0.07246434688568115,0.20183266699314117,-0.14778375625610352,0.27655333280563354,-0.050928711891174316,-0.06550724804401398,0.20752444863319397,-0.030761219561100006,-0.057178203016519547,0.15400037169456482,0.36345890164375305,-0.2033395916223526,-0.00665574986487627,-0.07299754023551941,0.35221439599990845,-0.04269717261195183,-0.03404698893427849,-0.045202307403087616,0.07903735339641571,-0.07296637445688248,0.26171979308128357,-0.005772603675723076,0.33290091156959534,-0.045017726719379425,-0.05895315483212471,-0.05047783628106117,-0.010703143663704395,0.27463531494140625,0.1176663488149643,-0.04519404470920563,-0.00833973940461874,-0.2439125031232834,0.04117642343044281,-0.01206870935857296,-0.04629425331950188,-0.06985149532556534,-0.060187432914972305,-0.04719062149524689,0.007107960991561413,-0.3255043625831604,-0.005435502156615257,0.29654011130332947,-0.04679550975561142,-0.0724194273352623,-0.008619554340839386,0.36439937353134155,-0.20852459967136383,0.20612478256225586,-0.07337654381990433,-0.04233640804886818,0.10705601423978806,0.3733444809913635,0.15308429300785065,-0.046918511390686035,-0.02640238218009472,0.19424420595169067,-0.07488447427749634,-0.05042117461562157,0.02607102133333683,-0.07534706592559814,-0.07533375918865204,0.028910396620631218,-0.014663428999483585,-0.2153480052947998,-0.07366292923688889,-0.05032999441027641,-0.07321041822433472,-0.007077052257955074,-0.05232913792133331,-0.06085086241364479,0.16533969342708588,-0.2905939221382141,0.30302655696868896,-0.13379602134227753,0.38869908452033997,0.3592039942741394,0.2690221965312958,-0.07516349852085114,0.1853169947862625,-0.019963137805461884,-0.07262812554836273,-0.0952671468257904,-0.16326069831848145,-0.02663361094892025,-0.04950996860861778,-0.04663816839456558,-0.07020707428455353,0.006659870967268944,-0.2032492458820343,-0.055366743355989456,-0.0692414864897728,-0.0502285398542881,0.29115840792655945,0.23158982396125793,-0.05089431256055832,-0.04892400652170181,-0.04633026570081711,-0.07390657067298889,-0.20724153518676758,-0.04088081419467926,-0.0028964888770133257,0.2761389911174774,-0.02591569535434246,-0.047817885875701904,-0.04737227410078049,0.11768824607133865,-0.05523502826690674,-0.004766783677041531,-0.10154925286769867,0.2945302724838257,-0.07013415545225143,-0.04781150072813034,-0.05848481133580208,0.23201340436935425,-0.07188230752944946,-0.327676922082901,-0.06982288509607315,-0.046788860112428665,-0.00668118242174387,-0.07796087861061096,-0.11915253847837448,-0.07290889322757721,-0.006812470033764839,-0.046432968229055405,-0.07360166311264038,-0.29019609093666077,-0.11965277045965195,-0.06937918812036514,-0.05940280854701996,-0.12326071411371231,-0.030700474977493286,-0.039859283715486526,0.056583188474178314,0.37563732266426086,-0.07298637181520462,-0.36711183190345764,-0.07295596599578857,-0.08578430116176605,-0.044992074370384216,-0.04737754911184311,0.10457337647676468,-0.029073504731059074,-0.07588530331850052,-0.3014330565929413,0.06716778874397278,-0.033192478120326996,-0.014467325992882252,0.3551408648490906,0.16665947437286377,0.13514937460422516,-0.019907280802726746,-0.07129522413015366,0.0974138155579567,-0.02173776924610138,-0.04758952558040619,-0.05408325418829918,-0.047003161162137985,-0.07147710025310516,-0.008511971682310104,-0.04252399876713753,-0.04465343430638313],\"z\":[-0.21722650527954102,0.007599435746669769,-0.2863095998764038,-0.28814569115638733,-0.11912396550178528,-0.23131246864795685,-0.29568538069725037,-0.21645505726337433,0.033268462866544724,-0.26662755012512207,-0.05585213750600815,0.09487462043762207,0.09887844324111938,-0.21631081402301788,0.09754171967506409,0.13564567267894745,0.1475936472415924,0.09668470919132233,-0.04847656935453415,0.12544000148773193,-0.10437221825122833,0.11713291704654694,0.09738457202911377,0.011524233967065811,-0.07183929532766342,-0.01777302660048008,-0.13774298131465912,-0.032789647579193115,-0.274120032787323,-0.157200887799263,0.06944940984249115,-0.2612048387527466,0.10239148139953613,0.14400328695774078,0.06518048048019409,0.11810192465782166,0.009169060736894608,-0.0874442458152771,-0.0550210066139698,-0.275527685880661,-0.047103554010391235,0.07880166172981262,-0.27189403772354126,0.0995284914970398,0.16892242431640625,0.1249561756849289,-0.08174028992652893,0.1112571656703949,-0.22777816653251648,-0.10044112801551819,0.03595735505223274,0.12264493107795715,-0.19697344303131104,0.09403163194656372,0.09285673499107361,-0.18553194403648376,-0.28888219594955444,-0.29200664162635803,0.11172385513782501,-0.13687889277935028,0.11263273656368256,-0.293516606092453,-0.2900136709213257,0.11190682649612427,0.04577289894223213,0.1446591466665268,-0.2816576361656189,0.1191263198852539,0.12219417095184326,-0.17859169840812683,-0.046148523688316345,-0.09702983498573303,-0.2822498381137848,0.12264379858970642,-0.2621300518512726,-0.15295368432998657,0.09811414778232574,-0.2997191548347473,0.006172832101583481,0.010999072343111038,0.10232144594192505,0.008341643959283829,0.15083299577236176,-0.1332082748413086,-0.2527734041213989,0.10798071324825287,-0.28821784257888794,-0.1298656016588211,-0.17871960997581482,0.12177300453186035,-0.1766963005065918,-0.23563985526561737,-0.25298985838890076,-0.18150027096271515,0.03998245671391487,-0.27314308285713196,0.04913976415991783,0.12323568761348724,0.010177310556173325,0.09179860353469849,0.13429082930088043,-0.2105744630098343,-0.2575262188911438,-0.2862367630004883,-0.19828182458877563,-0.2900925278663635,0.08698931336402893,0.1332869529724121,0.1376848816871643,-0.29699790477752686,-0.1491861492395401,0.1249198168516159,0.029416706413030624,0.1270710825920105,-0.14530649781227112,-0.26162365078926086,0.03254631534218788,-0.08768013119697571,0.13711251318454742,0.17586494982242584,0.0025316253304481506,-0.032945822924375534,0.15394051373004913,0.09488548338413239,-0.28522545099258423,0.17416509985923767,-0.26400214433670044,0.1143847405910492,0.0885305404663086,0.11207422614097595,-0.14242962002754211,-0.26494547724723816,-0.1856118142604828,-0.0457424521446228,0.1688394397497177,0.14248958230018616,-0.02933850884437561,-0.13351309299468994,0.09842459857463837,-0.18544940650463104,0.15416550636291504,0.12247227132320404,0.10827155411243439,-0.17318923771381378,-0.22199460864067078,-0.07074577361345291,-0.07889430224895477,0.04131929203867912,0.013737063854932785,-0.28087809681892395,-0.00434798002243042,0.10093441605567932,-0.2894698679447174,0.03914996609091759,-0.152693971991539,-0.02118096873164177,0.13935495913028717,0.1476999968290329,-0.22522707283496857,0.06293505430221558,0.13855673372745514,-0.1423824280500412,0.04521701857447624,-0.0625678300857544,-0.21463385224342346,-0.03426368907094002,0.10788090527057648,-0.05834319442510605,0.04682869836688042,-0.22852492332458496,-0.051585596054792404,-0.04986322671175003,-0.24490460753440857,-0.03778313472867012,-0.285060852766037,-0.11114585399627686,-0.09359036386013031,0.17030417919158936,-0.10028217732906342,0.14586329460144043,0.09945213794708252,0.09469650685787201,-0.22639645636081696,-0.1954243928194046,0.11003588140010834,0.08696085214614868,-0.2226598858833313,-0.12411393225193024,-0.1660841852426529,-0.2533251643180847,0.11116151511669159,-0.08110004663467407,-0.271520733833313,-0.026156354695558548,0.14936155080795288,0.1367395967245102,0.11336056888103485,0.08756616711616516,0.07721017301082611,0.09457333385944366,-0.023349745199084282,-0.18293392658233643,0.1654125452041626,0.0969020426273346,-0.22937510907649994,0.14992478489875793,0.12709201872348785,-0.016193538904190063,0.09507951140403748,0.11819763481616974,-0.07905548810958862,0.12350951135158539,-0.21665190160274506,0.12161949276924133,-0.28879514336586,0.1671154946088791,0.09658132493495941,0.09520450234413147,-0.12332808971405029,0.021417610347270966,0.0544394813477993,-0.04873703792691231,-0.12123142182826996,0.13027848303318024,0.07787097990512848,0.016801483929157257,-0.15478283166885376,0.15458407998085022,-0.2066439688205719,0.12311816215515137,0.15409502387046814,-0.26248788833618164,0.11217035353183746,0.055975835770368576,-0.052799783647060394,-0.26481181383132935,0.15535257756710052,0.09264516830444336,0.15471304953098297,0.11898402869701385,0.09411579370498657,0.09903450310230255,0.12408556044101715,0.1437620371580124,0.06040094420313835,-0.15261396765708923,0.0029234029352664948,0.1299232393503189,-0.18967263400554657,0.0917288213968277,-0.07854938507080078,-0.23866957426071167,-0.17254336178302765,-0.27412694692611694,-0.11329470574855804,-0.19816897809505463,-0.12192022800445557,0.12528061866760254,-0.24011917412281036,-0.21341465413570404,-0.010709326714277267,0.11941665410995483,0.12138833105564117,-0.24874301254749298,0.14727471768856049,0.12592343986034393,0.16423402726650238,0.09307776391506195,-0.26352158188819885,0.12320016324520111,-0.2830688953399658,-0.18068796396255493,0.0062656886875629425,-0.2758618891239166,0.1660367101430893,0.10069021582603455,0.12065903842449188,-0.03917168453335762,0.14107012748718262,0.0015791580080986023,0.015851840376853943,0.09702754020690918,0.1125367283821106,0.10584147274494171,0.023006360977888107,0.16047543287277222,-0.1448381394147873,-0.28772807121276855,-0.13711580634117126,-0.26485005021095276,-0.13643641769886017,0.13369761407375336,0.11794917285442352,-0.20592868328094482,0.07826054096221924,-0.039224475622177124,-0.03756564483046532,0.1623554229736328,0.12851323187351227,-0.26720839738845825,-0.004683809354901314,-0.0069858115166425705,-0.27465900778770447,-0.18334853649139404,0.16829204559326172,-0.1308334916830063,0.1096717119216919,0.1643935889005661,0.09813280403614044,-0.21085809171199799,-0.11317536234855652,-0.06045222282409668,0.00908488780260086,0.09878750145435333,-0.29775547981262207,-0.27870607376098633,-0.07252737879753113,-0.04536821320652962,-0.22311189770698547,-0.11463196575641632,0.09042853116989136,0.08784854412078857,0.020618226379156113,0.040457528084516525,0.17774878442287445,0.007900580763816833,-0.039883919060230255,-0.06435184925794601,-0.16337443888187408,0.020495988428592682,0.016533922404050827,0.09626685082912445,0.16562268137931824,-0.16434720158576965,0.14509481191635132,0.09755681455135345,-0.07066802680492401,-0.10846783220767975,0.08765776455402374,-0.23460140824317932,-0.28731122612953186,-0.19459715485572815,0.11479952931404114,-0.2898964285850525,0.09745410084724426,0.13598917424678802,0.14404822885990143,-0.29455989599227905,0.09265828132629395,0.17708797752857208,-0.14835096895694733,-0.259955495595932,0.12013697624206543,-0.22498400509357452,0.11355513334274292,-0.20434404909610748,-0.16624487936496735,-0.271470308303833,0.0594237856566906,-0.020379550755023956,-0.03882790729403496,-0.04161345586180687,0.11641724407672882,-0.00955948792397976,-0.293256014585495,-0.024048790335655212,0.15688711404800415,-0.2206386923789978,0.12538333237171173,0.08235417306423187,-0.25784626603126526,0.027967486530542374,-0.2751794755458832,0.14586298167705536,-0.045018427073955536,0.11038871109485626,0.15816159546375275,0.1170826107263565,0.14641201496124268,0.02597185969352722,-0.20852458477020264,0.052850980311632156,0.12669843435287476,0.15021875500679016,-0.09553545713424683,-0.002452971413731575,0.060835909098386765,-0.08383948355913162,0.12447115778923035,-0.2792663276195526,-0.26609569787979126,0.04677516594529152,0.08544735610485077,0.11380569636821747,0.09872044622898102,-0.29719868302345276,0.13554508984088898,-0.28418755531311035,0.12721970677375793,-0.05833106115460396,-0.2908526062965393,0.13278576731681824,0.14705146849155426,-0.22167040407657623,-0.1942259967327118,0.12772567570209503,-0.16386577486991882,0.0869266539812088,0.07957282662391663,0.03221644088625908,-0.27344536781311035,-0.13201965391635895,0.07783223688602448,0.060168828815221786,-0.13296756148338318,0.08230160176753998,0.0968908816576004,-0.054482586681842804,-0.1599404513835907,0.13710688054561615,0.12720169126987457,-0.09323552250862122,0.1528347134590149,0.025527235120534897,-0.24338991940021515,-0.07064895331859589,-0.15581780672073364,-0.292725533246994,0.15385517477989197,-0.2748440206050873,0.0058627501130104065,0.11187750101089478,0.12006078660488129,0.16520047187805176,0.012027870863676071,0.1175701767206192,0.11888711154460907,-0.009025102481245995,0.06133240833878517,0.1192779541015625,-0.05026734620332718,-0.13769017159938812,-0.2717759311199188,0.018558945506811142,-0.2963281273841858,-0.035009995102882385,0.12733867764472961,0.09402748942375183,-0.042043089866638184,-0.09428691864013672,-0.10137267410755157,0.11135616898536682,-0.23720012605190277,-0.0714852511882782,-0.21120023727416992,0.08643297851085663,0.08899888396263123,-0.161155104637146,-0.03289603441953659,0.09990841150283813,-0.02302641049027443,0.13295596837997437,-0.05710311233997345,0.07674126327037811,0.1038709431886673,-0.16495810449123383,0.07922036945819855,-0.15093845129013062,-0.10503970086574554,0.11170746386051178,0.09147191047668457,-0.2548098564147949,-0.29344993829727173,-0.2673189640045166,0.17488564550876617,-0.050482504069805145,0.12328441441059113,-0.04026028513908386,-0.12044960260391235,0.10827864706516266,-0.1033986359834671,0.09793856739997864,-0.29400086402893066,0.13046254217624664,-0.2940940856933594,-0.29456058144569397,-0.2177416831254959,0.09733746945858002,0.06086086854338646,-0.2884945571422577,-0.28887251019477844,-0.154865562915802,-0.20669279992580414,-0.28653016686439514,-0.13060565292835236,-0.18389630317687988,0.0034645982086658478,0.09550686180591583,0.12164455652236938,-0.10385119915008545,0.09421366453170776,0.06063966825604439,0.09013135731220245,0.11336594820022583,-0.010756531730294228,0.11231377720832825,-0.1447845697402954,0.11413788795471191,0.1420622169971466,-0.23997512459754944,-0.13800980150699615,-0.12014706432819366,0.15224552154541016,0.09292224049568176,-0.17894677817821503,-0.22199581563472748,0.10334710776805878,0.04657762125134468,-0.2992154359817505,0.09177486598491669,-0.23136518895626068,-0.11212658882141113,0.08649091422557831,0.13058753311634064,0.15609022974967957,0.13729916512966156,-0.09947885572910309,-0.28434640169143677,0.14510850608348846,-0.08342941105365753,-0.19283300638198853,-0.26570504903793335,-0.09937785565853119,0.07452061772346497,-0.17523041367530823,0.11204756796360016,-0.23846930265426636,-0.28055688738822937,0.09053605794906616,0.16796880960464478,-0.20530182123184204,-0.10792692005634308,-0.2723748981952667,-0.01970483362674713,0.10579371452331543,0.054994720965623856,0.09867464005947113,0.07565049827098846,-0.1761876344680786,0.1456545740365982,-0.2827625572681427,-0.053840041160583496,-0.14578740298748016,-0.0611538290977478,0.010232314467430115,0.12390591204166412,-0.08878997713327408,0.10437729954719543,0.12220178544521332,0.12866485118865967,0.10047471523284912,0.04132721200585365,-0.269901841878891,0.15236012637615204,0.10841059684753418,-0.2845943868160248,0.11806802451610565,-0.28232449293136597,-0.2590988874435425,0.11252537369728088,0.13172492384910583,0.09728725254535675,-0.28968581557273865,-0.062492735683918,0.09209536015987396,-0.268123984336853,-0.01043589599430561,-0.279958039522171,0.11570683121681213,-0.27291736006736755,-0.11882998049259186,-0.15685653686523438,0.1469111442565918,-0.24603720009326935,0.11242914199829102,-0.28582361340522766,-0.18647252023220062,0.07068264484405518,-0.13337332010269165,-0.02044813707470894,-0.249454528093338,-0.08515572547912598,-0.28770357370376587,0.15051324665546417,0.16014043986797333,-0.20410694181919098,0.14607447385787964,0.0097588412463665,-0.1064552515745163,0.09716883301734924,0.0944644957780838,-0.20939722657203674,0.14328382909297943,-0.26273584365844727,-0.2900504171848297,0.1535879522562027,0.08531689643859863,0.09788341820240021,-0.2588317096233368,0.10974106192588806,-0.11648093163967133,0.04924171045422554,-0.028190091252326965,0.11949960887432098,-0.19455839693546295,0.09009860455989838,-0.2193630039691925,-0.20830978453159332,0.1323077231645584,0.01951472833752632,0.09867055714130402,-0.10143952071666718,0.1347915530204773,0.11233128607273102,0.15285582840442657,0.08641976118087769,-0.28920185565948486,0.09955564141273499,-0.0026435144245624542,-0.13889668881893158,0.09561938047409058,0.13038571178913116,0.1595858633518219,-0.05281101167201996,-0.29537448287010193,-0.18447889387607574,-0.02627316676080227,0.07195450365543365,0.140694722533226,0.15845221281051636,-0.23049984872341156,-0.2916780412197113,0.12609237432479858,0.13040107488632202,-0.10829545557498932,-0.2732398211956024,0.1318914294242859,0.09602893888950348,0.08923298120498657,0.11718516051769257,0.1523054838180542,-0.28952276706695557,-0.11258861422538757,-0.23015227913856506,-0.2756248712539673,0.12519894540309906,-0.024855390191078186,0.14187903702259064,-0.2763077914714813,-0.2654159963130951,-0.265455037355423,0.032055389136075974,-0.06722790002822876,0.17351236939430237,-0.2141152173280716,-0.24630175530910492,-0.2647077143192291,0.08332100510597229,0.1547303944826126,0.11736451089382172,-0.26959869265556335,-0.06918304413557053,0.09158849716186523,-0.20322147011756897,-0.29547521471977234,-0.2834694981575012,-0.04557516425848007,0.012849710881710052,0.06244098022580147,-0.28526824712753296,0.1227007657289505,0.10006193816661835,0.03877765312790871,0.1392088234424591,0.06891180574893951,-0.29377061128616333,0.11106696724891663,0.01571401208639145,-0.28864336013793945,-0.13989539444446564,0.1508973240852356,0.15931309759616852,-0.005663519725203514,0.09389883279800415,0.1181255429983139,0.15390488505363464,-0.29174888134002686,-0.13844920694828033,-0.26374784111976624,-0.13610386848449707,-0.11489944159984589,-0.17078135907649994,-0.28862014412879944,0.12865132093429565,0.1188802719116211,0.10864350199699402,-0.1742638200521469,-0.01904573105275631,-0.2556001543998718,0.09901358187198639,-0.2709254324436188,-0.2948693037033081,0.09912514686584473,0.16130249202251434,0.1313886046409607,-0.10328108072280884,0.16836613416671753,0.05000467225909233,0.07926927506923676,-0.22506839036941528,-0.18196089565753937,0.14392556250095367,-0.05227259173989296,0.11324258148670197,-0.29521551728248596,-0.06347775459289551,0.1327471286058426,-0.29165443778038025,0.026440009474754333,0.00928274542093277,-0.07489655911922455,0.1258477419614792,0.10435040295124054,-0.028070418164134026,0.15388377010822296,0.1151987761259079,0.12165829539299011,0.10590924322605133,0.05271664634346962,-0.1780194640159607,-0.0747670829296112,0.06868594884872437,-0.1865008920431137,0.15230585634708405,0.1615612506866455,-0.2145770788192749,-0.2498762160539627,0.15335525572299957,0.12220782041549683,0.09731873869895935,0.09811531007289886,0.09515613317489624,0.12524272501468658,0.11303775012493134,0.1374010443687439,-0.13603326678276062,-0.28481772541999817,-0.07323452085256577,0.09805320203304291,0.0980873703956604,0.1490992307662964,0.05276716127991676,0.14093169569969177,-0.2697715163230896,0.010635614395141602,0.10751199722290039,0.12086853384971619,-0.2294614166021347,0.12997391819953918,-0.0004676561802625656,0.1473827213048935,0.1037093997001648,0.16236630082130432,0.12419471144676208,-0.10398699343204498,-0.2986169159412384,-0.28812432289123535,0.10346850752830505,-0.2824578285217285,-0.0008432529866695404,0.08829633891582489,-0.055623240768909454,0.14053399860858917,-0.11843155324459076,-0.29809555411338806,0.10918448865413666,0.13333889842033386,-0.13502129912376404,0.07319755852222443,0.11815884709358215,-0.06676670908927917,0.03725023195147514,0.11320585012435913,0.11338837444782257,-0.11774881184101105,-0.28989359736442566,-0.09147350490093231,0.096037358045578,0.17243637144565582,-0.28850123286247253,0.10642929375171661,-0.2791489362716675,-0.10437701642513275,0.1379815936088562,0.11175990104675293,0.0980537086725235,-0.06763498485088348,-0.0756058469414711,-0.20357608795166016,0.12500718235969543,-0.18018822371959686,-0.2949408292770386,-0.28440794348716736,-0.2153315395116806,-0.2612694501876831,-0.09870637953281403,0.15966150164604187,-0.14925554394721985,-0.09297631680965424,-0.18025027215480804,0.12299081683158875,-0.017713049426674843,0.11442680656909943,-0.27487730979919434,0.1240103542804718,0.1241314709186554,-0.05185198038816452,-0.29137951135635376,0.032357107847929,0.07806313037872314,0.04114151373505592,-0.19305747747421265,-0.14405734837055206,-0.27275365591049194,-0.09820333123207092,0.11180493235588074,0.08967916667461395,0.11882074177265167,-0.19932346045970917,-0.22117264568805695,-0.07783278822898865,-0.2852449119091034,0.09011204540729523,0.10029235482215881,0.13107198476791382,0.058122988790273666,-0.0018261019140481949,0.03808741644024849,-0.2919211685657501,0.003577660769224167,-0.08583932369947433,-0.2921445071697235,-0.16928908228874207,0.09382350742816925,0.16109642386436462,-0.20792153477668762,0.1448405683040619,0.038220133632421494,0.15089333057403564,-0.08624693751335144,-0.2705027163028717,-0.2613036036491394,-0.2728540897369385,0.1270727664232254,0.014228042215108871,-0.09066838026046753,-0.0901065319776535,-0.06791028380393982,-0.28806817531585693,-0.09624351561069489,0.1079493910074234,0.011872481554746628,0.12286314368247986,0.1631413996219635,-0.21473529934883118,0.10152474045753479,-0.2280127853155136,0.09198720753192902,0.09250782430171967,-0.1990308165550232,-0.13449198007583618,0.0845193862915039,0.1551910936832428,-0.1496690809726715,0.04186323657631874,-0.2964222729206085,0.06871725618839264,0.13582420349121094,-0.23617017269134521,-0.27275949716567993,-0.28184038400650024,-0.09095954895019531,0.12677349150180817,0.1037638932466507,0.1144997626543045,0.08935405313968658,0.11614102125167847,0.04108991101384163,-0.04752856120467186,0.05023645982146263,0.09415490925312042,0.04299802705645561,0.11735275387763977,0.00348803773522377,0.05878632143139839,0.16298256814479828,0.09495332837104797,-0.08854303508996964,-0.2125534564256668,0.11222444474697113,0.13611918687820435,0.15094129741191864,-0.003574492409825325,-0.08494362235069275,-0.21041926741600037,0.13288180530071259,0.08948415517807007,-0.09034166485071182,0.0687444657087326,-0.29185137152671814,-0.26574838161468506,-0.24911506474018097,0.14217010140419006,-0.29362526535987854,-0.2739560902118683,-0.13905562460422516,-0.16132868826389313,0.09829019010066986,0.16630294919013977,-0.28472983837127686,0.14417307078838348,-0.09202411770820618,0.12938177585601807,-0.2580659091472626,0.11523975431919098,-0.022480236366391182,0.06305909156799316,-0.24935582280158997,0.12562349438667297,-0.18950514495372772,0.059328559786081314,-0.006913430988788605,-0.007444005459547043,0.057162459939718246,0.05123458430171013,0.0952899307012558,0.1055353581905365,0.12081877887248993,0.14161653816699982,-0.2506142854690552,0.054672833532094955,0.08383658528327942,0.08944983780384064,-0.2925231456756592,0.11959432065486908,0.12230284512042999,-0.18870359659194946,0.025362055748701096,-0.15554271638393402,-0.026409760117530823,-0.07515797764062881,0.1623164713382721,-0.2988457679748535,0.15595166385173798,-0.12652744352817535,-0.21397167444229126,0.1334981769323349,-0.2672260105609894,0.11065895855426788,0.13887040317058563,0.13582827150821686,0.07884454727172852,0.10674162209033966,-0.15517301857471466,-0.10851015150547028,0.09866213798522949,-0.2697218358516693,0.1132664829492569,0.09979259967803955,-0.2928336262702942,-0.0022805016487836838,-0.15522919595241547,-0.17287839949131012,-0.2714610695838928,-0.028165312483906746,0.11426754295825958,-0.26206403970718384,0.12608106434345245,-0.12771742045879364,-0.13144606351852417,-0.29048827290534973,0.12418633699417114,0.11755327880382538,0.09942573308944702,0.03359491750597954,-0.07730153203010559,-0.2781970202922821,-0.20179483294487,0.10378843545913696,0.09046731889247894,-0.28208914399147034,0.09172867238521576,-0.12527209520339966,-0.29086771607398987,0.13812784850597382,0.09022533893585205,0.009684581309556961,0.16346585750579834,-0.10296764969825745,-0.237713024020195,-0.02942989207804203,0.1253315806388855,0.14539365470409393,-0.2887691557407379,0.09515522420406342,0.10342824459075928,0.11396743357181549,-0.0929165929555893,0.12608787417411804,-0.29133060574531555,0.16525393724441528,0.018776029348373413,0.10530242323875427,-0.1716812551021576,-0.28847604990005493,0.063377246260643,0.08852125704288483,-0.04531371593475342,0.12755097448825836,0.04977701976895332,0.13855022192001343,-0.2625834345817566,0.07244375348091125,-0.27079713344573975,0.118520587682724,0.09510843455791473,-0.04509402811527252,-0.11315533518791199,0.13266263902187347,-0.0462760329246521,-0.24224065244197845,0.16729748249053955,0.013522662222385406,-0.1796308308839798,0.10080891847610474,0.09476147592067719,0.1508384346961975,-0.0134060587733984,0.0770270824432373,0.054581645876169205,-0.22131700813770294,0.1226104348897934,0.11531634628772736,0.08445733785629272,-0.2914943993091583,-0.15222111344337463,0.1285410076379776,-0.28955498337745667,-0.26540809869766235,-0.290990948677063,0.12773726880550385,-0.26652756333351135,0.12747424840927124,0.15476876497268677,-0.2929314374923706,0.1557224988937378,-0.19516050815582275,0.019576605409383774,-0.01652642711997032,0.1513805091381073,-0.12491212785243988,-0.006081821396946907,0.020542465150356293,-0.24746651947498322,0.15783220529556274,-0.07349563390016556,0.10448978841304779,0.13341167569160461,0.0014861337840557098,0.11250802874565125,-0.06407515704631805,0.11555720865726471,-0.21823619306087494,-0.19004055857658386,0.017100472003221512,0.11141632497310638,-0.29754379391670227,0.1407826840877533,0.08605125546455383,0.07984612882137299,0.09511391818523407,-0.005428226664662361,-0.23121589422225952,-0.2762199640274048,-0.10940134525299072,0.15055720508098602,0.14355741441249847,-0.26452863216400146,-0.28943902254104614,0.08798204362392426,-0.22174856066703796,0.13735155761241913,0.13305576145648956,-0.2474210262298584,-0.03217390179634094,-0.07501056790351868,-0.04968816787004471,0.16164451837539673,0.09662902355194092,0.11505061388015747,-0.22092993557453156,0.1531444936990738,0.10670804977416992,0.1265125274658203,-0.018788540735840797,0.022265728563070297,-0.2710196375846863,-0.21309563517570496,-0.11112922430038452,-0.2738986909389496,-0.14822880923748016,0.14103902876377106,0.029155954718589783,-0.28138214349746704,-0.2272436022758484,0.13051310181617737,-0.0006240494549274445,-0.19830496609210968,0.1121949851512909,0.11799897253513336,-0.12559959292411804,-0.2971508800983429,-0.011703765019774437,0.16906939446926117,0.04662508890032768,-0.2887410819530487,0.09123840928077698,0.08165685832500458,-0.18199896812438965,0.05058310553431511,0.1438397765159607,-0.0004991702735424042,-0.12884527444839478,0.11099167168140411,0.15003839135169983,0.16274328529834747,0.1199863851070404,0.11952030658721924,-0.2604573965072632,-0.2390565723180771,-0.07552087306976318,0.04785313084721565,-0.019495191052556038,0.10826654732227325,0.10097181797027588,0.007822204381227493,-0.12067574262619019,0.05291043594479561,0.12507957220077515,-0.29197776317596436,-0.21472546458244324,0.06180056557059288,0.05122767761349678,-0.26898422837257385,0.0074917227029800415,-0.07118110358715057,-0.1522970348596573,0.09982466697692871,-0.07513777911663055,-0.008825087919831276,-0.20617790520191193,0.1221388727426529,-0.09827616810798645,0.15795962512493134,-0.05841777101159096,0.13800159096717834,0.0948336273431778,0.11923228204250336,-0.05661579966545105,-0.244122713804245,-0.12034426629543304,0.12829038500785828,-0.22961604595184326,0.028626054525375366,-0.19432277977466583,0.002627052366733551,0.11922229826450348,-0.10555227100849152,0.15637049078941345,0.13061721622943878,0.10306832194328308,0.12753987312316895,-0.05472312867641449,0.06691056489944458,-0.2934432327747345,-0.28285348415374756,0.11704254150390625,-0.2938205599784851,0.11180062592029572,0.06686189770698547,-0.008655911311507225,0.10848565399646759,0.08988617360591888,0.1269586682319641,-0.1278449296951294,0.07807531952857971,0.09083463251590729,0.0919642299413681,0.12474294006824493,0.13036088645458221,-0.0015108510851860046,-0.2702699303627014,0.11127370595932007,-0.2933844327926636,0.12057903409004211,0.0035866647958755493,-0.039026450365781784,-0.23178623616695404,-0.22483474016189575,0.17503158748149872,-0.20369577407836914,0.008951231837272644,0.01707065850496292,0.0939476490020752,0.12076248228549957,0.1082988977432251,-0.24623389542102814,-0.2047913819551468,-0.2746151387691498,0.1111622005701065,0.09916995465755463,-0.15707573294639587,-0.2615900933742523,0.1588055044412613,-0.19574445486068726,0.10867312550544739,-0.28574874997138977,0.08881346881389618,-0.2676898241043091,-0.14745032787322998,0.12357717752456665,-0.23197416961193085,0.09003724157810211,0.0983242392539978,-0.29216232895851135,0.09844985604286194,0.16389039158821106,-0.13290368020534515,0.07395006716251373,0.11842866241931915,0.11230969429016113,-0.26821520924568176,0.10756491124629974,-0.062153689563274384,-0.10982838273048401,0.054222505539655685,-0.28744471073150635,-0.048939742147922516,0.16811397671699524,0.09913253784179688,-0.14545699954032898,0.17649714648723602,0.11800482869148254,-0.25990599393844604,-0.2655196487903595,-0.29510051012039185,0.0011224579066038132,-0.26773181557655334,0.11696925759315491,-0.2970665395259857,-0.16289329528808594,-0.2604946792125702,-0.015106775797903538,0.1660192757844925,0.12957048416137695,0.13136275112628937,0.052915897220373154,0.015166081488132477,-0.26661837100982666,-0.16940727829933167,0.14818072319030762,0.14078377187252045,0.11417201161384583,0.053241003304719925,0.02984778583049774,-0.0759069174528122,0.02314213663339615,0.08569273352622986,0.16192680597305298,0.12558220326900482,-0.1618223637342453,-0.0570637471973896,0.00433320552110672,0.11604972183704376,-0.27848583459854126,-0.24835710227489471,-0.13164356350898743,-0.08315566182136536,-0.2918657958507538,-0.19128157198429108,-0.1696683168411255,0.09125612676143646,-0.10850441455841064,0.1272217184305191,-0.1978498250246048,-0.07987318933010101,0.09282827377319336,-0.18508586287498474,-0.03747362643480301,-0.07690458744764328,0.15716341137886047,-0.1765623837709427,-0.28754886984825134,0.1283760666847229,0.13988101482391357,-0.1142844706773758,0.10805605351924896,0.12409006059169769,0.11441256105899811,0.10591959953308105,0.14442004263401031,-0.2030555009841919,-0.1985708326101303,-0.18748092651367188,0.11687435209751129,0.12468114495277405,0.09617428481578827,-0.15553094446659088,-0.19929029047489166,-0.0036812592297792435,0.05754593387246132,-0.09942471981048584,0.13159151375293732,0.09782300889492035,-0.034649331122636795,0.09055101871490479,0.02977217361330986,0.11905483901500702,-0.27156543731689453,-0.2828981876373291,0.14764578640460968,-0.09586401283740997,0.09296037256717682,-0.0961383730173111,-0.2742568254470825,-0.007322389632463455,0.1320507675409317,-0.28625935316085815,-0.044267214834690094,-0.26798608899116516,0.10721166431903839,-0.2642778158187866,0.16272799670696259,-0.2778387665748596,0.10535834729671478,-0.2866232693195343,0.09923647344112396,0.08253653347492218,-0.019643856212496758,-0.15834015607833862,-0.0672544538974762,0.11282297968864441,0.12332506477832794,-0.15509600937366486,-0.22980405390262604,-0.19329728186130524,-0.28563037514686584,-0.11710435152053833,0.0706644356250763,-0.28364232182502747,0.02915949746966362,0.09593087434768677,0.16747701168060303,-0.280010849237442,0.08249585330486298,0.04133075103163719,-0.29049357771873474,0.09603951871395111,-0.051547810435295105,-0.07957574725151062,0.13044728338718414,-0.25189927220344543,0.09731380641460419,0.14099939167499542,0.10004957020282745,-0.14222100377082825,0.09455960988998413,-0.18239964544773102,-0.056112125515937805,0.09027576446533203,-0.20027463138103485,0.1241135448217392,-0.20154735445976257,-0.2912748456001282,-0.03257258981466293,0.1364407241344452,-0.12175546586513519,-0.13170255720615387,-0.07191821932792664,-0.19646647572517395,0.13314449787139893,0.12871669232845306,-0.2598722279071808,-0.00329585000872612,-0.002781407907605171,-0.2881786823272705,0.108881875872612,0.11092297732830048,0.13628540933132172,-0.13083617389202118,-0.013372281566262245,0.10726714134216309,0.12460964918136597,-0.009322434663772583,-0.0023055467754602432,0.08441472053527832,0.09332454204559326,-0.11787813901901245,-0.14274533092975616,-0.27772849798202515,-0.010584874078631401,0.11324086785316467,-0.14986801147460938,0.1333688646554947,-0.25608739256858826,-0.045516934245824814,-0.10269907116889954,0.032752376049757004,-0.1896660029888153,-0.2642558217048645,-0.2751837372779846,0.1290319561958313,-0.17592690885066986,0.14532993733882904,0.10492290556430817,0.0975000411272049,0.14197176694869995,-0.26278001070022583,0.11432191729545593,0.133125901222229,0.12215670943260193,-0.09172570705413818,0.11012455821037292,-0.24311412870883942,-0.2849055826663971,-0.012415597215294838,0.1390812247991562,-0.06446607410907745,0.045554179698228836,0.11363428831100464,-0.18343143165111542,-0.17718613147735596,0.010066293179988861,-0.2827152609825134,0.09804978966712952,-0.16466446220874786,0.010984070599079132,-0.18148301541805267,-0.11755327880382538,-0.18231414258480072,-0.04085049033164978,-0.1216789186000824,0.08593130111694336,-0.28905659914016724,-0.2870059907436371,0.09660492837429047,0.12276092171669006,-0.12105704843997955,-0.2690899670124054,-0.14990128576755524,-0.1737075001001358,0.11826561391353607,-0.2945103943347931,-0.1517428755760193,-0.2804686427116394,-0.01104205846786499,-0.1289609968662262,-0.21109358966350555,-0.2187749445438385,-0.1570514440536499,-0.08711583912372589,0.119947150349617,0.09732849895954132,-0.13694114983081818,-0.050642967224121094,-0.11667260527610779,-0.09441162645816803,0.00019724108278751373,0.08900468051433563,-0.2875032126903534,-0.055347640067338943,0.07736179232597351,-0.15851075947284698,-0.1786305010318756,0.09377709031105042,-0.14513327181339264,0.08610086143016815,0.02482864260673523,-0.28299257159233093,0.08661669492721558,-0.2282223403453827,-0.04009430855512619,-0.23420359194278717,-0.013054599985480309,-0.24584637582302094,-0.22770340740680695,0.17049527168273926,0.1675594449043274,-0.25070884823799133,0.12488003075122833,0.0884367823600769,-0.2934778332710266,0.10958743095397949,-0.2913144528865814,0.06927630305290222,-0.009582199156284332,-0.21463772654533386,-0.20329728722572327,-0.07776519656181335,-0.06451694667339325,-0.10356473922729492,-0.01626453548669815,-0.22138437628746033,0.11738331615924835,-0.2820725739002228,-0.16559343039989471,0.11061245203018188,0.14449790120124817,0.1719793975353241,-0.1624014973640442,0.12266500294208527,-0.18131324648857117,0.03601779416203499,-0.2925490140914917,0.10947290062904358,-0.28742533922195435,0.1296481043100357,0.12019315361976624,-0.21582935750484467,-0.10236266255378723,-0.03942224755883217,-0.19673864543437958,0.1593741774559021,-0.02286640740931034,-0.2111082524061203,-0.2748655676841736,-0.2204613834619522,0.09697911143302917,-0.09975090622901917,0.03597826138138771,-0.006686871871352196,-0.06727643311023712,-0.25511154532432556,-0.17653538286685944,-0.09807685017585754,0.1288156509399414,0.1584758311510086,-0.10973723232746124,0.11017905175685883,0.13128170371055603,0.1307210773229599,-0.2666776478290558,0.09423956274986267,0.141021728515625,0.105959951877594,0.1666233390569687,-0.2880367040634155,0.04090237244963646,0.0916648656129837,0.0362946055829525,0.09276613593101501,0.10103926062583923,-0.19398880004882812,-0.1181766539812088,0.07735107839107513,-0.15136922895908356,0.098879873752594,-0.2635990381240845,0.11374124884605408,-0.03991938754916191,-0.2735055983066559,0.15809468924999237,0.10810807347297668,0.05761585012078285,-0.27567416429519653,0.10490906238555908,-0.0947560966014862,0.1266072541475296,0.12996813654899597,-0.18274523317813873,0.15317483246326447,-0.2926328182220459,0.1084471344947815,-0.23469741642475128,0.11460669338703156,-0.06410852074623108,0.16370725631713867,-0.2915806770324707,-0.1875615119934082,-0.20407377183437347,0.14811138808727264,-0.2641804814338684,-0.14323867857456207,0.11944058537483215,0.11378265917301178,-0.03121362067759037,0.1569562405347824,-0.27488863468170166,0.10129691660404205,0.057196665555238724,0.03163626044988632,-0.2700175642967224,0.11575821042060852,-0.021183554083108902,-0.03458684682846069,0.09534439444541931,0.1633354276418686,-0.16826753318309784,-0.0015793833881616592,-0.04516417160630226,-0.2845240533351898,0.05736954137682915,-0.2663400173187256,0.14961549639701843,-0.07563503086566925,-0.2684112787246704,-0.28632697463035583,-0.14163023233413696,-0.28311145305633545,0.14319510757923126,-0.2550264000892639,-0.22844025492668152,-0.08263435959815979,0.10524831712245941,-0.19804567098617554,-0.29380321502685547,0.041939038783311844,0.14627934992313385,0.12567953765392303,0.10754448175430298,-0.11344097554683685,0.11520187556743622,0.170571967959404,-0.0830434262752533,-0.1669466346502304,0.1160007119178772,-0.008033717051148415,-0.07824061065912247,0.01424098014831543,-0.015002002008259296,-0.1853446215391159,0.026834636926651,0.09351661801338196,0.09414654970169067,-0.14089058339595795,0.14028172194957733,0.11867259442806244,-0.18669381737709045,0.11410176753997803,0.1093471348285675,-0.19874076545238495,-0.104728564620018,-0.06175614893436432,0.12097637355327606,0.10887247323989868,-0.021505624055862427,-0.29209211468696594,0.15538807213306427,-0.2119055986404419,0.00208345428109169,-0.2939385175704956,-0.2954642176628113,-0.07053513824939728,0.07303549349308014,0.05424434319138527,-0.13467465341091156,-0.05318578705191612,0.10545177757740021,0.0481601245701313,0.09835967421531677,0.14495785534381866,0.12147818505764008,0.004942465573549271,0.019247524440288544,0.030909352004528046,0.041946012526750565,-0.03527127951383591,-0.29313576221466064,-0.292704701423645,0.11131735146045685,-0.24275122582912445,-0.2526508867740631,0.1095457673072815,0.15375301241874695,0.097434863448143,-0.2670574188232422,-0.27399227023124695,-0.26945415139198303,0.12565641105175018,-0.24863778054714203,0.11142690479755402,-0.21028274297714233,0.09860986471176147,0.07407355308532715,-0.16160987317562103,0.10009719431400299,-0.00614214688539505,0.12379832565784454,-0.2585553824901581,-0.09945160150527954,-0.16462963819503784,-0.2892840802669525,-0.060486190021038055,0.01452312245965004,0.05737794563174248,0.14278161525726318,0.0477091483771801,0.09684795141220093,-0.1822202503681183,0.15631109476089478,-0.16139890253543854,0.08577018976211548,-0.29427456855773926,0.10200206935405731,-0.2685050666332245,-0.28740194439888,0.0905405730009079,-0.09577144682407379,-0.0013287179172039032,0.13392801582813263,-0.12173321843147278,0.12238910794258118,0.11436420679092407,-0.2642468512058258,0.16673408448696136,0.08927108347415924,-0.17727036774158478,-0.14093798398971558,0.14186806976795197,-0.27442556619644165,0.09056182205677032,-0.25847235321998596,0.16398170590400696,0.09571616351604462,-0.13723094761371613,-0.04200994595885277,-0.08935336023569107,0.044387031346559525,0.015325628221035004,0.12953394651412964,-0.24088598787784576,0.05469164624810219,-0.09410205483436584,0.043661829084157944,-0.11347416043281555,0.008401840925216675,0.14138305187225342,-0.1075098067522049,0.11784997582435608,-0.07437454909086227,-0.22693003714084625,0.11247479915618896,0.08848506212234497,-0.25064364075660706,0.0982382595539093,0.1178915947675705,0.1492738425731659,0.12521766126155853,-0.007054297253489494,0.10621562600135803,0.10030099749565125,-0.1246289312839508,-0.06270784884691238,0.012274626642465591,0.10467834770679474,-0.29306504130363464,0.15673764050006866,-0.00968099944293499,-0.026162434369325638,0.1448730230331421,0.17673537135124207,-0.0849773958325386,-0.2663921117782593,0.13311301171779633,0.11114799976348877,-0.29063722491264343,-0.13222017884254456,0.11476656794548035,0.0380086787045002,0.13880974054336548,-0.08580729365348816,0.06596969068050385,0.10086821019649506,-0.28658226132392883,-0.09835158288478851,-0.16002964973449707,-0.2931514382362366,-0.05831774324178696,0.001422269269824028,0.1423400491476059,-0.2080284059047699,0.12526266276836395,-0.05087246373295784,-0.10541458427906036,0.0923500508069992,-0.05977421998977661,-0.008730905130505562,0.15025989711284637,0.11374181509017944,0.06727831065654755,0.1592496782541275,0.10022912919521332,-0.15027807652950287,-0.23380330204963684,0.14110223948955536,-0.15719187259674072,0.058601249009370804,0.09633322060108185,0.15183058381080627,0.11226657032966614,-0.27258411049842834,0.004101831465959549,0.1353871077299118,0.12388919293880463,0.11653612554073334,-0.1932748407125473,-0.18425515294075012,0.06743854284286499,0.13917508721351624,0.09767156839370728,-0.2866532802581787,0.16045992076396942,0.09030298888683319,0.11777517199516296,-0.001200992614030838,0.030765894800424576,0.06265692412853241,0.08644717931747437,-0.26551932096481323,-0.12244024872779846,0.10134020447731018,-0.11975519359111786,0.1355838179588318,0.0736360102891922,-0.1440654993057251,0.11643439531326294,0.10657824575901031,-0.0440017506480217,0.047912467271089554,-0.16033878922462463,0.09464052319526672,-0.20261968672275543,-0.24543049931526184,-0.2537693679332733,-0.2769205868244171,-0.15410494804382324,0.14113090932369232,0.15788565576076508,-0.21250611543655396,-0.2794860005378723,0.1174439936876297,0.09584534168243408,-0.10345591604709625,-0.2823229432106018,-0.28504085540771484,-0.2900985777378082,0.00024685636162757874,-0.0250229574739933,-0.06489361077547073,0.11972041428089142,-0.12465989589691162,0.16020290553569794,-0.13410413265228271,0.11027148365974426,0.10543319582939148,0.10891653597354889,-0.12618567049503326,-0.27351638674736023,0.09616079926490784,0.10875840485095978,-0.14680518209934235,0.1715472787618637,0.03566707298159599,0.12295083701610565,0.16409072279930115,0.1126617044210434,0.12836672365665436,-0.01784403808414936,0.13803647458553314,-0.26736071705818176,0.1266443431377411,0.07416962087154388,0.10023730993270874,0.11736786365509033,0.06439533829689026,0.11729736626148224,0.10861575603485107,0.1699981391429901,0.09729574620723724,-0.057168006896972656,-0.11199748516082764,0.14282578229904175,-0.23662765324115753,-0.2891247570514679,-0.13713322579860687,0.10309624671936035,-0.233430877327919,0.1484927535057068,-0.06760656833648682,0.16027610003948212,-0.25525084137916565,-0.099525585770607,0.028741706162691116,0.09786660969257355,0.15137851238250732,0.10373057425022125,-0.09368665516376495,-0.030661316588521004,-0.29289352893829346,0.11670766770839691,0.10268308222293854,-0.13314685225486755,0.016470428556203842,0.10944139957427979,-0.18522895872592926,0.0880187600851059,0.11365590989589691,-0.18641667068004608,0.13098497688770294,-0.23472805321216583,-0.07103830575942993,0.030272532254457474,0.16983520984649658,-0.26471656560897827,0.1388479322195053,-0.24306580424308777,-0.2371368259191513,0.12914936244487762,0.14931043982505798,0.10997152328491211,-0.07526680827140808,-0.06066335737705231,0.13820719718933105,-0.14546923339366913,0.00942901149392128,0.11807499825954437,-0.2804422080516815,-0.07137671113014221,0.09914718568325043,0.061935845762491226,0.09997749328613281,0.020141281187534332,0.07384011149406433,-0.19781328737735748,-0.18493770062923431,-0.2845457196235657,-0.28829240798950195,0.13562174141407013,-0.2680019736289978,0.15468350052833557,0.09350831806659698,0.1522895246744156,0.1414189338684082,0.13017192482948303,0.07768440246582031,0.11421287059783936,-0.2186470478773117,-0.1516699492931366,-0.2756524384021759,-0.26668235659599304,-0.27152183651924133,0.060317087918519974,-0.16906249523162842,-0.015281344763934612,0.11764094233512878,-0.2877720594406128,-0.27584362030029297,-0.2611381411552429,-0.06930055469274521,0.12939710915088654,0.1463450938463211,0.048550043255090714,0.04779248312115669,-0.2563132047653198,0.00252721831202507,0.1009158194065094,-0.25065475702285767,-0.1802024394273758,0.1282721757888794,0.0885922908782959,-0.015931833535432816,-0.07508696615695953,0.10403980314731598,-0.020834559574723244,-0.1873248666524887,0.09114241600036621,0.13450078666210175,-0.2728721499443054,0.021050140261650085,0.1072605699300766,0.12355044484138489,0.023373838514089584,0.09168849885463715,0.005711212754249573,0.007419329136610031,-0.24910899996757507,0.007676038891077042,-0.2756975293159485,-0.18498367071151733,-0.14109499752521515,-0.2591758966445923,-0.17851762473583221,-0.29398733377456665,0.1129627525806427,0.0013982448726892471,0.009539201855659485,-0.27964556217193604,-0.12182193994522095,-0.24906091392040253,0.12521958351135254,0.1685885190963745,-0.1747191995382309,-0.2819691598415375,-0.18742406368255615,0.11390829086303711,-0.29653048515319824,-0.00408378429710865,0.1261843889951706,-0.020881906151771545,-0.05567439645528793,0.10297217965126038,0.12196934223175049,-0.29415082931518555,0.11575935781002045,0.1393529772758484,0.11156237125396729,0.10863496363162994,-0.0940273106098175,-0.28243783116340637,0.11359836161136627,-0.1452801674604416,-0.07700289785861969,-0.29735511541366577,-0.1569947600364685,-0.2126021385192871,-0.04881778359413147,-0.2883671224117279,0.11282941699028015],\"type\":\"scatter3d\"}],                        {\"scene\":{\"xaxis\":{\"visible\":false},\"yaxis\":{\"visible\":false},\"zaxis\":{\"visible\":false}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f941de25-346f-4fa1-8386-82112d493793');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"markdown","source":["### Outlier Dataset Generation\n","\n","\n","\n","\n"],"metadata":{"id":"gSk2GeaUlowI"}},{"cell_type":"code","source":["import h5py\n","import numpy as np"],"metadata":{"id":"HlD0B477vRSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"2VFeSaVO_GCo"}},{"cell_type":"code","source":["new_dataset_path = \"./data/shapenet_augmented_chair.hdf5\""],"metadata":{"id":"F1oWksv38Pbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original=h5py.File(\"./data/shapenet.hdf5\", 'r')\n","with h5py.File(new_dataset_path, 'a') as f:\n","   # Append new data to it\n","   for key in original.keys():\n","      group = f.require_group(key)\n","      old_data = original[key]['train']\n","      print(\"old data shape:\", old_data.shape)\n","      if key in ood_synsetid_to_pc:\n","        new_data = ood_synsetid_to_pc[key].cpu().numpy()\n","        print(new_data.shape)\n","        combined_data = np.concatenate([old_data, new_data], axis=0)\n","      else:\n","        combined_data = np.copy(old_data)\n","      print(\"new data shape:\", combined_data.shape)\n","      try:\n","        group.create_dataset(f'train', data=combined_data)\n","      except:\n","        pass\n","\n","      try:\n","        group.create_dataset(f'test', data=original[key]['test'])\n","        group.create_dataset(f'val', data=original[key]['val'])\n","      except:\n","        pass\n","original.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xnzk_XO03Cs8","executionInfo":{"status":"ok","timestamp":1712751477877,"user_tz":240,"elapsed":16559,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"69724ad3-22a4-408e-9e3c-813ff8205352"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["old data shape: (3438, 2048, 3)\n","new data shape: (3438, 2048, 3)\n","old data shape: (273, 2048, 3)\n","new data shape: (273, 2048, 3)\n","old data shape: (70, 2048, 3)\n","new data shape: (70, 2048, 3)\n","old data shape: (94, 2048, 3)\n","new data shape: (94, 2048, 3)\n","old data shape: (727, 2048, 3)\n","new data shape: (727, 2048, 3)\n","old data shape: (198, 2048, 3)\n","new data shape: (198, 2048, 3)\n","old data shape: (1536, 2048, 3)\n","new data shape: (1536, 2048, 3)\n","old data shape: (62, 2048, 3)\n","new data shape: (62, 2048, 3)\n","old data shape: (383, 2048, 3)\n","new data shape: (383, 2048, 3)\n","old data shape: (407, 2048, 3)\n","new data shape: (407, 2048, 3)\n","old data shape: (149, 2048, 3)\n","new data shape: (149, 2048, 3)\n","old data shape: (771, 2048, 3)\n","new data shape: (771, 2048, 3)\n","old data shape: (1311, 2048, 3)\n","new data shape: (1311, 2048, 3)\n","old data shape: (96, 2048, 3)\n","new data shape: (96, 2048, 3)\n","old data shape: (91, 2048, 3)\n","new data shape: (91, 2048, 3)\n","old data shape: (47, 2048, 3)\n","new data shape: (47, 2048, 3)\n","old data shape: (2986, 2048, 3)\n","new data shape: (2986, 2048, 3)\n","old data shape: (414, 2048, 3)\n","new data shape: (414, 2048, 3)\n","old data shape: (5602, 2048, 3)\n","(107, 2048, 3)\n","new data shape: (5709, 2048, 3)\n","old data shape: (553, 2048, 3)\n","new data shape: (553, 2048, 3)\n","old data shape: (54, 2048, 3)\n","new data shape: (54, 2048, 3)\n","old data shape: (79, 2048, 3)\n","new data shape: (79, 2048, 3)\n","old data shape: (929, 2048, 3)\n","new data shape: (929, 2048, 3)\n","old data shape: (62, 2048, 3)\n","new data shape: (62, 2048, 3)\n","old data shape: (630, 2048, 3)\n","new data shape: (630, 2048, 3)\n","old data shape: (253, 2048, 3)\n","new data shape: (253, 2048, 3)\n","old data shape: (677, 2048, 3)\n","new data shape: (677, 2048, 3)\n","old data shape: (137, 2048, 3)\n","new data shape: (137, 2048, 3)\n","old data shape: (479, 2048, 3)\n","new data shape: (479, 2048, 3)\n","old data shape: (360, 2048, 3)\n","new data shape: (360, 2048, 3)\n","old data shape: (1968, 2048, 3)\n","new data shape: (1968, 2048, 3)\n","old data shape: (383, 2048, 3)\n","new data shape: (383, 2048, 3)\n","old data shape: (1354, 2048, 3)\n","new data shape: (1354, 2048, 3)\n","old data shape: (79, 2048, 3)\n","new data shape: (79, 2048, 3)\n","old data shape: (56, 2048, 3)\n","new data shape: (56, 2048, 3)\n","old data shape: (129, 2048, 3)\n","new data shape: (129, 2048, 3)\n","old data shape: (286, 2048, 3)\n","new data shape: (286, 2048, 3)\n","old data shape: (181, 2048, 3)\n","new data shape: (181, 2048, 3)\n","old data shape: (203, 2048, 3)\n","new data shape: (203, 2048, 3)\n","old data shape: (81, 2048, 3)\n","new data shape: (81, 2048, 3)\n","old data shape: (225, 2048, 3)\n","new data shape: (225, 2048, 3)\n","old data shape: (511, 2048, 3)\n","new data shape: (511, 2048, 3)\n","old data shape: (140, 2048, 3)\n","new data shape: (140, 2048, 3)\n","old data shape: (56, 2048, 3)\n","new data shape: (56, 2048, 3)\n","old data shape: (2017, 2048, 3)\n","new data shape: (2017, 2048, 3)\n","old data shape: (72, 2048, 3)\n","new data shape: (72, 2048, 3)\n","old data shape: (129, 2048, 3)\n","new data shape: (129, 2048, 3)\n","old data shape: (2685, 2048, 3)\n","new data shape: (2685, 2048, 3)\n","old data shape: (185, 2048, 3)\n","new data shape: (185, 2048, 3)\n","old data shape: (7126, 2048, 3)\n","new data shape: (7126, 2048, 3)\n","old data shape: (477, 2048, 3)\n","new data shape: (477, 2048, 3)\n","old data shape: (104, 2048, 3)\n","new data shape: (104, 2048, 3)\n","old data shape: (330, 2048, 3)\n","new data shape: (330, 2048, 3)\n","old data shape: (1647, 2048, 3)\n","new data shape: (1647, 2048, 3)\n","old data shape: (141, 2048, 3)\n","new data shape: (141, 2048, 3)\n"]}]},{"cell_type":"code","source":["with h5py.File(new_dataset_path, 'a') as f:\n","  for key in f.keys():\n","    print(f[key]['val'].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-ygAWnGIBNh","executionInfo":{"status":"ok","timestamp":1712751486430,"user_tz":240,"elapsed":213,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"0dac8c96-5f39-40b2-c3ac-de24cff7689f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(607, 2048, 3)\n","(49, 2048, 3)\n","(13, 2048, 3)\n","(17, 2048, 3)\n","(129, 2048, 3)\n","(35, 2048, 3)\n","(272, 2048, 3)\n","(11, 2048, 3)\n","(68, 2048, 3)\n","(72, 2048, 3)\n","(27, 2048, 3)\n","(137, 2048, 3)\n","(232, 2048, 3)\n","(17, 2048, 3)\n","(17, 2048, 3)\n","(9, 2048, 3)\n","(528, 2048, 3)\n","(74, 2048, 3)\n","(989, 2048, 3)\n","(98, 2048, 3)\n","(10, 2048, 3)\n","(14, 2048, 3)\n","(164, 2048, 3)\n","(11, 2048, 3)\n","(112, 2048, 3)\n","(45, 2048, 3)\n","(120, 2048, 3)\n","(25, 2048, 3)\n","(85, 2048, 3)\n","(64, 2048, 3)\n","(348, 2048, 3)\n","(68, 2048, 3)\n","(240, 2048, 3)\n","(15, 2048, 3)\n","(11, 2048, 3)\n","(23, 2048, 3)\n","(51, 2048, 3)\n","(33, 2048, 3)\n","(36, 2048, 3)\n","(15, 2048, 3)\n","(40, 2048, 3)\n","(91, 2048, 3)\n","(25, 2048, 3)\n","(10, 2048, 3)\n","(356, 2048, 3)\n","(13, 2048, 3)\n","(23, 2048, 3)\n","(474, 2048, 3)\n","(33, 2048, 3)\n","(1258, 2048, 3)\n","(85, 2048, 3)\n","(19, 2048, 3)\n","(59, 2048, 3)\n","(291, 2048, 3)\n","(26, 2048, 3)\n"]}]},{"cell_type":"markdown","source":["### Comparison of Models between Inlier and Outlier Datasets\n","\n","---\n","\n"],"metadata":{"id":"uHtGeMwmlbse"}},{"cell_type":"code","source":["augmented_categories = [synsetid_to_cate[id] for id in ood_synsetid_to_pc]"],"metadata":{"id":"bSIc3RmgOrzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["augmented_categories"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5RTGKFMiPCzV","executionInfo":{"status":"ok","timestamp":1712751494142,"user_tz":240,"elapsed":2,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"d1f34291-18a6-4ed9-ba17-0808a571219e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['chair']"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["print(','.join(augmented_categories))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"My8JGCAJPz27","executionInfo":{"status":"ok","timestamp":1712332990927,"user_tz":240,"elapsed":579,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"b49649b0-9eb9-4558-9eb7-9355f83d743d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["chair,table,car,sofa,pot,microwave,bathtub,can,speaker,lamp,printer,file,bed,cabinet,vessel,bus,washer,dishwasher,clock,bowl,tin_can,jar,camera\n"]}]},{"cell_type":"code","source":["# train model on augmented dataset\n","!python3 train_ae.py --dataset_path {new_dataset_path} --log_root \"./logs_ae_chair\" --categories \"chair\"\n"],"metadata":{"id":"a7-zl8Vf0Mob","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712755033223,"user_tz":240,"elapsed":3458955,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"d17e3cfa-a2d4-458b-d4fb-0839752dca3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","[2024-04-10 13:02:55,673::train::INFO] [Train] Iter 15079 | Loss 0.277898 | Grad 0.0433 \n","[2024-04-10 13:02:55,792::train::INFO] [Train] Iter 15080 | Loss 0.335221 | Grad 0.0518 \n","[2024-04-10 13:02:55,915::train::INFO] [Train] Iter 15081 | Loss 0.339682 | Grad 0.0536 \n","[2024-04-10 13:02:56,033::train::INFO] [Train] Iter 15082 | Loss 0.321753 | Grad 0.0579 \n","[2024-04-10 13:02:56,152::train::INFO] [Train] Iter 15083 | Loss 0.329795 | Grad 0.0664 \n","[2024-04-10 13:02:56,271::train::INFO] [Train] Iter 15084 | Loss 0.326985 | Grad 0.0413 \n","[2024-04-10 13:02:56,389::train::INFO] [Train] Iter 15085 | Loss 0.276319 | Grad 0.0452 \n","[2024-04-10 13:02:56,509::train::INFO] [Train] Iter 15086 | Loss 0.314650 | Grad 0.0651 \n","[2024-04-10 13:02:56,628::train::INFO] [Train] Iter 15087 | Loss 0.358193 | Grad 0.0649 \n","[2024-04-10 13:02:56,747::train::INFO] [Train] Iter 15088 | Loss 0.294484 | Grad 0.0833 \n","[2024-04-10 13:02:56,866::train::INFO] [Train] Iter 15089 | Loss 0.319039 | Grad 0.0886 \n","[2024-04-10 13:02:56,990::train::INFO] [Train] Iter 15090 | Loss 0.307173 | Grad 0.0445 \n","[2024-04-10 13:02:57,109::train::INFO] [Train] Iter 15091 | Loss 0.318563 | Grad 0.0368 \n","[2024-04-10 13:02:57,229::train::INFO] [Train] Iter 15092 | Loss 0.308572 | Grad 0.0493 \n","[2024-04-10 13:02:57,349::train::INFO] [Train] Iter 15093 | Loss 0.307497 | Grad 0.0484 \n","[2024-04-10 13:02:57,472::train::INFO] [Train] Iter 15094 | Loss 0.345803 | Grad 0.0765 \n","[2024-04-10 13:02:57,592::train::INFO] [Train] Iter 15095 | Loss 0.340051 | Grad 0.0609 \n","[2024-04-10 13:02:57,711::train::INFO] [Train] Iter 15096 | Loss 0.319302 | Grad 0.0638 \n","[2024-04-10 13:02:57,829::train::INFO] [Train] Iter 15097 | Loss 0.333511 | Grad 0.0353 \n","[2024-04-10 13:02:57,947::train::INFO] [Train] Iter 15098 | Loss 0.315885 | Grad 0.0338 \n","[2024-04-10 13:02:58,065::train::INFO] [Train] Iter 15099 | Loss 0.322454 | Grad 0.0459 \n","[2024-04-10 13:02:58,186::train::INFO] [Train] Iter 15100 | Loss 0.332293 | Grad 0.0428 \n","[2024-04-10 13:02:58,304::train::INFO] [Train] Iter 15101 | Loss 0.324191 | Grad 0.0302 \n","[2024-04-10 13:02:58,424::train::INFO] [Train] Iter 15102 | Loss 0.316914 | Grad 0.0445 \n","[2024-04-10 13:02:58,544::train::INFO] [Train] Iter 15103 | Loss 0.320638 | Grad 0.0517 \n","[2024-04-10 13:02:58,665::train::INFO] [Train] Iter 15104 | Loss 0.325474 | Grad 0.0467 \n","[2024-04-10 13:02:58,784::train::INFO] [Train] Iter 15105 | Loss 0.293794 | Grad 0.0411 \n","[2024-04-10 13:02:58,903::train::INFO] [Train] Iter 15106 | Loss 0.315058 | Grad 0.0470 \n","[2024-04-10 13:02:59,022::train::INFO] [Train] Iter 15107 | Loss 0.284594 | Grad 0.0293 \n","[2024-04-10 13:02:59,142::train::INFO] [Train] Iter 15108 | Loss 0.308306 | Grad 0.0384 \n","[2024-04-10 13:02:59,262::train::INFO] [Train] Iter 15109 | Loss 0.354188 | Grad 0.0494 \n","[2024-04-10 13:02:59,381::train::INFO] [Train] Iter 15110 | Loss 0.337767 | Grad 0.0567 \n","[2024-04-10 13:02:59,504::train::INFO] [Train] Iter 15111 | Loss 0.275443 | Grad 0.0403 \n","[2024-04-10 13:02:59,623::train::INFO] [Train] Iter 15112 | Loss 0.335827 | Grad 0.0615 \n","[2024-04-10 13:02:59,744::train::INFO] [Train] Iter 15113 | Loss 0.266543 | Grad 0.0418 \n","[2024-04-10 13:02:59,864::train::INFO] [Train] Iter 15114 | Loss 0.303180 | Grad 0.0559 \n","[2024-04-10 13:02:59,984::train::INFO] [Train] Iter 15115 | Loss 0.307204 | Grad 0.0466 \n","[2024-04-10 13:03:00,106::train::INFO] [Train] Iter 15116 | Loss 0.311712 | Grad 0.0589 \n","[2024-04-10 13:03:00,226::train::INFO] [Train] Iter 15117 | Loss 0.290683 | Grad 0.0622 \n","[2024-04-10 13:03:00,345::train::INFO] [Train] Iter 15118 | Loss 0.339401 | Grad 0.0619 \n","[2024-04-10 13:03:00,466::train::INFO] [Train] Iter 15119 | Loss 0.318585 | Grad 0.0688 \n","[2024-04-10 13:03:00,542::train::INFO] [Train] Iter 15120 | Loss 0.339047 | Grad 0.0577 \n","[2024-04-10 13:03:00,660::train::INFO] [Train] Iter 15121 | Loss 0.307991 | Grad 0.0456 \n","[2024-04-10 13:03:00,780::train::INFO] [Train] Iter 15122 | Loss 0.333331 | Grad 0.0409 \n","[2024-04-10 13:03:00,898::train::INFO] [Train] Iter 15123 | Loss 0.340518 | Grad 0.0692 \n","[2024-04-10 13:03:01,019::train::INFO] [Train] Iter 15124 | Loss 0.339052 | Grad 0.0684 \n","[2024-04-10 13:03:01,137::train::INFO] [Train] Iter 15125 | Loss 0.297156 | Grad 0.0448 \n","[2024-04-10 13:03:01,256::train::INFO] [Train] Iter 15126 | Loss 0.281163 | Grad 0.0336 \n","[2024-04-10 13:03:01,376::train::INFO] [Train] Iter 15127 | Loss 0.305232 | Grad 0.0583 \n","[2024-04-10 13:03:01,497::train::INFO] [Train] Iter 15128 | Loss 0.356714 | Grad 0.0377 \n","[2024-04-10 13:03:01,618::train::INFO] [Train] Iter 15129 | Loss 0.282154 | Grad 0.0356 \n","[2024-04-10 13:03:01,737::train::INFO] [Train] Iter 15130 | Loss 0.363320 | Grad 0.0534 \n","[2024-04-10 13:03:01,857::train::INFO] [Train] Iter 15131 | Loss 0.301802 | Grad 0.0386 \n","[2024-04-10 13:03:01,975::train::INFO] [Train] Iter 15132 | Loss 0.343102 | Grad 0.0594 \n","[2024-04-10 13:03:02,093::train::INFO] [Train] Iter 15133 | Loss 0.354884 | Grad 0.0873 \n","[2024-04-10 13:03:02,216::train::INFO] [Train] Iter 15134 | Loss 0.319680 | Grad 0.0416 \n","[2024-04-10 13:03:02,334::train::INFO] [Train] Iter 15135 | Loss 0.329347 | Grad 0.0579 \n","[2024-04-10 13:03:02,453::train::INFO] [Train] Iter 15136 | Loss 0.302596 | Grad 0.0406 \n","[2024-04-10 13:03:02,571::train::INFO] [Train] Iter 15137 | Loss 0.360100 | Grad 0.0527 \n","[2024-04-10 13:03:02,689::train::INFO] [Train] Iter 15138 | Loss 0.326132 | Grad 0.0335 \n","[2024-04-10 13:03:02,808::train::INFO] [Train] Iter 15139 | Loss 0.283358 | Grad 0.0226 \n","[2024-04-10 13:03:02,928::train::INFO] [Train] Iter 15140 | Loss 0.303869 | Grad 0.0718 \n","[2024-04-10 13:03:03,047::train::INFO] [Train] Iter 15141 | Loss 0.315156 | Grad 0.0843 \n","[2024-04-10 13:03:03,171::train::INFO] [Train] Iter 15142 | Loss 0.347220 | Grad 0.0542 \n","[2024-04-10 13:03:03,297::train::INFO] [Train] Iter 15143 | Loss 0.311868 | Grad 0.0388 \n","[2024-04-10 13:03:03,417::train::INFO] [Train] Iter 15144 | Loss 0.301109 | Grad 0.0410 \n","[2024-04-10 13:03:03,542::train::INFO] [Train] Iter 15145 | Loss 0.309562 | Grad 0.0505 \n","[2024-04-10 13:03:03,662::train::INFO] [Train] Iter 15146 | Loss 0.346111 | Grad 0.0400 \n","[2024-04-10 13:03:03,784::train::INFO] [Train] Iter 15147 | Loss 0.342442 | Grad 0.0481 \n","[2024-04-10 13:03:03,909::train::INFO] [Train] Iter 15148 | Loss 0.382697 | Grad 0.0901 \n","[2024-04-10 13:03:04,036::train::INFO] [Train] Iter 15149 | Loss 0.330785 | Grad 0.0712 \n","[2024-04-10 13:03:04,157::train::INFO] [Train] Iter 15150 | Loss 0.299562 | Grad 0.0276 \n","[2024-04-10 13:03:04,279::train::INFO] [Train] Iter 15151 | Loss 0.307949 | Grad 0.0383 \n","[2024-04-10 13:03:04,400::train::INFO] [Train] Iter 15152 | Loss 0.297337 | Grad 0.0291 \n","[2024-04-10 13:03:04,520::train::INFO] [Train] Iter 15153 | Loss 0.345141 | Grad 0.0479 \n","[2024-04-10 13:03:04,641::train::INFO] [Train] Iter 15154 | Loss 0.324302 | Grad 0.0396 \n","[2024-04-10 13:03:04,767::train::INFO] [Train] Iter 15155 | Loss 0.285162 | Grad 0.0449 \n","[2024-04-10 13:03:04,889::train::INFO] [Train] Iter 15156 | Loss 0.286525 | Grad 0.0291 \n","[2024-04-10 13:03:05,012::train::INFO] [Train] Iter 15157 | Loss 0.297732 | Grad 0.0479 \n","[2024-04-10 13:03:05,133::train::INFO] [Train] Iter 15158 | Loss 0.332723 | Grad 0.0455 \n","[2024-04-10 13:03:05,254::train::INFO] [Train] Iter 15159 | Loss 0.288969 | Grad 0.0428 \n","[2024-04-10 13:03:05,376::train::INFO] [Train] Iter 15160 | Loss 0.300717 | Grad 0.0400 \n","[2024-04-10 13:03:05,497::train::INFO] [Train] Iter 15161 | Loss 0.319533 | Grad 0.0496 \n","[2024-04-10 13:03:05,619::train::INFO] [Train] Iter 15162 | Loss 0.313026 | Grad 0.0376 \n","[2024-04-10 13:03:05,747::train::INFO] [Train] Iter 15163 | Loss 0.307170 | Grad 0.0576 \n","[2024-04-10 13:03:05,870::train::INFO] [Train] Iter 15164 | Loss 0.355588 | Grad 0.1115 \n","[2024-04-10 13:03:05,949::train::INFO] [Train] Iter 15165 | Loss 0.293861 | Grad 0.0406 \n","[2024-04-10 13:03:06,069::train::INFO] [Train] Iter 15166 | Loss 0.325349 | Grad 0.0522 \n","[2024-04-10 13:03:06,194::train::INFO] [Train] Iter 15167 | Loss 0.336851 | Grad 0.0449 \n","[2024-04-10 13:03:06,316::train::INFO] [Train] Iter 15168 | Loss 0.294432 | Grad 0.0350 \n","[2024-04-10 13:03:06,435::train::INFO] [Train] Iter 15169 | Loss 0.287557 | Grad 0.0410 \n","[2024-04-10 13:03:06,557::train::INFO] [Train] Iter 15170 | Loss 0.310609 | Grad 0.0664 \n","[2024-04-10 13:03:06,676::train::INFO] [Train] Iter 15171 | Loss 0.323526 | Grad 0.0579 \n","[2024-04-10 13:03:06,795::train::INFO] [Train] Iter 15172 | Loss 0.281516 | Grad 0.0420 \n","[2024-04-10 13:03:06,913::train::INFO] [Train] Iter 15173 | Loss 0.311703 | Grad 0.0480 \n","[2024-04-10 13:03:07,032::train::INFO] [Train] Iter 15174 | Loss 0.337181 | Grad 0.0470 \n","[2024-04-10 13:03:07,153::train::INFO] [Train] Iter 15175 | Loss 0.328331 | Grad 0.0441 \n","[2024-04-10 13:03:07,272::train::INFO] [Train] Iter 15176 | Loss 0.286859 | Grad 0.0357 \n","[2024-04-10 13:03:07,391::train::INFO] [Train] Iter 15177 | Loss 0.335094 | Grad 0.0658 \n","[2024-04-10 13:03:07,511::train::INFO] [Train] Iter 15178 | Loss 0.315227 | Grad 0.0624 \n","[2024-04-10 13:03:07,631::train::INFO] [Train] Iter 15179 | Loss 0.324415 | Grad 0.0591 \n","[2024-04-10 13:03:07,750::train::INFO] [Train] Iter 15180 | Loss 0.301022 | Grad 0.0656 \n","[2024-04-10 13:03:07,870::train::INFO] [Train] Iter 15181 | Loss 0.309627 | Grad 0.0428 \n","[2024-04-10 13:03:07,993::train::INFO] [Train] Iter 15182 | Loss 0.320516 | Grad 0.0391 \n","[2024-04-10 13:03:08,112::train::INFO] [Train] Iter 15183 | Loss 0.341628 | Grad 0.0639 \n","[2024-04-10 13:03:08,232::train::INFO] [Train] Iter 15184 | Loss 0.301297 | Grad 0.0403 \n","[2024-04-10 13:03:08,351::train::INFO] [Train] Iter 15185 | Loss 0.302439 | Grad 0.0524 \n","[2024-04-10 13:03:08,472::train::INFO] [Train] Iter 15186 | Loss 0.314459 | Grad 0.0713 \n","[2024-04-10 13:03:08,591::train::INFO] [Train] Iter 15187 | Loss 0.300072 | Grad 0.0573 \n","[2024-04-10 13:03:08,712::train::INFO] [Train] Iter 15188 | Loss 0.316921 | Grad 0.0617 \n","[2024-04-10 13:03:08,831::train::INFO] [Train] Iter 15189 | Loss 0.288362 | Grad 0.0610 \n","[2024-04-10 13:03:08,951::train::INFO] [Train] Iter 15190 | Loss 0.308552 | Grad 0.0369 \n","[2024-04-10 13:03:09,076::train::INFO] [Train] Iter 15191 | Loss 0.330346 | Grad 0.0498 \n","[2024-04-10 13:03:09,195::train::INFO] [Train] Iter 15192 | Loss 0.342917 | Grad 0.0480 \n","[2024-04-10 13:03:09,313::train::INFO] [Train] Iter 15193 | Loss 0.355941 | Grad 0.0743 \n","[2024-04-10 13:03:09,433::train::INFO] [Train] Iter 15194 | Loss 0.336023 | Grad 0.0719 \n","[2024-04-10 13:03:09,554::train::INFO] [Train] Iter 15195 | Loss 0.311055 | Grad 0.0537 \n","[2024-04-10 13:03:09,676::train::INFO] [Train] Iter 15196 | Loss 0.242879 | Grad 0.0480 \n","[2024-04-10 13:03:09,795::train::INFO] [Train] Iter 15197 | Loss 0.310119 | Grad 0.0512 \n","[2024-04-10 13:03:09,915::train::INFO] [Train] Iter 15198 | Loss 0.318826 | Grad 0.0410 \n","[2024-04-10 13:03:10,038::train::INFO] [Train] Iter 15199 | Loss 0.340845 | Grad 0.0596 \n","[2024-04-10 13:03:10,157::train::INFO] [Train] Iter 15200 | Loss 0.338924 | Grad 0.0605 \n","[2024-04-10 13:03:10,279::train::INFO] [Train] Iter 15201 | Loss 0.348523 | Grad 0.0677 \n","[2024-04-10 13:03:10,399::train::INFO] [Train] Iter 15202 | Loss 0.320116 | Grad 0.0633 \n","[2024-04-10 13:03:10,518::train::INFO] [Train] Iter 15203 | Loss 0.313033 | Grad 0.0326 \n","[2024-04-10 13:03:10,638::train::INFO] [Train] Iter 15204 | Loss 0.315465 | Grad 0.0315 \n","[2024-04-10 13:03:10,757::train::INFO] [Train] Iter 15205 | Loss 0.295748 | Grad 0.0504 \n","[2024-04-10 13:03:10,876::train::INFO] [Train] Iter 15206 | Loss 0.308317 | Grad 0.0399 \n","[2024-04-10 13:03:10,995::train::INFO] [Train] Iter 15207 | Loss 0.363184 | Grad 0.0609 \n","[2024-04-10 13:03:11,113::train::INFO] [Train] Iter 15208 | Loss 0.295968 | Grad 0.0407 \n","[2024-04-10 13:03:11,232::train::INFO] [Train] Iter 15209 | Loss 0.319552 | Grad 0.0495 \n","[2024-04-10 13:03:11,307::train::INFO] [Train] Iter 15210 | Loss 0.305615 | Grad 0.0612 \n","[2024-04-10 13:03:11,425::train::INFO] [Train] Iter 15211 | Loss 0.309055 | Grad 0.0422 \n","[2024-04-10 13:03:11,546::train::INFO] [Train] Iter 15212 | Loss 0.306384 | Grad 0.0418 \n","[2024-04-10 13:03:11,666::train::INFO] [Train] Iter 15213 | Loss 0.344395 | Grad 0.0548 \n","[2024-04-10 13:03:11,786::train::INFO] [Train] Iter 15214 | Loss 0.323545 | Grad 0.0343 \n","[2024-04-10 13:03:11,906::train::INFO] [Train] Iter 15215 | Loss 0.355367 | Grad 0.0599 \n","[2024-04-10 13:03:12,025::train::INFO] [Train] Iter 15216 | Loss 0.323620 | Grad 0.0454 \n","[2024-04-10 13:03:12,144::train::INFO] [Train] Iter 15217 | Loss 0.282751 | Grad 0.0742 \n","[2024-04-10 13:03:12,267::train::INFO] [Train] Iter 15218 | Loss 0.357776 | Grad 0.0681 \n","[2024-04-10 13:03:12,389::train::INFO] [Train] Iter 15219 | Loss 0.253336 | Grad 0.0253 \n","[2024-04-10 13:03:12,508::train::INFO] [Train] Iter 15220 | Loss 0.326899 | Grad 0.0410 \n","[2024-04-10 13:03:12,628::train::INFO] [Train] Iter 15221 | Loss 0.316682 | Grad 0.0537 \n","[2024-04-10 13:03:12,748::train::INFO] [Train] Iter 15222 | Loss 0.336380 | Grad 0.0625 \n","[2024-04-10 13:03:12,867::train::INFO] [Train] Iter 15223 | Loss 0.300314 | Grad 0.0465 \n","[2024-04-10 13:03:12,992::train::INFO] [Train] Iter 15224 | Loss 0.297740 | Grad 0.0441 \n","[2024-04-10 13:03:13,111::train::INFO] [Train] Iter 15225 | Loss 0.349131 | Grad 0.0525 \n","[2024-04-10 13:03:13,232::train::INFO] [Train] Iter 15226 | Loss 0.317663 | Grad 0.0569 \n","[2024-04-10 13:03:13,351::train::INFO] [Train] Iter 15227 | Loss 0.357685 | Grad 0.0821 \n","[2024-04-10 13:03:13,473::train::INFO] [Train] Iter 15228 | Loss 0.303120 | Grad 0.0340 \n","[2024-04-10 13:03:13,596::train::INFO] [Train] Iter 15229 | Loss 0.302670 | Grad 0.0376 \n","[2024-04-10 13:03:13,715::train::INFO] [Train] Iter 15230 | Loss 0.293744 | Grad 0.0564 \n","[2024-04-10 13:03:13,836::train::INFO] [Train] Iter 15231 | Loss 0.340268 | Grad 0.0789 \n","[2024-04-10 13:03:13,956::train::INFO] [Train] Iter 15232 | Loss 0.264139 | Grad 0.0471 \n","[2024-04-10 13:03:14,076::train::INFO] [Train] Iter 15233 | Loss 0.357446 | Grad 0.0951 \n","[2024-04-10 13:03:14,199::train::INFO] [Train] Iter 15234 | Loss 0.305276 | Grad 0.0537 \n","[2024-04-10 13:03:14,318::train::INFO] [Train] Iter 15235 | Loss 0.341842 | Grad 0.0599 \n","[2024-04-10 13:03:14,438::train::INFO] [Train] Iter 15236 | Loss 0.325767 | Grad 0.0537 \n","[2024-04-10 13:03:14,558::train::INFO] [Train] Iter 15237 | Loss 0.284605 | Grad 0.0452 \n","[2024-04-10 13:03:14,677::train::INFO] [Train] Iter 15238 | Loss 0.317908 | Grad 0.0519 \n","[2024-04-10 13:03:14,799::train::INFO] [Train] Iter 15239 | Loss 0.325480 | Grad 0.0616 \n","[2024-04-10 13:03:14,917::train::INFO] [Train] Iter 15240 | Loss 0.346588 | Grad 0.0756 \n","[2024-04-10 13:03:15,036::train::INFO] [Train] Iter 15241 | Loss 0.298939 | Grad 0.0483 \n","[2024-04-10 13:03:15,154::train::INFO] [Train] Iter 15242 | Loss 0.328851 | Grad 0.0566 \n","[2024-04-10 13:03:15,275::train::INFO] [Train] Iter 15243 | Loss 0.344594 | Grad 0.0394 \n","[2024-04-10 13:03:15,394::train::INFO] [Train] Iter 15244 | Loss 0.310513 | Grad 0.0472 \n","[2024-04-10 13:03:15,513::train::INFO] [Train] Iter 15245 | Loss 0.303399 | Grad 0.0435 \n","[2024-04-10 13:03:15,632::train::INFO] [Train] Iter 15246 | Loss 0.377309 | Grad 0.0716 \n","[2024-04-10 13:03:15,750::train::INFO] [Train] Iter 15247 | Loss 0.321489 | Grad 0.0522 \n","[2024-04-10 13:03:15,869::train::INFO] [Train] Iter 15248 | Loss 0.293898 | Grad 0.0419 \n","[2024-04-10 13:03:15,989::train::INFO] [Train] Iter 15249 | Loss 0.298032 | Grad 0.0475 \n","[2024-04-10 13:03:16,110::train::INFO] [Train] Iter 15250 | Loss 0.320369 | Grad 0.0450 \n","[2024-04-10 13:03:16,230::train::INFO] [Train] Iter 15251 | Loss 0.337364 | Grad 0.0514 \n","[2024-04-10 13:03:16,350::train::INFO] [Train] Iter 15252 | Loss 0.321142 | Grad 0.0608 \n","[2024-04-10 13:03:16,472::train::INFO] [Train] Iter 15253 | Loss 0.325976 | Grad 0.0715 \n","[2024-04-10 13:03:16,594::train::INFO] [Train] Iter 15254 | Loss 0.294062 | Grad 0.0393 \n","[2024-04-10 13:03:16,680::train::INFO] [Train] Iter 15255 | Loss 0.273564 | Grad 0.0519 \n","[2024-04-10 13:03:16,803::train::INFO] [Train] Iter 15256 | Loss 0.306236 | Grad 0.0603 \n","[2024-04-10 13:03:16,922::train::INFO] [Train] Iter 15257 | Loss 0.280517 | Grad 0.0317 \n","[2024-04-10 13:03:17,043::train::INFO] [Train] Iter 15258 | Loss 0.358549 | Grad 0.0580 \n","[2024-04-10 13:03:17,163::train::INFO] [Train] Iter 15259 | Loss 0.316763 | Grad 0.0595 \n","[2024-04-10 13:03:17,287::train::INFO] [Train] Iter 15260 | Loss 0.326029 | Grad 0.0560 \n","[2024-04-10 13:03:17,407::train::INFO] [Train] Iter 15261 | Loss 0.377333 | Grad 0.0646 \n","[2024-04-10 13:03:17,529::train::INFO] [Train] Iter 15262 | Loss 0.282468 | Grad 0.0855 \n","[2024-04-10 13:03:17,650::train::INFO] [Train] Iter 15263 | Loss 0.305385 | Grad 0.0387 \n","[2024-04-10 13:03:17,772::train::INFO] [Train] Iter 15264 | Loss 0.292630 | Grad 0.0326 \n","[2024-04-10 13:03:17,898::train::INFO] [Train] Iter 15265 | Loss 0.316416 | Grad 0.0443 \n","[2024-04-10 13:03:18,020::train::INFO] [Train] Iter 15266 | Loss 0.308132 | Grad 0.0410 \n","[2024-04-10 13:03:18,139::train::INFO] [Train] Iter 15267 | Loss 0.304949 | Grad 0.0583 \n","[2024-04-10 13:03:18,259::train::INFO] [Train] Iter 15268 | Loss 0.309004 | Grad 0.0409 \n","[2024-04-10 13:03:18,385::train::INFO] [Train] Iter 15269 | Loss 0.282332 | Grad 0.0635 \n","[2024-04-10 13:03:18,505::train::INFO] [Train] Iter 15270 | Loss 0.336277 | Grad 0.0772 \n","[2024-04-10 13:03:18,630::train::INFO] [Train] Iter 15271 | Loss 0.342108 | Grad 0.0674 \n","[2024-04-10 13:03:18,749::train::INFO] [Train] Iter 15272 | Loss 0.294817 | Grad 0.0418 \n","[2024-04-10 13:03:18,869::train::INFO] [Train] Iter 15273 | Loss 0.330983 | Grad 0.0463 \n","[2024-04-10 13:03:18,990::train::INFO] [Train] Iter 15274 | Loss 0.295320 | Grad 0.0438 \n","[2024-04-10 13:03:19,110::train::INFO] [Train] Iter 15275 | Loss 0.273635 | Grad 0.0560 \n","[2024-04-10 13:03:19,233::train::INFO] [Train] Iter 15276 | Loss 0.290318 | Grad 0.0714 \n","[2024-04-10 13:03:19,354::train::INFO] [Train] Iter 15277 | Loss 0.301221 | Grad 0.0729 \n","[2024-04-10 13:03:19,476::train::INFO] [Train] Iter 15278 | Loss 0.343351 | Grad 0.0530 \n","[2024-04-10 13:03:19,597::train::INFO] [Train] Iter 15279 | Loss 0.320502 | Grad 0.0363 \n","[2024-04-10 13:03:19,716::train::INFO] [Train] Iter 15280 | Loss 0.331950 | Grad 0.0534 \n","[2024-04-10 13:03:19,836::train::INFO] [Train] Iter 15281 | Loss 0.323518 | Grad 0.0442 \n","[2024-04-10 13:03:19,954::train::INFO] [Train] Iter 15282 | Loss 0.331870 | Grad 0.0451 \n","[2024-04-10 13:03:20,074::train::INFO] [Train] Iter 15283 | Loss 0.343931 | Grad 0.0573 \n","[2024-04-10 13:03:20,192::train::INFO] [Train] Iter 15284 | Loss 0.291772 | Grad 0.0440 \n","[2024-04-10 13:03:20,310::train::INFO] [Train] Iter 15285 | Loss 0.296607 | Grad 0.0566 \n","[2024-04-10 13:03:20,432::train::INFO] [Train] Iter 15286 | Loss 0.288972 | Grad 0.0576 \n","[2024-04-10 13:03:20,551::train::INFO] [Train] Iter 15287 | Loss 0.310043 | Grad 0.0409 \n","[2024-04-10 13:03:20,670::train::INFO] [Train] Iter 15288 | Loss 0.281864 | Grad 0.0405 \n","[2024-04-10 13:03:20,792::train::INFO] [Train] Iter 15289 | Loss 0.274799 | Grad 0.0443 \n","[2024-04-10 13:03:20,910::train::INFO] [Train] Iter 15290 | Loss 0.351191 | Grad 0.0468 \n","[2024-04-10 13:03:21,030::train::INFO] [Train] Iter 15291 | Loss 0.264623 | Grad 0.0292 \n","[2024-04-10 13:03:21,149::train::INFO] [Train] Iter 15292 | Loss 0.300590 | Grad 0.0417 \n","[2024-04-10 13:03:21,267::train::INFO] [Train] Iter 15293 | Loss 0.299002 | Grad 0.0602 \n","[2024-04-10 13:03:21,387::train::INFO] [Train] Iter 15294 | Loss 0.284177 | Grad 0.0564 \n","[2024-04-10 13:03:21,508::train::INFO] [Train] Iter 15295 | Loss 0.316570 | Grad 0.0468 \n","[2024-04-10 13:03:21,629::train::INFO] [Train] Iter 15296 | Loss 0.333712 | Grad 0.0470 \n","[2024-04-10 13:03:21,748::train::INFO] [Train] Iter 15297 | Loss 0.329069 | Grad 0.0330 \n","[2024-04-10 13:03:21,868::train::INFO] [Train] Iter 15298 | Loss 0.345426 | Grad 0.0686 \n","[2024-04-10 13:03:21,987::train::INFO] [Train] Iter 15299 | Loss 0.315954 | Grad 0.0549 \n","[2024-04-10 13:03:22,062::train::INFO] [Train] Iter 15300 | Loss 0.283636 | Grad 0.0829 \n","[2024-04-10 13:03:22,182::train::INFO] [Train] Iter 15301 | Loss 0.314937 | Grad 0.0589 \n","[2024-04-10 13:03:22,302::train::INFO] [Train] Iter 15302 | Loss 0.355699 | Grad 0.0466 \n","[2024-04-10 13:03:22,420::train::INFO] [Train] Iter 15303 | Loss 0.323722 | Grad 0.0377 \n","[2024-04-10 13:03:22,539::train::INFO] [Train] Iter 15304 | Loss 0.304190 | Grad 0.0428 \n","[2024-04-10 13:03:22,658::train::INFO] [Train] Iter 15305 | Loss 0.275289 | Grad 0.0471 \n","[2024-04-10 13:03:22,776::train::INFO] [Train] Iter 15306 | Loss 0.349794 | Grad 0.0786 \n","[2024-04-10 13:03:22,895::train::INFO] [Train] Iter 15307 | Loss 0.335154 | Grad 0.1075 \n","[2024-04-10 13:03:23,015::train::INFO] [Train] Iter 15308 | Loss 0.326511 | Grad 0.0704 \n","[2024-04-10 13:03:23,135::train::INFO] [Train] Iter 15309 | Loss 0.337253 | Grad 0.0486 \n","[2024-04-10 13:03:23,256::train::INFO] [Train] Iter 15310 | Loss 0.335762 | Grad 0.0470 \n","[2024-04-10 13:03:23,377::train::INFO] [Train] Iter 15311 | Loss 0.323464 | Grad 0.0451 \n","[2024-04-10 13:03:23,497::train::INFO] [Train] Iter 15312 | Loss 0.315202 | Grad 0.0629 \n","[2024-04-10 13:03:23,618::train::INFO] [Train] Iter 15313 | Loss 0.335664 | Grad 0.0581 \n","[2024-04-10 13:03:23,737::train::INFO] [Train] Iter 15314 | Loss 0.324521 | Grad 0.0879 \n","[2024-04-10 13:03:23,863::train::INFO] [Train] Iter 15315 | Loss 0.333827 | Grad 0.0509 \n","[2024-04-10 13:03:23,983::train::INFO] [Train] Iter 15316 | Loss 0.260670 | Grad 0.0365 \n","[2024-04-10 13:03:24,101::train::INFO] [Train] Iter 15317 | Loss 0.327620 | Grad 0.0453 \n","[2024-04-10 13:03:24,221::train::INFO] [Train] Iter 15318 | Loss 0.333738 | Grad 0.0472 \n","[2024-04-10 13:03:24,341::train::INFO] [Train] Iter 15319 | Loss 0.343629 | Grad 0.0453 \n","[2024-04-10 13:03:24,465::train::INFO] [Train] Iter 15320 | Loss 0.338240 | Grad 0.0483 \n","[2024-04-10 13:03:24,584::train::INFO] [Train] Iter 15321 | Loss 0.310242 | Grad 0.0774 \n","[2024-04-10 13:03:24,704::train::INFO] [Train] Iter 15322 | Loss 0.341217 | Grad 0.0848 \n","[2024-04-10 13:03:24,824::train::INFO] [Train] Iter 15323 | Loss 0.353485 | Grad 0.0482 \n","[2024-04-10 13:03:24,943::train::INFO] [Train] Iter 15324 | Loss 0.330346 | Grad 0.0545 \n","[2024-04-10 13:03:25,065::train::INFO] [Train] Iter 15325 | Loss 0.308405 | Grad 0.0496 \n","[2024-04-10 13:03:25,184::train::INFO] [Train] Iter 15326 | Loss 0.326291 | Grad 0.0441 \n","[2024-04-10 13:03:25,303::train::INFO] [Train] Iter 15327 | Loss 0.296053 | Grad 0.0411 \n","[2024-04-10 13:03:25,422::train::INFO] [Train] Iter 15328 | Loss 0.294163 | Grad 0.0484 \n","[2024-04-10 13:03:25,543::train::INFO] [Train] Iter 15329 | Loss 0.318787 | Grad 0.0665 \n","[2024-04-10 13:03:25,666::train::INFO] [Train] Iter 15330 | Loss 0.305779 | Grad 0.0346 \n","[2024-04-10 13:03:25,787::train::INFO] [Train] Iter 15331 | Loss 0.302030 | Grad 0.0523 \n","[2024-04-10 13:03:25,907::train::INFO] [Train] Iter 15332 | Loss 0.328549 | Grad 0.0441 \n","[2024-04-10 13:03:26,027::train::INFO] [Train] Iter 15333 | Loss 0.296312 | Grad 0.0374 \n","[2024-04-10 13:03:26,147::train::INFO] [Train] Iter 15334 | Loss 0.296953 | Grad 0.0602 \n","[2024-04-10 13:03:26,269::train::INFO] [Train] Iter 15335 | Loss 0.281536 | Grad 0.0309 \n","[2024-04-10 13:03:26,388::train::INFO] [Train] Iter 15336 | Loss 0.309306 | Grad 0.0354 \n","[2024-04-10 13:03:26,507::train::INFO] [Train] Iter 15337 | Loss 0.325133 | Grad 0.0463 \n","[2024-04-10 13:03:26,630::train::INFO] [Train] Iter 15338 | Loss 0.333489 | Grad 0.0683 \n","[2024-04-10 13:03:26,750::train::INFO] [Train] Iter 15339 | Loss 0.351378 | Grad 0.0340 \n","[2024-04-10 13:03:26,872::train::INFO] [Train] Iter 15340 | Loss 0.357456 | Grad 0.0530 \n","[2024-04-10 13:03:26,991::train::INFO] [Train] Iter 15341 | Loss 0.345291 | Grad 0.0612 \n","[2024-04-10 13:03:27,111::train::INFO] [Train] Iter 15342 | Loss 0.335321 | Grad 0.0476 \n","[2024-04-10 13:03:27,232::train::INFO] [Train] Iter 15343 | Loss 0.311016 | Grad 0.0461 \n","[2024-04-10 13:03:27,352::train::INFO] [Train] Iter 15344 | Loss 0.343563 | Grad 0.0553 \n","[2024-04-10 13:03:27,428::train::INFO] [Train] Iter 15345 | Loss 0.267734 | Grad 0.0254 \n","[2024-04-10 13:03:27,551::train::INFO] [Train] Iter 15346 | Loss 0.325714 | Grad 0.0476 \n","[2024-04-10 13:03:27,673::train::INFO] [Train] Iter 15347 | Loss 0.335959 | Grad 0.0495 \n","[2024-04-10 13:03:27,793::train::INFO] [Train] Iter 15348 | Loss 0.317010 | Grad 0.0429 \n","[2024-04-10 13:03:27,912::train::INFO] [Train] Iter 15349 | Loss 0.315713 | Grad 0.0331 \n","[2024-04-10 13:03:28,032::train::INFO] [Train] Iter 15350 | Loss 0.321820 | Grad 0.0905 \n","[2024-04-10 13:03:28,151::train::INFO] [Train] Iter 15351 | Loss 0.333669 | Grad 0.0607 \n","[2024-04-10 13:03:28,273::train::INFO] [Train] Iter 15352 | Loss 0.352056 | Grad 0.0539 \n","[2024-04-10 13:03:28,391::train::INFO] [Train] Iter 15353 | Loss 0.336460 | Grad 0.0684 \n","[2024-04-10 13:03:28,510::train::INFO] [Train] Iter 15354 | Loss 0.338196 | Grad 0.0496 \n","[2024-04-10 13:03:28,628::train::INFO] [Train] Iter 15355 | Loss 0.297170 | Grad 0.0294 \n","[2024-04-10 13:03:28,747::train::INFO] [Train] Iter 15356 | Loss 0.326189 | Grad 0.0430 \n","[2024-04-10 13:03:28,866::train::INFO] [Train] Iter 15357 | Loss 0.336500 | Grad 0.0584 \n","[2024-04-10 13:03:28,986::train::INFO] [Train] Iter 15358 | Loss 0.330483 | Grad 0.0517 \n","[2024-04-10 13:03:29,105::train::INFO] [Train] Iter 15359 | Loss 0.279344 | Grad 0.0599 \n","[2024-04-10 13:03:29,225::train::INFO] [Train] Iter 15360 | Loss 0.347273 | Grad 0.1044 \n","[2024-04-10 13:03:29,346::train::INFO] [Train] Iter 15361 | Loss 0.342091 | Grad 0.0536 \n","[2024-04-10 13:03:29,465::train::INFO] [Train] Iter 15362 | Loss 0.340068 | Grad 0.0440 \n","[2024-04-10 13:03:29,587::train::INFO] [Train] Iter 15363 | Loss 0.323806 | Grad 0.0410 \n","[2024-04-10 13:03:29,713::train::INFO] [Train] Iter 15364 | Loss 0.348564 | Grad 0.0564 \n","[2024-04-10 13:03:29,835::train::INFO] [Train] Iter 15365 | Loss 0.329939 | Grad 0.0479 \n","[2024-04-10 13:03:29,956::train::INFO] [Train] Iter 15366 | Loss 0.304294 | Grad 0.0562 \n","[2024-04-10 13:03:30,080::train::INFO] [Train] Iter 15367 | Loss 0.303012 | Grad 0.0559 \n","[2024-04-10 13:03:30,199::train::INFO] [Train] Iter 15368 | Loss 0.340245 | Grad 0.0654 \n","[2024-04-10 13:03:30,320::train::INFO] [Train] Iter 15369 | Loss 0.298222 | Grad 0.0639 \n","[2024-04-10 13:03:30,439::train::INFO] [Train] Iter 15370 | Loss 0.326574 | Grad 0.0565 \n","[2024-04-10 13:03:30,561::train::INFO] [Train] Iter 15371 | Loss 0.338705 | Grad 0.0427 \n","[2024-04-10 13:03:30,683::train::INFO] [Train] Iter 15372 | Loss 0.347740 | Grad 0.0539 \n","[2024-04-10 13:03:30,811::train::INFO] [Train] Iter 15373 | Loss 0.327009 | Grad 0.0601 \n","[2024-04-10 13:03:30,933::train::INFO] [Train] Iter 15374 | Loss 0.294582 | Grad 0.0656 \n","[2024-04-10 13:03:31,054::train::INFO] [Train] Iter 15375 | Loss 0.299196 | Grad 0.0519 \n","[2024-04-10 13:03:31,180::train::INFO] [Train] Iter 15376 | Loss 0.338746 | Grad 0.0887 \n","[2024-04-10 13:03:31,300::train::INFO] [Train] Iter 15377 | Loss 0.319565 | Grad 0.0417 \n","[2024-04-10 13:03:31,425::train::INFO] [Train] Iter 15378 | Loss 0.246447 | Grad 0.0363 \n","[2024-04-10 13:03:31,546::train::INFO] [Train] Iter 15379 | Loss 0.330743 | Grad 0.0434 \n","[2024-04-10 13:03:31,667::train::INFO] [Train] Iter 15380 | Loss 0.294301 | Grad 0.0363 \n","[2024-04-10 13:03:31,786::train::INFO] [Train] Iter 15381 | Loss 0.328906 | Grad 0.0636 \n","[2024-04-10 13:03:31,907::train::INFO] [Train] Iter 15382 | Loss 0.323873 | Grad 0.0472 \n","[2024-04-10 13:03:32,028::train::INFO] [Train] Iter 15383 | Loss 0.329316 | Grad 0.0646 \n","[2024-04-10 13:03:32,150::train::INFO] [Train] Iter 15384 | Loss 0.318139 | Grad 0.0538 \n","[2024-04-10 13:03:32,272::train::INFO] [Train] Iter 15385 | Loss 0.280438 | Grad 0.0510 \n","[2024-04-10 13:03:32,396::train::INFO] [Train] Iter 15386 | Loss 0.301017 | Grad 0.0422 \n","[2024-04-10 13:03:32,518::train::INFO] [Train] Iter 15387 | Loss 0.335150 | Grad 0.0564 \n","[2024-04-10 13:03:32,638::train::INFO] [Train] Iter 15388 | Loss 0.343215 | Grad 0.0670 \n","[2024-04-10 13:03:32,757::train::INFO] [Train] Iter 15389 | Loss 0.308591 | Grad 0.0667 \n","[2024-04-10 13:03:32,833::train::INFO] [Train] Iter 15390 | Loss 0.227457 | Grad 0.0296 \n","[2024-04-10 13:03:32,951::train::INFO] [Train] Iter 15391 | Loss 0.319274 | Grad 0.0415 \n","[2024-04-10 13:03:33,070::train::INFO] [Train] Iter 15392 | Loss 0.304943 | Grad 0.0443 \n","[2024-04-10 13:03:33,190::train::INFO] [Train] Iter 15393 | Loss 0.306341 | Grad 0.0338 \n","[2024-04-10 13:03:33,311::train::INFO] [Train] Iter 15394 | Loss 0.315152 | Grad 0.0512 \n","[2024-04-10 13:03:33,431::train::INFO] [Train] Iter 15395 | Loss 0.338868 | Grad 0.0570 \n","[2024-04-10 13:03:33,551::train::INFO] [Train] Iter 15396 | Loss 0.289965 | Grad 0.0402 \n","[2024-04-10 13:03:33,670::train::INFO] [Train] Iter 15397 | Loss 0.334180 | Grad 0.0722 \n","[2024-04-10 13:03:33,790::train::INFO] [Train] Iter 15398 | Loss 0.260438 | Grad 0.0584 \n","[2024-04-10 13:03:33,915::train::INFO] [Train] Iter 15399 | Loss 0.339639 | Grad 0.0513 \n","[2024-04-10 13:03:34,034::train::INFO] [Train] Iter 15400 | Loss 0.359770 | Grad 0.0606 \n","[2024-04-10 13:03:34,153::train::INFO] [Train] Iter 15401 | Loss 0.350517 | Grad 0.0465 \n","[2024-04-10 13:03:34,272::train::INFO] [Train] Iter 15402 | Loss 0.300339 | Grad 0.0414 \n","[2024-04-10 13:03:34,392::train::INFO] [Train] Iter 15403 | Loss 0.330606 | Grad 0.0642 \n","[2024-04-10 13:03:34,512::train::INFO] [Train] Iter 15404 | Loss 0.323439 | Grad 0.0750 \n","[2024-04-10 13:03:34,633::train::INFO] [Train] Iter 15405 | Loss 0.341005 | Grad 0.0768 \n","[2024-04-10 13:03:34,753::train::INFO] [Train] Iter 15406 | Loss 0.306179 | Grad 0.0676 \n","[2024-04-10 13:03:34,875::train::INFO] [Train] Iter 15407 | Loss 0.321422 | Grad 0.0627 \n","[2024-04-10 13:03:34,995::train::INFO] [Train] Iter 15408 | Loss 0.301847 | Grad 0.0417 \n","[2024-04-10 13:03:35,117::train::INFO] [Train] Iter 15409 | Loss 0.331031 | Grad 0.0509 \n","[2024-04-10 13:03:35,235::train::INFO] [Train] Iter 15410 | Loss 0.316184 | Grad 0.0654 \n","[2024-04-10 13:03:35,354::train::INFO] [Train] Iter 15411 | Loss 0.295404 | Grad 0.0569 \n","[2024-04-10 13:03:35,474::train::INFO] [Train] Iter 15412 | Loss 0.321297 | Grad 0.0830 \n","[2024-04-10 13:03:35,593::train::INFO] [Train] Iter 15413 | Loss 0.319074 | Grad 0.0766 \n","[2024-04-10 13:03:35,716::train::INFO] [Train] Iter 15414 | Loss 0.321186 | Grad 0.0584 \n","[2024-04-10 13:03:35,835::train::INFO] [Train] Iter 15415 | Loss 0.299726 | Grad 0.0330 \n","[2024-04-10 13:03:35,956::train::INFO] [Train] Iter 15416 | Loss 0.329669 | Grad 0.0556 \n","[2024-04-10 13:03:36,076::train::INFO] [Train] Iter 15417 | Loss 0.317686 | Grad 0.0633 \n","[2024-04-10 13:03:36,195::train::INFO] [Train] Iter 15418 | Loss 0.354373 | Grad 0.0427 \n","[2024-04-10 13:03:36,317::train::INFO] [Train] Iter 15419 | Loss 0.288879 | Grad 0.0522 \n","[2024-04-10 13:03:36,436::train::INFO] [Train] Iter 15420 | Loss 0.340964 | Grad 0.0744 \n","[2024-04-10 13:03:36,556::train::INFO] [Train] Iter 15421 | Loss 0.359713 | Grad 0.0703 \n","[2024-04-10 13:03:36,675::train::INFO] [Train] Iter 15422 | Loss 0.340315 | Grad 0.0714 \n","[2024-04-10 13:03:36,793::train::INFO] [Train] Iter 15423 | Loss 0.298860 | Grad 0.0425 \n","[2024-04-10 13:03:36,914::train::INFO] [Train] Iter 15424 | Loss 0.298423 | Grad 0.0620 \n","[2024-04-10 13:03:37,033::train::INFO] [Train] Iter 15425 | Loss 0.312147 | Grad 0.0387 \n","[2024-04-10 13:03:37,151::train::INFO] [Train] Iter 15426 | Loss 0.351097 | Grad 0.0472 \n","[2024-04-10 13:03:37,269::train::INFO] [Train] Iter 15427 | Loss 0.316425 | Grad 0.0507 \n","[2024-04-10 13:03:37,388::train::INFO] [Train] Iter 15428 | Loss 0.303832 | Grad 0.0501 \n","[2024-04-10 13:03:37,507::train::INFO] [Train] Iter 15429 | Loss 0.270431 | Grad 0.0513 \n","[2024-04-10 13:03:37,628::train::INFO] [Train] Iter 15430 | Loss 0.304558 | Grad 0.0400 \n","[2024-04-10 13:03:37,748::train::INFO] [Train] Iter 15431 | Loss 0.369421 | Grad 0.0638 \n","[2024-04-10 13:03:37,868::train::INFO] [Train] Iter 15432 | Loss 0.314567 | Grad 0.0406 \n","[2024-04-10 13:03:37,988::train::INFO] [Train] Iter 15433 | Loss 0.295456 | Grad 0.0612 \n","[2024-04-10 13:03:38,106::train::INFO] [Train] Iter 15434 | Loss 0.274289 | Grad 0.0366 \n","[2024-04-10 13:03:38,183::train::INFO] [Train] Iter 15435 | Loss 0.299875 | Grad 0.0594 \n","[2024-04-10 13:03:38,303::train::INFO] [Train] Iter 15436 | Loss 0.294615 | Grad 0.0666 \n","[2024-04-10 13:03:38,422::train::INFO] [Train] Iter 15437 | Loss 0.326273 | Grad 0.1033 \n","[2024-04-10 13:03:38,542::train::INFO] [Train] Iter 15438 | Loss 0.326564 | Grad 0.0455 \n","[2024-04-10 13:03:38,661::train::INFO] [Train] Iter 15439 | Loss 0.333707 | Grad 0.0353 \n","[2024-04-10 13:03:38,779::train::INFO] [Train] Iter 15440 | Loss 0.301795 | Grad 0.0656 \n","[2024-04-10 13:03:38,898::train::INFO] [Train] Iter 15441 | Loss 0.335110 | Grad 0.0928 \n","[2024-04-10 13:03:39,016::train::INFO] [Train] Iter 15442 | Loss 0.296582 | Grad 0.0734 \n","[2024-04-10 13:03:39,139::train::INFO] [Train] Iter 15443 | Loss 0.300328 | Grad 0.0765 \n","[2024-04-10 13:03:39,260::train::INFO] [Train] Iter 15444 | Loss 0.300344 | Grad 0.0451 \n","[2024-04-10 13:03:39,378::train::INFO] [Train] Iter 15445 | Loss 0.338050 | Grad 0.0605 \n","[2024-04-10 13:03:39,497::train::INFO] [Train] Iter 15446 | Loss 0.339967 | Grad 0.0445 \n","[2024-04-10 13:03:39,617::train::INFO] [Train] Iter 15447 | Loss 0.339106 | Grad 0.0551 \n","[2024-04-10 13:03:39,741::train::INFO] [Train] Iter 15448 | Loss 0.325844 | Grad 0.0585 \n","[2024-04-10 13:03:39,860::train::INFO] [Train] Iter 15449 | Loss 0.337904 | Grad 0.0760 \n","[2024-04-10 13:03:39,979::train::INFO] [Train] Iter 15450 | Loss 0.405122 | Grad 0.0702 \n","[2024-04-10 13:03:40,099::train::INFO] [Train] Iter 15451 | Loss 0.323528 | Grad 0.0556 \n","[2024-04-10 13:03:40,217::train::INFO] [Train] Iter 15452 | Loss 0.342741 | Grad 0.0669 \n","[2024-04-10 13:03:40,337::train::INFO] [Train] Iter 15453 | Loss 0.351468 | Grad 0.0459 \n","[2024-04-10 13:03:40,456::train::INFO] [Train] Iter 15454 | Loss 0.353850 | Grad 0.0437 \n","[2024-04-10 13:03:40,577::train::INFO] [Train] Iter 15455 | Loss 0.327580 | Grad 0.0467 \n","[2024-04-10 13:03:40,696::train::INFO] [Train] Iter 15456 | Loss 0.322809 | Grad 0.0681 \n","[2024-04-10 13:03:40,814::train::INFO] [Train] Iter 15457 | Loss 0.296803 | Grad 0.0333 \n","[2024-04-10 13:03:40,934::train::INFO] [Train] Iter 15458 | Loss 0.285900 | Grad 0.0592 \n","[2024-04-10 13:03:41,056::train::INFO] [Train] Iter 15459 | Loss 0.366604 | Grad 0.0912 \n","[2024-04-10 13:03:41,175::train::INFO] [Train] Iter 15460 | Loss 0.314885 | Grad 0.0438 \n","[2024-04-10 13:03:41,294::train::INFO] [Train] Iter 15461 | Loss 0.327161 | Grad 0.0597 \n","[2024-04-10 13:03:41,414::train::INFO] [Train] Iter 15462 | Loss 0.311749 | Grad 0.0460 \n","[2024-04-10 13:03:41,535::train::INFO] [Train] Iter 15463 | Loss 0.271787 | Grad 0.0420 \n","[2024-04-10 13:03:41,654::train::INFO] [Train] Iter 15464 | Loss 0.263177 | Grad 0.0318 \n","[2024-04-10 13:03:41,774::train::INFO] [Train] Iter 15465 | Loss 0.286747 | Grad 0.0383 \n","[2024-04-10 13:03:41,893::train::INFO] [Train] Iter 15466 | Loss 0.313548 | Grad 0.0651 \n","[2024-04-10 13:03:42,013::train::INFO] [Train] Iter 15467 | Loss 0.302323 | Grad 0.0581 \n","[2024-04-10 13:03:42,135::train::INFO] [Train] Iter 15468 | Loss 0.283191 | Grad 0.0383 \n","[2024-04-10 13:03:42,255::train::INFO] [Train] Iter 15469 | Loss 0.301389 | Grad 0.0470 \n","[2024-04-10 13:03:42,374::train::INFO] [Train] Iter 15470 | Loss 0.324169 | Grad 0.0416 \n","[2024-04-10 13:03:42,494::train::INFO] [Train] Iter 15471 | Loss 0.331928 | Grad 0.0468 \n","[2024-04-10 13:03:42,613::train::INFO] [Train] Iter 15472 | Loss 0.285955 | Grad 0.0422 \n","[2024-04-10 13:03:42,739::train::INFO] [Train] Iter 15473 | Loss 0.362320 | Grad 0.0607 \n","[2024-04-10 13:03:42,862::train::INFO] [Train] Iter 15474 | Loss 0.325666 | Grad 0.0673 \n","[2024-04-10 13:03:42,989::train::INFO] [Train] Iter 15475 | Loss 0.358026 | Grad 0.0751 \n","[2024-04-10 13:03:43,110::train::INFO] [Train] Iter 15476 | Loss 0.341447 | Grad 0.0441 \n","[2024-04-10 13:03:43,232::train::INFO] [Train] Iter 15477 | Loss 0.364143 | Grad 0.0534 \n","[2024-04-10 13:03:43,354::train::INFO] [Train] Iter 15478 | Loss 0.299634 | Grad 0.0535 \n","[2024-04-10 13:03:43,478::train::INFO] [Train] Iter 15479 | Loss 0.286632 | Grad 0.0577 \n","[2024-04-10 13:03:43,553::train::INFO] [Train] Iter 15480 | Loss 0.278468 | Grad 0.0489 \n","[2024-04-10 13:03:43,673::train::INFO] [Train] Iter 15481 | Loss 0.331106 | Grad 0.0561 \n","[2024-04-10 13:03:43,798::train::INFO] [Train] Iter 15482 | Loss 0.332443 | Grad 0.0459 \n","[2024-04-10 13:03:43,920::train::INFO] [Train] Iter 15483 | Loss 0.360007 | Grad 0.0397 \n","[2024-04-10 13:03:44,041::train::INFO] [Train] Iter 15484 | Loss 0.305312 | Grad 0.0257 \n","[2024-04-10 13:03:44,160::train::INFO] [Train] Iter 15485 | Loss 0.291918 | Grad 0.0496 \n","[2024-04-10 13:03:44,282::train::INFO] [Train] Iter 15486 | Loss 0.301174 | Grad 0.0396 \n","[2024-04-10 13:03:44,401::train::INFO] [Train] Iter 15487 | Loss 0.368424 | Grad 0.0769 \n","[2024-04-10 13:03:44,524::train::INFO] [Train] Iter 15488 | Loss 0.281778 | Grad 0.0357 \n","[2024-04-10 13:03:44,645::train::INFO] [Train] Iter 15489 | Loss 0.291374 | Grad 0.0413 \n","[2024-04-10 13:03:44,773::train::INFO] [Train] Iter 15490 | Loss 0.291314 | Grad 0.0507 \n","[2024-04-10 13:03:44,894::train::INFO] [Train] Iter 15491 | Loss 0.248298 | Grad 0.0220 \n","[2024-04-10 13:03:45,015::train::INFO] [Train] Iter 15492 | Loss 0.289916 | Grad 0.0409 \n","[2024-04-10 13:03:45,139::train::INFO] [Train] Iter 15493 | Loss 0.318318 | Grad 0.0260 \n","[2024-04-10 13:03:45,259::train::INFO] [Train] Iter 15494 | Loss 0.335276 | Grad 0.0555 \n","[2024-04-10 13:03:45,382::train::INFO] [Train] Iter 15495 | Loss 0.280716 | Grad 0.0453 \n","[2024-04-10 13:03:45,503::train::INFO] [Train] Iter 15496 | Loss 0.280842 | Grad 0.0620 \n","[2024-04-10 13:03:45,625::train::INFO] [Train] Iter 15497 | Loss 0.375313 | Grad 0.1215 \n","[2024-04-10 13:03:45,748::train::INFO] [Train] Iter 15498 | Loss 0.284384 | Grad 0.0370 \n","[2024-04-10 13:03:45,867::train::INFO] [Train] Iter 15499 | Loss 0.362346 | Grad 0.0554 \n","[2024-04-10 13:03:45,988::train::INFO] [Train] Iter 15500 | Loss 0.306407 | Grad 0.0342 \n","[2024-04-10 13:03:46,107::train::INFO] [Train] Iter 15501 | Loss 0.307558 | Grad 0.0558 \n","[2024-04-10 13:03:46,228::train::INFO] [Train] Iter 15502 | Loss 0.326580 | Grad 0.0584 \n","[2024-04-10 13:03:46,348::train::INFO] [Train] Iter 15503 | Loss 0.260233 | Grad 0.0501 \n","[2024-04-10 13:03:46,468::train::INFO] [Train] Iter 15504 | Loss 0.310934 | Grad 0.0602 \n","[2024-04-10 13:03:46,588::train::INFO] [Train] Iter 15505 | Loss 0.330602 | Grad 0.0851 \n","[2024-04-10 13:03:46,708::train::INFO] [Train] Iter 15506 | Loss 0.317637 | Grad 0.0705 \n","[2024-04-10 13:03:46,827::train::INFO] [Train] Iter 15507 | Loss 0.308523 | Grad 0.0395 \n","[2024-04-10 13:03:46,946::train::INFO] [Train] Iter 15508 | Loss 0.314237 | Grad 0.0331 \n","[2024-04-10 13:03:47,064::train::INFO] [Train] Iter 15509 | Loss 0.369828 | Grad 0.0582 \n","[2024-04-10 13:03:47,185::train::INFO] [Train] Iter 15510 | Loss 0.325922 | Grad 0.0464 \n","[2024-04-10 13:03:47,304::train::INFO] [Train] Iter 15511 | Loss 0.342619 | Grad 0.0738 \n","[2024-04-10 13:03:47,424::train::INFO] [Train] Iter 15512 | Loss 0.323723 | Grad 0.0598 \n","[2024-04-10 13:03:47,545::train::INFO] [Train] Iter 15513 | Loss 0.331006 | Grad 0.0448 \n","[2024-04-10 13:03:47,665::train::INFO] [Train] Iter 15514 | Loss 0.284960 | Grad 0.0373 \n","[2024-04-10 13:03:47,791::train::INFO] [Train] Iter 15515 | Loss 0.343881 | Grad 0.0482 \n","[2024-04-10 13:03:47,911::train::INFO] [Train] Iter 15516 | Loss 0.301561 | Grad 0.0353 \n","[2024-04-10 13:03:48,031::train::INFO] [Train] Iter 15517 | Loss 0.314925 | Grad 0.0557 \n","[2024-04-10 13:03:48,150::train::INFO] [Train] Iter 15518 | Loss 0.279738 | Grad 0.0306 \n","[2024-04-10 13:03:48,269::train::INFO] [Train] Iter 15519 | Loss 0.290041 | Grad 0.0422 \n","[2024-04-10 13:03:48,389::train::INFO] [Train] Iter 15520 | Loss 0.314403 | Grad 0.0461 \n","[2024-04-10 13:03:48,508::train::INFO] [Train] Iter 15521 | Loss 0.329744 | Grad 0.0398 \n","[2024-04-10 13:03:48,626::train::INFO] [Train] Iter 15522 | Loss 0.296841 | Grad 0.0436 \n","[2024-04-10 13:03:48,745::train::INFO] [Train] Iter 15523 | Loss 0.352076 | Grad 0.0456 \n","[2024-04-10 13:03:48,865::train::INFO] [Train] Iter 15524 | Loss 0.337367 | Grad 0.0380 \n","[2024-04-10 13:03:48,939::train::INFO] [Train] Iter 15525 | Loss 0.274854 | Grad 0.0461 \n","[2024-04-10 13:03:49,060::train::INFO] [Train] Iter 15526 | Loss 0.315353 | Grad 0.0531 \n","[2024-04-10 13:03:49,181::train::INFO] [Train] Iter 15527 | Loss 0.313455 | Grad 0.0610 \n","[2024-04-10 13:03:49,302::train::INFO] [Train] Iter 15528 | Loss 0.304638 | Grad 0.0587 \n","[2024-04-10 13:03:49,422::train::INFO] [Train] Iter 15529 | Loss 0.311796 | Grad 0.0427 \n","[2024-04-10 13:03:49,542::train::INFO] [Train] Iter 15530 | Loss 0.326667 | Grad 0.0413 \n","[2024-04-10 13:03:49,660::train::INFO] [Train] Iter 15531 | Loss 0.299559 | Grad 0.0382 \n","[2024-04-10 13:03:49,778::train::INFO] [Train] Iter 15532 | Loss 0.340150 | Grad 0.0843 \n","[2024-04-10 13:03:49,896::train::INFO] [Train] Iter 15533 | Loss 0.338420 | Grad 0.0901 \n","[2024-04-10 13:03:50,015::train::INFO] [Train] Iter 15534 | Loss 0.336156 | Grad 0.0597 \n","[2024-04-10 13:03:50,134::train::INFO] [Train] Iter 15535 | Loss 0.262093 | Grad 0.0281 \n","[2024-04-10 13:03:50,254::train::INFO] [Train] Iter 15536 | Loss 0.322693 | Grad 0.0473 \n","[2024-04-10 13:03:50,374::train::INFO] [Train] Iter 15537 | Loss 0.327562 | Grad 0.0643 \n","[2024-04-10 13:03:50,493::train::INFO] [Train] Iter 15538 | Loss 0.307470 | Grad 0.0388 \n","[2024-04-10 13:03:50,617::train::INFO] [Train] Iter 15539 | Loss 0.335163 | Grad 0.0468 \n","[2024-04-10 13:03:50,736::train::INFO] [Train] Iter 15540 | Loss 0.295311 | Grad 0.0394 \n","[2024-04-10 13:03:50,854::train::INFO] [Train] Iter 15541 | Loss 0.319877 | Grad 0.0649 \n","[2024-04-10 13:03:50,974::train::INFO] [Train] Iter 15542 | Loss 0.322099 | Grad 0.0761 \n","[2024-04-10 13:03:51,093::train::INFO] [Train] Iter 15543 | Loss 0.290301 | Grad 0.0371 \n","[2024-04-10 13:03:51,214::train::INFO] [Train] Iter 15544 | Loss 0.325483 | Grad 0.0402 \n","[2024-04-10 13:03:51,334::train::INFO] [Train] Iter 15545 | Loss 0.315921 | Grad 0.0356 \n","[2024-04-10 13:03:51,455::train::INFO] [Train] Iter 15546 | Loss 0.292104 | Grad 0.0628 \n","[2024-04-10 13:03:51,577::train::INFO] [Train] Iter 15547 | Loss 0.312604 | Grad 0.0423 \n","[2024-04-10 13:03:51,698::train::INFO] [Train] Iter 15548 | Loss 0.303111 | Grad 0.0405 \n","[2024-04-10 13:03:51,819::train::INFO] [Train] Iter 15549 | Loss 0.287731 | Grad 0.0442 \n","[2024-04-10 13:03:51,938::train::INFO] [Train] Iter 15550 | Loss 0.315612 | Grad 0.0594 \n","[2024-04-10 13:03:52,058::train::INFO] [Train] Iter 15551 | Loss 0.300065 | Grad 0.0418 \n","[2024-04-10 13:03:52,178::train::INFO] [Train] Iter 15552 | Loss 0.305699 | Grad 0.0539 \n","[2024-04-10 13:03:52,298::train::INFO] [Train] Iter 15553 | Loss 0.332649 | Grad 0.0658 \n","[2024-04-10 13:03:52,420::train::INFO] [Train] Iter 15554 | Loss 0.304936 | Grad 0.0358 \n","[2024-04-10 13:03:52,540::train::INFO] [Train] Iter 15555 | Loss 0.286450 | Grad 0.0373 \n","[2024-04-10 13:03:52,660::train::INFO] [Train] Iter 15556 | Loss 0.304912 | Grad 0.0521 \n","[2024-04-10 13:03:52,780::train::INFO] [Train] Iter 15557 | Loss 0.327468 | Grad 0.0493 \n","[2024-04-10 13:03:52,901::train::INFO] [Train] Iter 15558 | Loss 0.295310 | Grad 0.0546 \n","[2024-04-10 13:03:53,020::train::INFO] [Train] Iter 15559 | Loss 0.327999 | Grad 0.0654 \n","[2024-04-10 13:03:53,140::train::INFO] [Train] Iter 15560 | Loss 0.272848 | Grad 0.0353 \n","[2024-04-10 13:03:53,260::train::INFO] [Train] Iter 15561 | Loss 0.270069 | Grad 0.0309 \n","[2024-04-10 13:03:53,381::train::INFO] [Train] Iter 15562 | Loss 0.304320 | Grad 0.0409 \n","[2024-04-10 13:03:53,502::train::INFO] [Train] Iter 15563 | Loss 0.297439 | Grad 0.0437 \n","[2024-04-10 13:03:53,621::train::INFO] [Train] Iter 15564 | Loss 0.357983 | Grad 0.0532 \n","[2024-04-10 13:03:53,740::train::INFO] [Train] Iter 15565 | Loss 0.276240 | Grad 0.0453 \n","[2024-04-10 13:03:53,859::train::INFO] [Train] Iter 15566 | Loss 0.311816 | Grad 0.0655 \n","[2024-04-10 13:03:53,978::train::INFO] [Train] Iter 15567 | Loss 0.319022 | Grad 0.0398 \n","[2024-04-10 13:03:54,098::train::INFO] [Train] Iter 15568 | Loss 0.327145 | Grad 0.0417 \n","[2024-04-10 13:03:54,219::train::INFO] [Train] Iter 15569 | Loss 0.315667 | Grad 0.0335 \n","[2024-04-10 13:03:54,297::train::INFO] [Train] Iter 15570 | Loss 0.350237 | Grad 0.0565 \n","[2024-04-10 13:03:54,420::train::INFO] [Train] Iter 15571 | Loss 0.352553 | Grad 0.0659 \n","[2024-04-10 13:03:54,544::train::INFO] [Train] Iter 15572 | Loss 0.336602 | Grad 0.0590 \n","[2024-04-10 13:03:54,663::train::INFO] [Train] Iter 15573 | Loss 0.298803 | Grad 0.0508 \n","[2024-04-10 13:03:54,782::train::INFO] [Train] Iter 15574 | Loss 0.317833 | Grad 0.0476 \n","[2024-04-10 13:03:54,901::train::INFO] [Train] Iter 15575 | Loss 0.335933 | Grad 0.0337 \n","[2024-04-10 13:03:55,020::train::INFO] [Train] Iter 15576 | Loss 0.311127 | Grad 0.0456 \n","[2024-04-10 13:03:55,140::train::INFO] [Train] Iter 15577 | Loss 0.349088 | Grad 0.0915 \n","[2024-04-10 13:03:55,260::train::INFO] [Train] Iter 15578 | Loss 0.298494 | Grad 0.0545 \n","[2024-04-10 13:03:55,379::train::INFO] [Train] Iter 15579 | Loss 0.320323 | Grad 0.0517 \n","[2024-04-10 13:03:55,498::train::INFO] [Train] Iter 15580 | Loss 0.297664 | Grad 0.0404 \n","[2024-04-10 13:03:55,618::train::INFO] [Train] Iter 15581 | Loss 0.275374 | Grad 0.0443 \n","[2024-04-10 13:03:55,739::train::INFO] [Train] Iter 15582 | Loss 0.291483 | Grad 0.0304 \n","[2024-04-10 13:03:55,866::train::INFO] [Train] Iter 15583 | Loss 0.347479 | Grad 0.0358 \n","[2024-04-10 13:03:55,987::train::INFO] [Train] Iter 15584 | Loss 0.320730 | Grad 0.0555 \n","[2024-04-10 13:03:56,107::train::INFO] [Train] Iter 15585 | Loss 0.291555 | Grad 0.0362 \n","[2024-04-10 13:03:56,232::train::INFO] [Train] Iter 15586 | Loss 0.320078 | Grad 0.0735 \n","[2024-04-10 13:03:56,353::train::INFO] [Train] Iter 15587 | Loss 0.351781 | Grad 0.1024 \n","[2024-04-10 13:03:56,476::train::INFO] [Train] Iter 15588 | Loss 0.331725 | Grad 0.0448 \n","[2024-04-10 13:03:56,603::train::INFO] [Train] Iter 15589 | Loss 0.316314 | Grad 0.0566 \n","[2024-04-10 13:03:56,724::train::INFO] [Train] Iter 15590 | Loss 0.306684 | Grad 0.0423 \n","[2024-04-10 13:03:56,849::train::INFO] [Train] Iter 15591 | Loss 0.355008 | Grad 0.0678 \n","[2024-04-10 13:03:56,970::train::INFO] [Train] Iter 15592 | Loss 0.316453 | Grad 0.0457 \n","[2024-04-10 13:03:57,091::train::INFO] [Train] Iter 15593 | Loss 0.352931 | Grad 0.0561 \n","[2024-04-10 13:03:57,217::train::INFO] [Train] Iter 15594 | Loss 0.339740 | Grad 0.0515 \n","[2024-04-10 13:03:57,338::train::INFO] [Train] Iter 15595 | Loss 0.286658 | Grad 0.0252 \n","[2024-04-10 13:03:57,463::train::INFO] [Train] Iter 15596 | Loss 0.259880 | Grad 0.0367 \n","[2024-04-10 13:03:57,583::train::INFO] [Train] Iter 15597 | Loss 0.288940 | Grad 0.0366 \n","[2024-04-10 13:03:57,704::train::INFO] [Train] Iter 15598 | Loss 0.315713 | Grad 0.0469 \n","[2024-04-10 13:03:57,825::train::INFO] [Train] Iter 15599 | Loss 0.303705 | Grad 0.0398 \n","[2024-04-10 13:03:57,945::train::INFO] [Train] Iter 15600 | Loss 0.322968 | Grad 0.0407 \n","[2024-04-10 13:03:58,065::train::INFO] [Train] Iter 15601 | Loss 0.339607 | Grad 0.0605 \n","[2024-04-10 13:03:58,185::train::INFO] [Train] Iter 15602 | Loss 0.290417 | Grad 0.0364 \n","[2024-04-10 13:03:58,307::train::INFO] [Train] Iter 15603 | Loss 0.281081 | Grad 0.0308 \n","[2024-04-10 13:03:58,427::train::INFO] [Train] Iter 15604 | Loss 0.303175 | Grad 0.0772 \n","[2024-04-10 13:03:58,549::train::INFO] [Train] Iter 15605 | Loss 0.335670 | Grad 0.0754 \n","[2024-04-10 13:03:58,671::train::INFO] [Train] Iter 15606 | Loss 0.335057 | Grad 0.0597 \n","[2024-04-10 13:03:58,793::train::INFO] [Train] Iter 15607 | Loss 0.320928 | Grad 0.0394 \n","[2024-04-10 13:03:58,915::train::INFO] [Train] Iter 15608 | Loss 0.327682 | Grad 0.0423 \n","[2024-04-10 13:03:59,035::train::INFO] [Train] Iter 15609 | Loss 0.305227 | Grad 0.0635 \n","[2024-04-10 13:03:59,155::train::INFO] [Train] Iter 15610 | Loss 0.304434 | Grad 0.0721 \n","[2024-04-10 13:03:59,275::train::INFO] [Train] Iter 15611 | Loss 0.344593 | Grad 0.0559 \n","[2024-04-10 13:03:59,395::train::INFO] [Train] Iter 15612 | Loss 0.300311 | Grad 0.0382 \n","[2024-04-10 13:03:59,515::train::INFO] [Train] Iter 15613 | Loss 0.335044 | Grad 0.0535 \n","[2024-04-10 13:03:59,635::train::INFO] [Train] Iter 15614 | Loss 0.321569 | Grad 0.0538 \n","[2024-04-10 13:03:59,709::train::INFO] [Train] Iter 15615 | Loss 0.322935 | Grad 0.0431 \n","[2024-04-10 13:03:59,828::train::INFO] [Train] Iter 15616 | Loss 0.303386 | Grad 0.0513 \n","[2024-04-10 13:03:59,947::train::INFO] [Train] Iter 15617 | Loss 0.366196 | Grad 0.0544 \n","[2024-04-10 13:04:00,066::train::INFO] [Train] Iter 15618 | Loss 0.306426 | Grad 0.0343 \n","[2024-04-10 13:04:00,185::train::INFO] [Train] Iter 15619 | Loss 0.302740 | Grad 0.0365 \n","[2024-04-10 13:04:00,303::train::INFO] [Train] Iter 15620 | Loss 0.306022 | Grad 0.0436 \n","[2024-04-10 13:04:00,423::train::INFO] [Train] Iter 15621 | Loss 0.360245 | Grad 0.0388 \n","[2024-04-10 13:04:00,543::train::INFO] [Train] Iter 15622 | Loss 0.291058 | Grad 0.0350 \n","[2024-04-10 13:04:00,665::train::INFO] [Train] Iter 15623 | Loss 0.331718 | Grad 0.0823 \n","[2024-04-10 13:04:00,786::train::INFO] [Train] Iter 15624 | Loss 0.307522 | Grad 0.0440 \n","[2024-04-10 13:04:00,905::train::INFO] [Train] Iter 15625 | Loss 0.332170 | Grad 0.0425 \n","[2024-04-10 13:04:01,024::train::INFO] [Train] Iter 15626 | Loss 0.346536 | Grad 0.0407 \n","[2024-04-10 13:04:01,144::train::INFO] [Train] Iter 15627 | Loss 0.346174 | Grad 0.0326 \n","[2024-04-10 13:04:01,265::train::INFO] [Train] Iter 15628 | Loss 0.336750 | Grad 0.0457 \n","[2024-04-10 13:04:01,385::train::INFO] [Train] Iter 15629 | Loss 0.280977 | Grad 0.0322 \n","[2024-04-10 13:04:01,505::train::INFO] [Train] Iter 15630 | Loss 0.367773 | Grad 0.0585 \n","[2024-04-10 13:04:01,626::train::INFO] [Train] Iter 15631 | Loss 0.328162 | Grad 0.0529 \n","[2024-04-10 13:04:01,745::train::INFO] [Train] Iter 15632 | Loss 0.357444 | Grad 0.0673 \n","[2024-04-10 13:04:01,864::train::INFO] [Train] Iter 15633 | Loss 0.323456 | Grad 0.0378 \n","[2024-04-10 13:04:01,983::train::INFO] [Train] Iter 15634 | Loss 0.315532 | Grad 0.0454 \n","[2024-04-10 13:04:02,102::train::INFO] [Train] Iter 15635 | Loss 0.322014 | Grad 0.0336 \n","[2024-04-10 13:04:02,220::train::INFO] [Train] Iter 15636 | Loss 0.306212 | Grad 0.0362 \n","[2024-04-10 13:04:02,339::train::INFO] [Train] Iter 15637 | Loss 0.309578 | Grad 0.0451 \n","[2024-04-10 13:04:02,459::train::INFO] [Train] Iter 15638 | Loss 0.320627 | Grad 0.0440 \n","[2024-04-10 13:04:02,580::train::INFO] [Train] Iter 15639 | Loss 0.350506 | Grad 0.0679 \n","[2024-04-10 13:04:02,700::train::INFO] [Train] Iter 15640 | Loss 0.286422 | Grad 0.0263 \n","[2024-04-10 13:04:02,819::train::INFO] [Train] Iter 15641 | Loss 0.310412 | Grad 0.0454 \n","[2024-04-10 13:04:02,941::train::INFO] [Train] Iter 15642 | Loss 0.298648 | Grad 0.0283 \n","[2024-04-10 13:04:03,060::train::INFO] [Train] Iter 15643 | Loss 0.303685 | Grad 0.0473 \n","[2024-04-10 13:04:03,179::train::INFO] [Train] Iter 15644 | Loss 0.316336 | Grad 0.0441 \n","[2024-04-10 13:04:03,300::train::INFO] [Train] Iter 15645 | Loss 0.302013 | Grad 0.0333 \n","[2024-04-10 13:04:03,421::train::INFO] [Train] Iter 15646 | Loss 0.319988 | Grad 0.0396 \n","[2024-04-10 13:04:03,541::train::INFO] [Train] Iter 15647 | Loss 0.310222 | Grad 0.0906 \n","[2024-04-10 13:04:03,661::train::INFO] [Train] Iter 15648 | Loss 0.316340 | Grad 0.0341 \n","[2024-04-10 13:04:03,788::train::INFO] [Train] Iter 15649 | Loss 0.301058 | Grad 0.0390 \n","[2024-04-10 13:04:03,907::train::INFO] [Train] Iter 15650 | Loss 0.302903 | Grad 0.0507 \n","[2024-04-10 13:04:04,026::train::INFO] [Train] Iter 15651 | Loss 0.331719 | Grad 0.0438 \n","[2024-04-10 13:04:04,145::train::INFO] [Train] Iter 15652 | Loss 0.319034 | Grad 0.0340 \n","[2024-04-10 13:04:04,263::train::INFO] [Train] Iter 15653 | Loss 0.336881 | Grad 0.0529 \n","[2024-04-10 13:04:04,386::train::INFO] [Train] Iter 15654 | Loss 0.290490 | Grad 0.0378 \n","[2024-04-10 13:04:04,506::train::INFO] [Train] Iter 15655 | Loss 0.336992 | Grad 0.0529 \n","[2024-04-10 13:04:04,625::train::INFO] [Train] Iter 15656 | Loss 0.316580 | Grad 0.0523 \n","[2024-04-10 13:04:04,744::train::INFO] [Train] Iter 15657 | Loss 0.295809 | Grad 0.0575 \n","[2024-04-10 13:04:04,864::train::INFO] [Train] Iter 15658 | Loss 0.317786 | Grad 0.0519 \n","[2024-04-10 13:04:04,987::train::INFO] [Train] Iter 15659 | Loss 0.349819 | Grad 0.0510 \n","[2024-04-10 13:04:05,062::train::INFO] [Train] Iter 15660 | Loss 0.381702 | Grad 0.0575 \n","[2024-04-10 13:04:05,181::train::INFO] [Train] Iter 15661 | Loss 0.271785 | Grad 0.0376 \n","[2024-04-10 13:04:05,302::train::INFO] [Train] Iter 15662 | Loss 0.293734 | Grad 0.0470 \n","[2024-04-10 13:04:05,422::train::INFO] [Train] Iter 15663 | Loss 0.361760 | Grad 0.0635 \n","[2024-04-10 13:04:05,542::train::INFO] [Train] Iter 15664 | Loss 0.372926 | Grad 0.0538 \n","[2024-04-10 13:04:05,663::train::INFO] [Train] Iter 15665 | Loss 0.320323 | Grad 0.0406 \n","[2024-04-10 13:04:05,784::train::INFO] [Train] Iter 15666 | Loss 0.319248 | Grad 0.0412 \n","[2024-04-10 13:04:05,905::train::INFO] [Train] Iter 15667 | Loss 0.308657 | Grad 0.0512 \n","[2024-04-10 13:04:06,027::train::INFO] [Train] Iter 15668 | Loss 0.312109 | Grad 0.0464 \n","[2024-04-10 13:04:06,149::train::INFO] [Train] Iter 15669 | Loss 0.334968 | Grad 0.0701 \n","[2024-04-10 13:04:06,269::train::INFO] [Train] Iter 15670 | Loss 0.309693 | Grad 0.0429 \n","[2024-04-10 13:04:06,390::train::INFO] [Train] Iter 15671 | Loss 0.326762 | Grad 0.0413 \n","[2024-04-10 13:04:06,511::train::INFO] [Train] Iter 15672 | Loss 0.280400 | Grad 0.0266 \n","[2024-04-10 13:04:06,632::train::INFO] [Train] Iter 15673 | Loss 0.299933 | Grad 0.0343 \n","[2024-04-10 13:04:06,752::train::INFO] [Train] Iter 15674 | Loss 0.326622 | Grad 0.0397 \n","[2024-04-10 13:04:06,874::train::INFO] [Train] Iter 15675 | Loss 0.320045 | Grad 0.0315 \n","[2024-04-10 13:04:06,994::train::INFO] [Train] Iter 15676 | Loss 0.320003 | Grad 0.0511 \n","[2024-04-10 13:04:07,115::train::INFO] [Train] Iter 15677 | Loss 0.349692 | Grad 0.1103 \n","[2024-04-10 13:04:07,237::train::INFO] [Train] Iter 15678 | Loss 0.280143 | Grad 0.0323 \n","[2024-04-10 13:04:07,357::train::INFO] [Train] Iter 15679 | Loss 0.268784 | Grad 0.0298 \n","[2024-04-10 13:04:07,476::train::INFO] [Train] Iter 15680 | Loss 0.329032 | Grad 0.0363 \n","[2024-04-10 13:04:07,594::train::INFO] [Train] Iter 15681 | Loss 0.297055 | Grad 0.0395 \n","[2024-04-10 13:04:07,713::train::INFO] [Train] Iter 15682 | Loss 0.330956 | Grad 0.0426 \n","[2024-04-10 13:04:07,831::train::INFO] [Train] Iter 15683 | Loss 0.298474 | Grad 0.0399 \n","[2024-04-10 13:04:07,950::train::INFO] [Train] Iter 15684 | Loss 0.297995 | Grad 0.0330 \n","[2024-04-10 13:04:08,069::train::INFO] [Train] Iter 15685 | Loss 0.327925 | Grad 0.0495 \n","[2024-04-10 13:04:08,189::train::INFO] [Train] Iter 15686 | Loss 0.334699 | Grad 0.0312 \n","[2024-04-10 13:04:08,309::train::INFO] [Train] Iter 15687 | Loss 0.321020 | Grad 0.0579 \n","[2024-04-10 13:04:08,433::train::INFO] [Train] Iter 15688 | Loss 0.336832 | Grad 0.0501 \n","[2024-04-10 13:04:08,553::train::INFO] [Train] Iter 15689 | Loss 0.292479 | Grad 0.0344 \n","[2024-04-10 13:04:08,672::train::INFO] [Train] Iter 15690 | Loss 0.340070 | Grad 0.0481 \n","[2024-04-10 13:04:08,790::train::INFO] [Train] Iter 15691 | Loss 0.294984 | Grad 0.0426 \n","[2024-04-10 13:04:08,909::train::INFO] [Train] Iter 15692 | Loss 0.334530 | Grad 0.0488 \n","[2024-04-10 13:04:09,037::train::INFO] [Train] Iter 15693 | Loss 0.306327 | Grad 0.0323 \n","[2024-04-10 13:04:09,159::train::INFO] [Train] Iter 15694 | Loss 0.296742 | Grad 0.0461 \n","[2024-04-10 13:04:09,279::train::INFO] [Train] Iter 15695 | Loss 0.319531 | Grad 0.0610 \n","[2024-04-10 13:04:09,399::train::INFO] [Train] Iter 15696 | Loss 0.293440 | Grad 0.0300 \n","[2024-04-10 13:04:09,519::train::INFO] [Train] Iter 15697 | Loss 0.334805 | Grad 0.0404 \n","[2024-04-10 13:04:09,640::train::INFO] [Train] Iter 15698 | Loss 0.307959 | Grad 0.0299 \n","[2024-04-10 13:04:09,763::train::INFO] [Train] Iter 15699 | Loss 0.317514 | Grad 0.0374 \n","[2024-04-10 13:04:09,882::train::INFO] [Train] Iter 15700 | Loss 0.347462 | Grad 0.0534 \n","[2024-04-10 13:04:10,002::train::INFO] [Train] Iter 15701 | Loss 0.312603 | Grad 0.0526 \n","[2024-04-10 13:04:10,122::train::INFO] [Train] Iter 15702 | Loss 0.313321 | Grad 0.0420 \n","[2024-04-10 13:04:10,248::train::INFO] [Train] Iter 15703 | Loss 0.284327 | Grad 0.0339 \n","[2024-04-10 13:04:10,372::train::INFO] [Train] Iter 15704 | Loss 0.327210 | Grad 0.0407 \n","[2024-04-10 13:04:10,448::train::INFO] [Train] Iter 15705 | Loss 0.304479 | Grad 0.0747 \n","[2024-04-10 13:04:10,570::train::INFO] [Train] Iter 15706 | Loss 0.344567 | Grad 0.0631 \n","[2024-04-10 13:04:10,692::train::INFO] [Train] Iter 15707 | Loss 0.310162 | Grad 0.0396 \n","[2024-04-10 13:04:10,818::train::INFO] [Train] Iter 15708 | Loss 0.330975 | Grad 0.0480 \n","[2024-04-10 13:04:10,940::train::INFO] [Train] Iter 15709 | Loss 0.338597 | Grad 0.0549 \n","[2024-04-10 13:04:11,061::train::INFO] [Train] Iter 15710 | Loss 0.328475 | Grad 0.0492 \n","[2024-04-10 13:04:11,187::train::INFO] [Train] Iter 15711 | Loss 0.340222 | Grad 0.0328 \n","[2024-04-10 13:04:11,308::train::INFO] [Train] Iter 15712 | Loss 0.306439 | Grad 0.0409 \n","[2024-04-10 13:04:11,430::train::INFO] [Train] Iter 15713 | Loss 0.320139 | Grad 0.0428 \n","[2024-04-10 13:04:11,566::train::INFO] [Train] Iter 15714 | Loss 0.366664 | Grad 0.0472 \n","[2024-04-10 13:04:11,688::train::INFO] [Train] Iter 15715 | Loss 0.349472 | Grad 0.0436 \n","[2024-04-10 13:04:11,809::train::INFO] [Train] Iter 15716 | Loss 0.333028 | Grad 0.0581 \n","[2024-04-10 13:04:11,932::train::INFO] [Train] Iter 15717 | Loss 0.325338 | Grad 0.0337 \n","[2024-04-10 13:04:12,055::train::INFO] [Train] Iter 15718 | Loss 0.313008 | Grad 0.0337 \n","[2024-04-10 13:04:12,182::train::INFO] [Train] Iter 15719 | Loss 0.344913 | Grad 0.0527 \n","[2024-04-10 13:04:12,300::train::INFO] [Train] Iter 15720 | Loss 0.338201 | Grad 0.0461 \n","[2024-04-10 13:04:12,418::train::INFO] [Train] Iter 15721 | Loss 0.318521 | Grad 0.0378 \n","[2024-04-10 13:04:12,538::train::INFO] [Train] Iter 15722 | Loss 0.322619 | Grad 0.0719 \n","[2024-04-10 13:04:12,657::train::INFO] [Train] Iter 15723 | Loss 0.322985 | Grad 0.0619 \n","[2024-04-10 13:04:12,778::train::INFO] [Train] Iter 15724 | Loss 0.322849 | Grad 0.0470 \n","[2024-04-10 13:04:12,896::train::INFO] [Train] Iter 15725 | Loss 0.249642 | Grad 0.0321 \n","[2024-04-10 13:04:13,017::train::INFO] [Train] Iter 15726 | Loss 0.326790 | Grad 0.0581 \n","[2024-04-10 13:04:13,136::train::INFO] [Train] Iter 15727 | Loss 0.365236 | Grad 0.0590 \n","[2024-04-10 13:04:13,258::train::INFO] [Train] Iter 15728 | Loss 0.343665 | Grad 0.0557 \n","[2024-04-10 13:04:13,379::train::INFO] [Train] Iter 15729 | Loss 0.336312 | Grad 0.0849 \n","[2024-04-10 13:04:13,499::train::INFO] [Train] Iter 15730 | Loss 0.324146 | Grad 0.0492 \n","[2024-04-10 13:04:13,619::train::INFO] [Train] Iter 15731 | Loss 0.344594 | Grad 0.0548 \n","[2024-04-10 13:04:13,739::train::INFO] [Train] Iter 15732 | Loss 0.335470 | Grad 0.0541 \n","[2024-04-10 13:04:13,859::train::INFO] [Train] Iter 15733 | Loss 0.312395 | Grad 0.0612 \n","[2024-04-10 13:04:13,979::train::INFO] [Train] Iter 15734 | Loss 0.337199 | Grad 0.0313 \n","[2024-04-10 13:04:14,099::train::INFO] [Train] Iter 15735 | Loss 0.322085 | Grad 0.0406 \n","[2024-04-10 13:04:14,219::train::INFO] [Train] Iter 15736 | Loss 0.303223 | Grad 0.0430 \n","[2024-04-10 13:04:14,339::train::INFO] [Train] Iter 15737 | Loss 0.309975 | Grad 0.0571 \n","[2024-04-10 13:04:14,460::train::INFO] [Train] Iter 15738 | Loss 0.293930 | Grad 0.0344 \n","[2024-04-10 13:04:14,581::train::INFO] [Train] Iter 15739 | Loss 0.301373 | Grad 0.0626 \n","[2024-04-10 13:04:14,701::train::INFO] [Train] Iter 15740 | Loss 0.340171 | Grad 0.0982 \n","[2024-04-10 13:04:14,819::train::INFO] [Train] Iter 15741 | Loss 0.319129 | Grad 0.0531 \n","[2024-04-10 13:04:14,937::train::INFO] [Train] Iter 15742 | Loss 0.292632 | Grad 0.0472 \n","[2024-04-10 13:04:15,056::train::INFO] [Train] Iter 15743 | Loss 0.327952 | Grad 0.0398 \n","[2024-04-10 13:04:15,175::train::INFO] [Train] Iter 15744 | Loss 0.308191 | Grad 0.0311 \n","[2024-04-10 13:04:15,294::train::INFO] [Train] Iter 15745 | Loss 0.340578 | Grad 0.0504 \n","[2024-04-10 13:04:15,414::train::INFO] [Train] Iter 15746 | Loss 0.327872 | Grad 0.0651 \n","[2024-04-10 13:04:15,534::train::INFO] [Train] Iter 15747 | Loss 0.284608 | Grad 0.0317 \n","[2024-04-10 13:04:15,653::train::INFO] [Train] Iter 15748 | Loss 0.314952 | Grad 0.0311 \n","[2024-04-10 13:04:15,773::train::INFO] [Train] Iter 15749 | Loss 0.312792 | Grad 0.0442 \n","[2024-04-10 13:04:15,851::train::INFO] [Train] Iter 15750 | Loss 0.251319 | Grad 0.0311 \n","[2024-04-10 13:04:15,970::train::INFO] [Train] Iter 15751 | Loss 0.340114 | Grad 0.0535 \n","[2024-04-10 13:04:16,088::train::INFO] [Train] Iter 15752 | Loss 0.304453 | Grad 0.0438 \n","[2024-04-10 13:04:16,206::train::INFO] [Train] Iter 15753 | Loss 0.329152 | Grad 0.0550 \n","[2024-04-10 13:04:16,325::train::INFO] [Train] Iter 15754 | Loss 0.253491 | Grad 0.0379 \n","[2024-04-10 13:04:16,443::train::INFO] [Train] Iter 15755 | Loss 0.336265 | Grad 0.0404 \n","[2024-04-10 13:04:16,563::train::INFO] [Train] Iter 15756 | Loss 0.290594 | Grad 0.0255 \n","[2024-04-10 13:04:16,682::train::INFO] [Train] Iter 15757 | Loss 0.300787 | Grad 0.0424 \n","[2024-04-10 13:04:16,802::train::INFO] [Train] Iter 15758 | Loss 0.309834 | Grad 0.0455 \n","[2024-04-10 13:04:16,921::train::INFO] [Train] Iter 15759 | Loss 0.278243 | Grad 0.0475 \n","[2024-04-10 13:04:17,041::train::INFO] [Train] Iter 15760 | Loss 0.321577 | Grad 0.0495 \n","[2024-04-10 13:04:17,162::train::INFO] [Train] Iter 15761 | Loss 0.326495 | Grad 0.0887 \n","[2024-04-10 13:04:17,282::train::INFO] [Train] Iter 15762 | Loss 0.273153 | Grad 0.0285 \n","[2024-04-10 13:04:17,406::train::INFO] [Train] Iter 15763 | Loss 0.292420 | Grad 0.0332 \n","[2024-04-10 13:04:17,525::train::INFO] [Train] Iter 15764 | Loss 0.312542 | Grad 0.0317 \n","[2024-04-10 13:04:17,644::train::INFO] [Train] Iter 15765 | Loss 0.334917 | Grad 0.0395 \n","[2024-04-10 13:04:17,764::train::INFO] [Train] Iter 15766 | Loss 0.348256 | Grad 0.0651 \n","[2024-04-10 13:04:17,884::train::INFO] [Train] Iter 15767 | Loss 0.316650 | Grad 0.0633 \n","[2024-04-10 13:04:18,006::train::INFO] [Train] Iter 15768 | Loss 0.312949 | Grad 0.0426 \n","[2024-04-10 13:04:18,125::train::INFO] [Train] Iter 15769 | Loss 0.289850 | Grad 0.0290 \n","[2024-04-10 13:04:18,245::train::INFO] [Train] Iter 15770 | Loss 0.283398 | Grad 0.0323 \n","[2024-04-10 13:04:18,367::train::INFO] [Train] Iter 15771 | Loss 0.311802 | Grad 0.0322 \n","[2024-04-10 13:04:18,487::train::INFO] [Train] Iter 15772 | Loss 0.333664 | Grad 0.0338 \n","[2024-04-10 13:04:18,610::train::INFO] [Train] Iter 15773 | Loss 0.293394 | Grad 0.0499 \n","[2024-04-10 13:04:18,729::train::INFO] [Train] Iter 15774 | Loss 0.382646 | Grad 0.0433 \n","[2024-04-10 13:04:18,848::train::INFO] [Train] Iter 15775 | Loss 0.306254 | Grad 0.0347 \n","[2024-04-10 13:04:18,969::train::INFO] [Train] Iter 15776 | Loss 0.305500 | Grad 0.0330 \n","[2024-04-10 13:04:19,090::train::INFO] [Train] Iter 15777 | Loss 0.313027 | Grad 0.0275 \n","[2024-04-10 13:04:19,212::train::INFO] [Train] Iter 15778 | Loss 0.333955 | Grad 0.0563 \n","[2024-04-10 13:04:19,331::train::INFO] [Train] Iter 15779 | Loss 0.304770 | Grad 0.0407 \n","[2024-04-10 13:04:19,451::train::INFO] [Train] Iter 15780 | Loss 0.366774 | Grad 0.0496 \n","[2024-04-10 13:04:19,570::train::INFO] [Train] Iter 15781 | Loss 0.302030 | Grad 0.0273 \n","[2024-04-10 13:04:19,688::train::INFO] [Train] Iter 15782 | Loss 0.348564 | Grad 0.0506 \n","[2024-04-10 13:04:19,808::train::INFO] [Train] Iter 15783 | Loss 0.290216 | Grad 0.0415 \n","[2024-04-10 13:04:19,927::train::INFO] [Train] Iter 15784 | Loss 0.318155 | Grad 0.0642 \n","[2024-04-10 13:04:20,045::train::INFO] [Train] Iter 15785 | Loss 0.309557 | Grad 0.0808 \n","[2024-04-10 13:04:20,163::train::INFO] [Train] Iter 15786 | Loss 0.320274 | Grad 0.0455 \n","[2024-04-10 13:04:20,282::train::INFO] [Train] Iter 15787 | Loss 0.332205 | Grad 0.0462 \n","[2024-04-10 13:04:20,408::train::INFO] [Train] Iter 15788 | Loss 0.299026 | Grad 0.0440 \n","[2024-04-10 13:04:20,526::train::INFO] [Train] Iter 15789 | Loss 0.306714 | Grad 0.0395 \n","[2024-04-10 13:04:20,644::train::INFO] [Train] Iter 15790 | Loss 0.317518 | Grad 0.0513 \n","[2024-04-10 13:04:20,764::train::INFO] [Train] Iter 15791 | Loss 0.328003 | Grad 0.0456 \n","[2024-04-10 13:04:20,883::train::INFO] [Train] Iter 15792 | Loss 0.382698 | Grad 0.1068 \n","[2024-04-10 13:04:21,002::train::INFO] [Train] Iter 15793 | Loss 0.287577 | Grad 0.0430 \n","[2024-04-10 13:04:21,124::train::INFO] [Train] Iter 15794 | Loss 0.346459 | Grad 0.0434 \n","[2024-04-10 13:04:21,199::train::INFO] [Train] Iter 15795 | Loss 0.298721 | Grad 0.0359 \n","[2024-04-10 13:04:21,317::train::INFO] [Train] Iter 15796 | Loss 0.331666 | Grad 0.0677 \n","[2024-04-10 13:04:21,439::train::INFO] [Train] Iter 15797 | Loss 0.320027 | Grad 0.0577 \n","[2024-04-10 13:04:21,558::train::INFO] [Train] Iter 15798 | Loss 0.328377 | Grad 0.0522 \n","[2024-04-10 13:04:21,677::train::INFO] [Train] Iter 15799 | Loss 0.301271 | Grad 0.0393 \n","[2024-04-10 13:04:21,796::train::INFO] [Train] Iter 15800 | Loss 0.323200 | Grad 0.0543 \n","[2024-04-10 13:04:21,915::train::INFO] [Train] Iter 15801 | Loss 0.262552 | Grad 0.0218 \n","[2024-04-10 13:04:22,033::train::INFO] [Train] Iter 15802 | Loss 0.296260 | Grad 0.0519 \n","[2024-04-10 13:04:22,154::train::INFO] [Train] Iter 15803 | Loss 0.294206 | Grad 0.0521 \n","[2024-04-10 13:04:22,275::train::INFO] [Train] Iter 15804 | Loss 0.347181 | Grad 0.0634 \n","[2024-04-10 13:04:22,397::train::INFO] [Train] Iter 15805 | Loss 0.349451 | Grad 0.0525 \n","[2024-04-10 13:04:22,523::train::INFO] [Train] Iter 15806 | Loss 0.322115 | Grad 0.0306 \n","[2024-04-10 13:04:22,649::train::INFO] [Train] Iter 15807 | Loss 0.327829 | Grad 0.0385 \n","[2024-04-10 13:04:22,770::train::INFO] [Train] Iter 15808 | Loss 0.321010 | Grad 0.0334 \n","[2024-04-10 13:04:22,890::train::INFO] [Train] Iter 15809 | Loss 0.325651 | Grad 0.0335 \n","[2024-04-10 13:04:23,010::train::INFO] [Train] Iter 15810 | Loss 0.324575 | Grad 0.0377 \n","[2024-04-10 13:04:23,131::train::INFO] [Train] Iter 15811 | Loss 0.347571 | Grad 0.0440 \n","[2024-04-10 13:04:23,252::train::INFO] [Train] Iter 15812 | Loss 0.302157 | Grad 0.0430 \n","[2024-04-10 13:04:23,375::train::INFO] [Train] Iter 15813 | Loss 0.336199 | Grad 0.0563 \n","[2024-04-10 13:04:23,499::train::INFO] [Train] Iter 15814 | Loss 0.349866 | Grad 0.0405 \n","[2024-04-10 13:04:23,623::train::INFO] [Train] Iter 15815 | Loss 0.289257 | Grad 0.0436 \n","[2024-04-10 13:04:23,742::train::INFO] [Train] Iter 15816 | Loss 0.353276 | Grad 0.0737 \n","[2024-04-10 13:04:23,863::train::INFO] [Train] Iter 15817 | Loss 0.280455 | Grad 0.0350 \n","[2024-04-10 13:04:23,983::train::INFO] [Train] Iter 15818 | Loss 0.266013 | Grad 0.0376 \n","[2024-04-10 13:04:24,106::train::INFO] [Train] Iter 15819 | Loss 0.347530 | Grad 0.0473 \n","[2024-04-10 13:04:24,227::train::INFO] [Train] Iter 15820 | Loss 0.340509 | Grad 0.0506 \n","[2024-04-10 13:04:24,346::train::INFO] [Train] Iter 15821 | Loss 0.302869 | Grad 0.0432 \n","[2024-04-10 13:04:24,466::train::INFO] [Train] Iter 15822 | Loss 0.335259 | Grad 0.0560 \n","[2024-04-10 13:04:24,586::train::INFO] [Train] Iter 15823 | Loss 0.327107 | Grad 0.0640 \n","[2024-04-10 13:04:24,713::train::INFO] [Train] Iter 15824 | Loss 0.349970 | Grad 0.0491 \n","[2024-04-10 13:04:24,836::train::INFO] [Train] Iter 15825 | Loss 0.287253 | Grad 0.0406 \n","[2024-04-10 13:04:24,956::train::INFO] [Train] Iter 15826 | Loss 0.322740 | Grad 0.0345 \n","[2024-04-10 13:04:25,076::train::INFO] [Train] Iter 15827 | Loss 0.303011 | Grad 0.0520 \n","[2024-04-10 13:04:25,198::train::INFO] [Train] Iter 15828 | Loss 0.314479 | Grad 0.0361 \n","[2024-04-10 13:04:25,322::train::INFO] [Train] Iter 15829 | Loss 0.299535 | Grad 0.0757 \n","[2024-04-10 13:04:25,446::train::INFO] [Train] Iter 15830 | Loss 0.319298 | Grad 0.0872 \n","[2024-04-10 13:04:25,565::train::INFO] [Train] Iter 15831 | Loss 0.345178 | Grad 0.0431 \n","[2024-04-10 13:04:25,686::train::INFO] [Train] Iter 15832 | Loss 0.287766 | Grad 0.0257 \n","[2024-04-10 13:04:25,807::train::INFO] [Train] Iter 15833 | Loss 0.300260 | Grad 0.0330 \n","[2024-04-10 13:04:25,926::train::INFO] [Train] Iter 15834 | Loss 0.270337 | Grad 0.0237 \n","[2024-04-10 13:04:26,048::train::INFO] [Train] Iter 15835 | Loss 0.299463 | Grad 0.0419 \n","[2024-04-10 13:04:26,167::train::INFO] [Train] Iter 15836 | Loss 0.297163 | Grad 0.0427 \n","[2024-04-10 13:04:26,286::train::INFO] [Train] Iter 15837 | Loss 0.321093 | Grad 0.0468 \n","[2024-04-10 13:04:26,406::train::INFO] [Train] Iter 15838 | Loss 0.297476 | Grad 0.0628 \n","[2024-04-10 13:04:26,525::train::INFO] [Train] Iter 15839 | Loss 0.352188 | Grad 0.0778 \n","[2024-04-10 13:04:26,602::train::INFO] [Train] Iter 15840 | Loss 0.312902 | Grad 0.0633 \n","[2024-04-10 13:04:26,721::train::INFO] [Train] Iter 15841 | Loss 0.348080 | Grad 0.0702 \n","[2024-04-10 13:04:26,841::train::INFO] [Train] Iter 15842 | Loss 0.289866 | Grad 0.0476 \n","[2024-04-10 13:04:26,963::train::INFO] [Train] Iter 15843 | Loss 0.320110 | Grad 0.0720 \n","[2024-04-10 13:04:27,081::train::INFO] [Train] Iter 15844 | Loss 0.321458 | Grad 0.0648 \n","[2024-04-10 13:04:27,199::train::INFO] [Train] Iter 15845 | Loss 0.308579 | Grad 0.0717 \n","[2024-04-10 13:04:27,318::train::INFO] [Train] Iter 15846 | Loss 0.318844 | Grad 0.0611 \n","[2024-04-10 13:04:27,437::train::INFO] [Train] Iter 15847 | Loss 0.301913 | Grad 0.0812 \n","[2024-04-10 13:04:27,557::train::INFO] [Train] Iter 15848 | Loss 0.327640 | Grad 0.0694 \n","[2024-04-10 13:04:27,676::train::INFO] [Train] Iter 15849 | Loss 0.284620 | Grad 0.0554 \n","[2024-04-10 13:04:27,796::train::INFO] [Train] Iter 15850 | Loss 0.295659 | Grad 0.0525 \n","[2024-04-10 13:04:27,914::train::INFO] [Train] Iter 15851 | Loss 0.331318 | Grad 0.0726 \n","[2024-04-10 13:04:28,032::train::INFO] [Train] Iter 15852 | Loss 0.332105 | Grad 0.0662 \n","[2024-04-10 13:04:28,154::train::INFO] [Train] Iter 15853 | Loss 0.304936 | Grad 0.0299 \n","[2024-04-10 13:04:28,272::train::INFO] [Train] Iter 15854 | Loss 0.326711 | Grad 0.0395 \n","[2024-04-10 13:04:28,390::train::INFO] [Train] Iter 15855 | Loss 0.270208 | Grad 0.0309 \n","[2024-04-10 13:04:28,509::train::INFO] [Train] Iter 15856 | Loss 0.382640 | Grad 0.0582 \n","[2024-04-10 13:04:28,629::train::INFO] [Train] Iter 15857 | Loss 0.319129 | Grad 0.0823 \n","[2024-04-10 13:04:28,749::train::INFO] [Train] Iter 15858 | Loss 0.288396 | Grad 0.0517 \n","[2024-04-10 13:04:28,869::train::INFO] [Train] Iter 15859 | Loss 0.290601 | Grad 0.0361 \n","[2024-04-10 13:04:28,990::train::INFO] [Train] Iter 15860 | Loss 0.349547 | Grad 0.0511 \n","[2024-04-10 13:04:29,110::train::INFO] [Train] Iter 15861 | Loss 0.338725 | Grad 0.0347 \n","[2024-04-10 13:04:29,231::train::INFO] [Train] Iter 15862 | Loss 0.320070 | Grad 0.0511 \n","[2024-04-10 13:04:29,352::train::INFO] [Train] Iter 15863 | Loss 0.351422 | Grad 0.0591 \n","[2024-04-10 13:04:29,474::train::INFO] [Train] Iter 15864 | Loss 0.328734 | Grad 0.0746 \n","[2024-04-10 13:04:29,593::train::INFO] [Train] Iter 15865 | Loss 0.302877 | Grad 0.0353 \n","[2024-04-10 13:04:29,713::train::INFO] [Train] Iter 15866 | Loss 0.337738 | Grad 0.0320 \n","[2024-04-10 13:04:29,833::train::INFO] [Train] Iter 15867 | Loss 0.275493 | Grad 0.0440 \n","[2024-04-10 13:04:29,953::train::INFO] [Train] Iter 15868 | Loss 0.314402 | Grad 0.0699 \n","[2024-04-10 13:04:30,073::train::INFO] [Train] Iter 15869 | Loss 0.300505 | Grad 0.0511 \n","[2024-04-10 13:04:30,193::train::INFO] [Train] Iter 15870 | Loss 0.308110 | Grad 0.0484 \n","[2024-04-10 13:04:30,313::train::INFO] [Train] Iter 15871 | Loss 0.330480 | Grad 0.0536 \n","[2024-04-10 13:04:30,433::train::INFO] [Train] Iter 15872 | Loss 0.315676 | Grad 0.0535 \n","[2024-04-10 13:04:30,554::train::INFO] [Train] Iter 15873 | Loss 0.353567 | Grad 0.0590 \n","[2024-04-10 13:04:30,675::train::INFO] [Train] Iter 15874 | Loss 0.317456 | Grad 0.0723 \n","[2024-04-10 13:04:30,797::train::INFO] [Train] Iter 15875 | Loss 0.325595 | Grad 0.0601 \n","[2024-04-10 13:04:30,915::train::INFO] [Train] Iter 15876 | Loss 0.271911 | Grad 0.0353 \n","[2024-04-10 13:04:31,034::train::INFO] [Train] Iter 15877 | Loss 0.292210 | Grad 0.0576 \n","[2024-04-10 13:04:31,154::train::INFO] [Train] Iter 15878 | Loss 0.316177 | Grad 0.0636 \n","[2024-04-10 13:04:31,274::train::INFO] [Train] Iter 15879 | Loss 0.317015 | Grad 0.0287 \n","[2024-04-10 13:04:31,394::train::INFO] [Train] Iter 15880 | Loss 0.273766 | Grad 0.0321 \n","[2024-04-10 13:04:31,515::train::INFO] [Train] Iter 15881 | Loss 0.317565 | Grad 0.0635 \n","[2024-04-10 13:04:31,634::train::INFO] [Train] Iter 15882 | Loss 0.272192 | Grad 0.0577 \n","[2024-04-10 13:04:31,754::train::INFO] [Train] Iter 15883 | Loss 0.366388 | Grad 0.0740 \n","[2024-04-10 13:04:31,872::train::INFO] [Train] Iter 15884 | Loss 0.323966 | Grad 0.0579 \n","[2024-04-10 13:04:31,946::train::INFO] [Train] Iter 15885 | Loss 0.362803 | Grad 0.0498 \n","[2024-04-10 13:04:32,065::train::INFO] [Train] Iter 15886 | Loss 0.306036 | Grad 0.0494 \n","[2024-04-10 13:04:32,183::train::INFO] [Train] Iter 15887 | Loss 0.332211 | Grad 0.0561 \n","[2024-04-10 13:04:32,302::train::INFO] [Train] Iter 15888 | Loss 0.280409 | Grad 0.0391 \n","[2024-04-10 13:04:32,421::train::INFO] [Train] Iter 15889 | Loss 0.303106 | Grad 0.0620 \n","[2024-04-10 13:04:32,541::train::INFO] [Train] Iter 15890 | Loss 0.297817 | Grad 0.0579 \n","[2024-04-10 13:04:32,662::train::INFO] [Train] Iter 15891 | Loss 0.334797 | Grad 0.0642 \n","[2024-04-10 13:04:32,784::train::INFO] [Train] Iter 15892 | Loss 0.332771 | Grad 0.0670 \n","[2024-04-10 13:04:32,910::train::INFO] [Train] Iter 15893 | Loss 0.327046 | Grad 0.0339 \n","[2024-04-10 13:04:33,030::train::INFO] [Train] Iter 15894 | Loss 0.308083 | Grad 0.0370 \n","[2024-04-10 13:04:33,149::train::INFO] [Train] Iter 15895 | Loss 0.343863 | Grad 0.0773 \n","[2024-04-10 13:04:33,269::train::INFO] [Train] Iter 15896 | Loss 0.342914 | Grad 0.0657 \n","[2024-04-10 13:04:33,389::train::INFO] [Train] Iter 15897 | Loss 0.336195 | Grad 0.0443 \n","[2024-04-10 13:04:33,516::train::INFO] [Train] Iter 15898 | Loss 0.305385 | Grad 0.0493 \n","[2024-04-10 13:04:33,635::train::INFO] [Train] Iter 15899 | Loss 0.286049 | Grad 0.0398 \n","[2024-04-10 13:04:33,755::train::INFO] [Train] Iter 15900 | Loss 0.339463 | Grad 0.0342 \n","[2024-04-10 13:04:33,875::train::INFO] [Train] Iter 15901 | Loss 0.315772 | Grad 0.0518 \n","[2024-04-10 13:04:33,996::train::INFO] [Train] Iter 15902 | Loss 0.339691 | Grad 0.0727 \n","[2024-04-10 13:04:34,116::train::INFO] [Train] Iter 15903 | Loss 0.312923 | Grad 0.0623 \n","[2024-04-10 13:04:34,236::train::INFO] [Train] Iter 15904 | Loss 0.325573 | Grad 0.0293 \n","[2024-04-10 13:04:34,356::train::INFO] [Train] Iter 15905 | Loss 0.310541 | Grad 0.0430 \n","[2024-04-10 13:04:34,476::train::INFO] [Train] Iter 15906 | Loss 0.329728 | Grad 0.0595 \n","[2024-04-10 13:04:34,596::train::INFO] [Train] Iter 15907 | Loss 0.322406 | Grad 0.0336 \n","[2024-04-10 13:04:34,719::train::INFO] [Train] Iter 15908 | Loss 0.295078 | Grad 0.0329 \n","[2024-04-10 13:04:34,842::train::INFO] [Train] Iter 15909 | Loss 0.307688 | Grad 0.0541 \n","[2024-04-10 13:04:34,962::train::INFO] [Train] Iter 15910 | Loss 0.326252 | Grad 0.0529 \n","[2024-04-10 13:04:35,082::train::INFO] [Train] Iter 15911 | Loss 0.332831 | Grad 0.0372 \n","[2024-04-10 13:04:35,203::train::INFO] [Train] Iter 15912 | Loss 0.351618 | Grad 0.0386 \n","[2024-04-10 13:04:35,324::train::INFO] [Train] Iter 15913 | Loss 0.293288 | Grad 0.0206 \n","[2024-04-10 13:04:35,444::train::INFO] [Train] Iter 15914 | Loss 0.323903 | Grad 0.0651 \n","[2024-04-10 13:04:35,566::train::INFO] [Train] Iter 15915 | Loss 0.289164 | Grad 0.0311 \n","[2024-04-10 13:04:35,688::train::INFO] [Train] Iter 15916 | Loss 0.306800 | Grad 0.0466 \n","[2024-04-10 13:04:35,818::train::INFO] [Train] Iter 15917 | Loss 0.342824 | Grad 0.0458 \n","[2024-04-10 13:04:35,938::train::INFO] [Train] Iter 15918 | Loss 0.322474 | Grad 0.0336 \n","[2024-04-10 13:04:36,059::train::INFO] [Train] Iter 15919 | Loss 0.305223 | Grad 0.0531 \n","[2024-04-10 13:04:36,179::train::INFO] [Train] Iter 15920 | Loss 0.347055 | Grad 0.0781 \n","[2024-04-10 13:04:36,301::train::INFO] [Train] Iter 15921 | Loss 0.288393 | Grad 0.0550 \n","[2024-04-10 13:04:36,424::train::INFO] [Train] Iter 15922 | Loss 0.337512 | Grad 0.0503 \n","[2024-04-10 13:04:36,552::train::INFO] [Train] Iter 15923 | Loss 0.327059 | Grad 0.0714 \n","[2024-04-10 13:04:36,676::train::INFO] [Train] Iter 15924 | Loss 0.267950 | Grad 0.0366 \n","[2024-04-10 13:04:36,800::train::INFO] [Train] Iter 15925 | Loss 0.284578 | Grad 0.0406 \n","[2024-04-10 13:04:36,927::train::INFO] [Train] Iter 15926 | Loss 0.270468 | Grad 0.0325 \n","[2024-04-10 13:04:37,049::train::INFO] [Train] Iter 15927 | Loss 0.329290 | Grad 0.0539 \n","[2024-04-10 13:04:37,169::train::INFO] [Train] Iter 15928 | Loss 0.313000 | Grad 0.0571 \n","[2024-04-10 13:04:37,290::train::INFO] [Train] Iter 15929 | Loss 0.316429 | Grad 0.0471 \n","[2024-04-10 13:04:37,368::train::INFO] [Train] Iter 15930 | Loss 0.342842 | Grad 0.0559 \n","[2024-04-10 13:04:37,488::train::INFO] [Train] Iter 15931 | Loss 0.296529 | Grad 0.0327 \n","[2024-04-10 13:04:37,610::train::INFO] [Train] Iter 15932 | Loss 0.343465 | Grad 0.0432 \n","[2024-04-10 13:04:37,731::train::INFO] [Train] Iter 15933 | Loss 0.312622 | Grad 0.0400 \n","[2024-04-10 13:04:37,854::train::INFO] [Train] Iter 15934 | Loss 0.283025 | Grad 0.0494 \n","[2024-04-10 13:04:37,974::train::INFO] [Train] Iter 15935 | Loss 0.355992 | Grad 0.0607 \n","[2024-04-10 13:04:38,095::train::INFO] [Train] Iter 15936 | Loss 0.309725 | Grad 0.0503 \n","[2024-04-10 13:04:38,215::train::INFO] [Train] Iter 15937 | Loss 0.307545 | Grad 0.0474 \n","[2024-04-10 13:04:38,335::train::INFO] [Train] Iter 15938 | Loss 0.292109 | Grad 0.0290 \n","[2024-04-10 13:04:38,457::train::INFO] [Train] Iter 15939 | Loss 0.305300 | Grad 0.0494 \n","[2024-04-10 13:04:38,578::train::INFO] [Train] Iter 15940 | Loss 0.307836 | Grad 0.0449 \n","[2024-04-10 13:04:38,701::train::INFO] [Train] Iter 15941 | Loss 0.307950 | Grad 0.0422 \n","[2024-04-10 13:04:38,820::train::INFO] [Train] Iter 15942 | Loss 0.341624 | Grad 0.0559 \n","[2024-04-10 13:04:38,939::train::INFO] [Train] Iter 15943 | Loss 0.336172 | Grad 0.0629 \n","[2024-04-10 13:04:39,058::train::INFO] [Train] Iter 15944 | Loss 0.296515 | Grad 0.0398 \n","[2024-04-10 13:04:39,176::train::INFO] [Train] Iter 15945 | Loss 0.323349 | Grad 0.0454 \n","[2024-04-10 13:04:39,295::train::INFO] [Train] Iter 15946 | Loss 0.307399 | Grad 0.0295 \n","[2024-04-10 13:04:39,413::train::INFO] [Train] Iter 15947 | Loss 0.321210 | Grad 0.0489 \n","[2024-04-10 13:04:39,532::train::INFO] [Train] Iter 15948 | Loss 0.334180 | Grad 0.0673 \n","[2024-04-10 13:04:39,651::train::INFO] [Train] Iter 15949 | Loss 0.325354 | Grad 0.0599 \n","[2024-04-10 13:04:39,770::train::INFO] [Train] Iter 15950 | Loss 0.297386 | Grad 0.0366 \n","[2024-04-10 13:04:39,889::train::INFO] [Train] Iter 15951 | Loss 0.297429 | Grad 0.0333 \n","[2024-04-10 13:04:40,008::train::INFO] [Train] Iter 15952 | Loss 0.298804 | Grad 0.0445 \n","[2024-04-10 13:04:40,130::train::INFO] [Train] Iter 15953 | Loss 0.339249 | Grad 0.0476 \n","[2024-04-10 13:04:40,251::train::INFO] [Train] Iter 15954 | Loss 0.303832 | Grad 0.0317 \n","[2024-04-10 13:04:40,372::train::INFO] [Train] Iter 15955 | Loss 0.308890 | Grad 0.0483 \n","[2024-04-10 13:04:40,491::train::INFO] [Train] Iter 15956 | Loss 0.347733 | Grad 0.0571 \n","[2024-04-10 13:04:40,612::train::INFO] [Train] Iter 15957 | Loss 0.302391 | Grad 0.0555 \n","[2024-04-10 13:04:40,734::train::INFO] [Train] Iter 15958 | Loss 0.293431 | Grad 0.0268 \n","[2024-04-10 13:04:40,854::train::INFO] [Train] Iter 15959 | Loss 0.294936 | Grad 0.0415 \n","[2024-04-10 13:04:40,973::train::INFO] [Train] Iter 15960 | Loss 0.314564 | Grad 0.0452 \n","[2024-04-10 13:04:41,092::train::INFO] [Train] Iter 15961 | Loss 0.282312 | Grad 0.0419 \n","[2024-04-10 13:04:41,211::train::INFO] [Train] Iter 15962 | Loss 0.366487 | Grad 0.0735 \n","[2024-04-10 13:04:41,331::train::INFO] [Train] Iter 15963 | Loss 0.332857 | Grad 0.0419 \n","[2024-04-10 13:04:41,449::train::INFO] [Train] Iter 15964 | Loss 0.336751 | Grad 0.0495 \n","[2024-04-10 13:04:41,569::train::INFO] [Train] Iter 15965 | Loss 0.300655 | Grad 0.0538 \n","[2024-04-10 13:04:41,687::train::INFO] [Train] Iter 15966 | Loss 0.301065 | Grad 0.0375 \n","[2024-04-10 13:04:41,806::train::INFO] [Train] Iter 15967 | Loss 0.290505 | Grad 0.0512 \n","[2024-04-10 13:04:41,925::train::INFO] [Train] Iter 15968 | Loss 0.305149 | Grad 0.0766 \n","[2024-04-10 13:04:42,045::train::INFO] [Train] Iter 15969 | Loss 0.336243 | Grad 0.0541 \n","[2024-04-10 13:04:42,165::train::INFO] [Train] Iter 15970 | Loss 0.336377 | Grad 0.0382 \n","[2024-04-10 13:04:42,285::train::INFO] [Train] Iter 15971 | Loss 0.325632 | Grad 0.0398 \n","[2024-04-10 13:04:42,406::train::INFO] [Train] Iter 15972 | Loss 0.316549 | Grad 0.0411 \n","[2024-04-10 13:04:42,526::train::INFO] [Train] Iter 15973 | Loss 0.293800 | Grad 0.0382 \n","[2024-04-10 13:04:42,646::train::INFO] [Train] Iter 15974 | Loss 0.305502 | Grad 0.0534 \n","[2024-04-10 13:04:42,721::train::INFO] [Train] Iter 15975 | Loss 0.306029 | Grad 0.0489 \n","[2024-04-10 13:04:42,840::train::INFO] [Train] Iter 15976 | Loss 0.270441 | Grad 0.0485 \n","[2024-04-10 13:04:42,959::train::INFO] [Train] Iter 15977 | Loss 0.313423 | Grad 0.0573 \n","[2024-04-10 13:04:43,078::train::INFO] [Train] Iter 15978 | Loss 0.333357 | Grad 0.0589 \n","[2024-04-10 13:04:43,196::train::INFO] [Train] Iter 15979 | Loss 0.299652 | Grad 0.0515 \n","[2024-04-10 13:04:43,315::train::INFO] [Train] Iter 15980 | Loss 0.343921 | Grad 0.0559 \n","[2024-04-10 13:04:43,434::train::INFO] [Train] Iter 15981 | Loss 0.387444 | Grad 0.1097 \n","[2024-04-10 13:04:43,552::train::INFO] [Train] Iter 15982 | Loss 0.327859 | Grad 0.0499 \n","[2024-04-10 13:04:43,671::train::INFO] [Train] Iter 15983 | Loss 0.325943 | Grad 0.0362 \n","[2024-04-10 13:04:43,790::train::INFO] [Train] Iter 15984 | Loss 0.343329 | Grad 0.0500 \n","[2024-04-10 13:04:43,911::train::INFO] [Train] Iter 15985 | Loss 0.282314 | Grad 0.0313 \n","[2024-04-10 13:04:44,031::train::INFO] [Train] Iter 15986 | Loss 0.330929 | Grad 0.0352 \n","[2024-04-10 13:04:44,153::train::INFO] [Train] Iter 15987 | Loss 0.327384 | Grad 0.0486 \n","[2024-04-10 13:04:44,273::train::INFO] [Train] Iter 15988 | Loss 0.315196 | Grad 0.0414 \n","[2024-04-10 13:04:44,392::train::INFO] [Train] Iter 15989 | Loss 0.316948 | Grad 0.0394 \n","[2024-04-10 13:04:44,512::train::INFO] [Train] Iter 15990 | Loss 0.331968 | Grad 0.0388 \n","[2024-04-10 13:04:44,631::train::INFO] [Train] Iter 15991 | Loss 0.311780 | Grad 0.0467 \n","[2024-04-10 13:04:44,749::train::INFO] [Train] Iter 15992 | Loss 0.305422 | Grad 0.0364 \n","[2024-04-10 13:04:44,868::train::INFO] [Train] Iter 15993 | Loss 0.338193 | Grad 0.0507 \n","[2024-04-10 13:04:44,986::train::INFO] [Train] Iter 15994 | Loss 0.321840 | Grad 0.0477 \n","[2024-04-10 13:04:45,105::train::INFO] [Train] Iter 15995 | Loss 0.318023 | Grad 0.0438 \n","[2024-04-10 13:04:45,225::train::INFO] [Train] Iter 15996 | Loss 0.305579 | Grad 0.0315 \n","[2024-04-10 13:04:45,344::train::INFO] [Train] Iter 15997 | Loss 0.365773 | Grad 0.0581 \n","[2024-04-10 13:04:45,464::train::INFO] [Train] Iter 15998 | Loss 0.367190 | Grad 0.0344 \n","[2024-04-10 13:04:45,584::train::INFO] [Train] Iter 15999 | Loss 0.305166 | Grad 0.0345 \n","[2024-04-10 13:04:45,706::train::INFO] [Train] Iter 16000 | Loss 0.315540 | Grad 0.0464 \n","Validate: 100% 31/31 [00:48<00:00,  1.57s/it]\n","EMD-CD: 100% 31/31 [00:00<00:00, 136.40it/s]\n","[2024-04-10 13:05:34,630::train::INFO] [Val] Iter 16000 | CD 0.000856 | EMD 0.000000  \n","Inspect:   3% 1/31 [00:03<01:33,  3.12s/it]\n","[2024-04-10 13:05:37,992::train::INFO] [Train] Iter 16001 | Loss 0.335833 | Grad 0.0508 \n","[2024-04-10 13:05:38,113::train::INFO] [Train] Iter 16002 | Loss 0.244241 | Grad 0.0543 \n","[2024-04-10 13:05:38,233::train::INFO] [Train] Iter 16003 | Loss 0.351733 | Grad 0.0573 \n","[2024-04-10 13:05:38,351::train::INFO] [Train] Iter 16004 | Loss 0.335974 | Grad 0.0508 \n","[2024-04-10 13:05:38,472::train::INFO] [Train] Iter 16005 | Loss 0.291191 | Grad 0.0375 \n","[2024-04-10 13:05:38,591::train::INFO] [Train] Iter 16006 | Loss 0.295932 | Grad 0.0371 \n","[2024-04-10 13:05:38,712::train::INFO] [Train] Iter 16007 | Loss 0.301542 | Grad 0.0295 \n","[2024-04-10 13:05:38,833::train::INFO] [Train] Iter 16008 | Loss 0.311802 | Grad 0.0321 \n","[2024-04-10 13:05:38,952::train::INFO] [Train] Iter 16009 | Loss 0.377198 | Grad 0.0569 \n","[2024-04-10 13:05:39,073::train::INFO] [Train] Iter 16010 | Loss 0.320537 | Grad 0.0471 \n","[2024-04-10 13:05:39,193::train::INFO] [Train] Iter 16011 | Loss 0.290217 | Grad 0.0329 \n","[2024-04-10 13:05:39,312::train::INFO] [Train] Iter 16012 | Loss 0.304194 | Grad 0.0295 \n","[2024-04-10 13:05:39,431::train::INFO] [Train] Iter 16013 | Loss 0.320092 | Grad 0.0375 \n","[2024-04-10 13:05:39,551::train::INFO] [Train] Iter 16014 | Loss 0.358985 | Grad 0.0420 \n","[2024-04-10 13:05:39,671::train::INFO] [Train] Iter 16015 | Loss 0.297863 | Grad 0.0468 \n","[2024-04-10 13:05:39,790::train::INFO] [Train] Iter 16016 | Loss 0.307835 | Grad 0.0310 \n","[2024-04-10 13:05:39,912::train::INFO] [Train] Iter 16017 | Loss 0.307983 | Grad 0.0552 \n","[2024-04-10 13:05:40,032::train::INFO] [Train] Iter 16018 | Loss 0.266019 | Grad 0.0353 \n","[2024-04-10 13:05:40,152::train::INFO] [Train] Iter 16019 | Loss 0.324064 | Grad 0.0539 \n","[2024-04-10 13:05:40,228::train::INFO] [Train] Iter 16020 | Loss 0.304522 | Grad 0.0575 \n","[2024-04-10 13:05:40,347::train::INFO] [Train] Iter 16021 | Loss 0.293022 | Grad 0.0603 \n","[2024-04-10 13:05:40,466::train::INFO] [Train] Iter 16022 | Loss 0.337485 | Grad 0.0705 \n","[2024-04-10 13:05:40,585::train::INFO] [Train] Iter 16023 | Loss 0.392846 | Grad 0.0825 \n","[2024-04-10 13:05:40,704::train::INFO] [Train] Iter 16024 | Loss 0.281023 | Grad 0.0424 \n","[2024-04-10 13:05:40,831::train::INFO] [Train] Iter 16025 | Loss 0.327347 | Grad 0.0463 \n","[2024-04-10 13:05:40,952::train::INFO] [Train] Iter 16026 | Loss 0.328422 | Grad 0.0680 \n","[2024-04-10 13:05:41,072::train::INFO] [Train] Iter 16027 | Loss 0.325136 | Grad 0.0807 \n","[2024-04-10 13:05:41,196::train::INFO] [Train] Iter 16028 | Loss 0.307637 | Grad 0.0433 \n","[2024-04-10 13:05:41,316::train::INFO] [Train] Iter 16029 | Loss 0.339346 | Grad 0.0471 \n","[2024-04-10 13:05:41,437::train::INFO] [Train] Iter 16030 | Loss 0.290297 | Grad 0.0419 \n","[2024-04-10 13:05:41,561::train::INFO] [Train] Iter 16031 | Loss 0.267725 | Grad 0.0355 \n","[2024-04-10 13:05:41,686::train::INFO] [Train] Iter 16032 | Loss 0.344282 | Grad 0.0647 \n","[2024-04-10 13:05:41,808::train::INFO] [Train] Iter 16033 | Loss 0.312141 | Grad 0.0565 \n","[2024-04-10 13:05:41,933::train::INFO] [Train] Iter 16034 | Loss 0.312002 | Grad 0.0615 \n","[2024-04-10 13:05:42,065::train::INFO] [Train] Iter 16035 | Loss 0.302079 | Grad 0.0481 \n","[2024-04-10 13:05:42,189::train::INFO] [Train] Iter 16036 | Loss 0.284958 | Grad 0.0404 \n","[2024-04-10 13:05:42,309::train::INFO] [Train] Iter 16037 | Loss 0.331168 | Grad 0.0449 \n","[2024-04-10 13:05:42,431::train::INFO] [Train] Iter 16038 | Loss 0.296266 | Grad 0.0611 \n","[2024-04-10 13:05:42,553::train::INFO] [Train] Iter 16039 | Loss 0.321587 | Grad 0.0498 \n","[2024-04-10 13:05:42,674::train::INFO] [Train] Iter 16040 | Loss 0.307863 | Grad 0.0278 \n","[2024-04-10 13:05:42,795::train::INFO] [Train] Iter 16041 | Loss 0.319812 | Grad 0.0591 \n","[2024-04-10 13:05:42,916::train::INFO] [Train] Iter 16042 | Loss 0.305814 | Grad 0.0419 \n","[2024-04-10 13:05:43,037::train::INFO] [Train] Iter 16043 | Loss 0.262644 | Grad 0.0280 \n","[2024-04-10 13:05:43,158::train::INFO] [Train] Iter 16044 | Loss 0.314340 | Grad 0.0411 \n","[2024-04-10 13:05:43,282::train::INFO] [Train] Iter 16045 | Loss 0.357814 | Grad 0.0586 \n","[2024-04-10 13:05:43,403::train::INFO] [Train] Iter 16046 | Loss 0.259977 | Grad 0.0228 \n","[2024-04-10 13:05:43,526::train::INFO] [Train] Iter 16047 | Loss 0.296357 | Grad 0.0490 \n","[2024-04-10 13:05:43,647::train::INFO] [Train] Iter 16048 | Loss 0.308708 | Grad 0.0501 \n","[2024-04-10 13:05:43,768::train::INFO] [Train] Iter 16049 | Loss 0.258241 | Grad 0.0328 \n","[2024-04-10 13:05:43,889::train::INFO] [Train] Iter 16050 | Loss 0.333553 | Grad 0.0481 \n","[2024-04-10 13:05:44,007::train::INFO] [Train] Iter 16051 | Loss 0.359373 | Grad 0.0512 \n","[2024-04-10 13:05:44,127::train::INFO] [Train] Iter 16052 | Loss 0.325128 | Grad 0.0393 \n","[2024-04-10 13:05:44,246::train::INFO] [Train] Iter 16053 | Loss 0.284310 | Grad 0.0270 \n","[2024-04-10 13:05:44,365::train::INFO] [Train] Iter 16054 | Loss 0.329792 | Grad 0.0536 \n","[2024-04-10 13:05:44,484::train::INFO] [Train] Iter 16055 | Loss 0.339062 | Grad 0.0442 \n","[2024-04-10 13:05:44,604::train::INFO] [Train] Iter 16056 | Loss 0.333808 | Grad 0.0581 \n","[2024-04-10 13:05:44,725::train::INFO] [Train] Iter 16057 | Loss 0.324518 | Grad 0.0597 \n","[2024-04-10 13:05:44,844::train::INFO] [Train] Iter 16058 | Loss 0.308483 | Grad 0.0518 \n","[2024-04-10 13:05:44,965::train::INFO] [Train] Iter 16059 | Loss 0.289189 | Grad 0.0479 \n","[2024-04-10 13:05:45,086::train::INFO] [Train] Iter 16060 | Loss 0.325494 | Grad 0.0462 \n","[2024-04-10 13:05:45,206::train::INFO] [Train] Iter 16061 | Loss 0.329038 | Grad 0.0365 \n","[2024-04-10 13:05:45,326::train::INFO] [Train] Iter 16062 | Loss 0.290964 | Grad 0.0599 \n","[2024-04-10 13:05:45,446::train::INFO] [Train] Iter 16063 | Loss 0.322171 | Grad 0.0497 \n","[2024-04-10 13:05:45,566::train::INFO] [Train] Iter 16064 | Loss 0.288529 | Grad 0.0328 \n","[2024-04-10 13:05:45,640::train::INFO] [Train] Iter 16065 | Loss 0.332256 | Grad 0.0758 \n","[2024-04-10 13:05:45,760::train::INFO] [Train] Iter 16066 | Loss 0.341215 | Grad 0.0752 \n","[2024-04-10 13:05:45,887::train::INFO] [Train] Iter 16067 | Loss 0.339279 | Grad 0.0556 \n","[2024-04-10 13:05:46,011::train::INFO] [Train] Iter 16068 | Loss 0.320846 | Grad 0.0652 \n","[2024-04-10 13:05:46,130::train::INFO] [Train] Iter 16069 | Loss 0.310320 | Grad 0.0436 \n","[2024-04-10 13:05:46,248::train::INFO] [Train] Iter 16070 | Loss 0.317155 | Grad 0.0390 \n","[2024-04-10 13:05:46,367::train::INFO] [Train] Iter 16071 | Loss 0.331337 | Grad 0.0579 \n","[2024-04-10 13:05:46,486::train::INFO] [Train] Iter 16072 | Loss 0.297062 | Grad 0.0431 \n","[2024-04-10 13:05:46,605::train::INFO] [Train] Iter 16073 | Loss 0.308936 | Grad 0.0461 \n","[2024-04-10 13:05:46,724::train::INFO] [Train] Iter 16074 | Loss 0.311865 | Grad 0.0611 \n","[2024-04-10 13:05:46,844::train::INFO] [Train] Iter 16075 | Loss 0.282146 | Grad 0.0508 \n","[2024-04-10 13:05:46,966::train::INFO] [Train] Iter 16076 | Loss 0.313948 | Grad 0.0411 \n","[2024-04-10 13:05:47,086::train::INFO] [Train] Iter 16077 | Loss 0.315513 | Grad 0.0557 \n","[2024-04-10 13:05:47,205::train::INFO] [Train] Iter 16078 | Loss 0.341155 | Grad 0.0528 \n","[2024-04-10 13:05:47,324::train::INFO] [Train] Iter 16079 | Loss 0.319517 | Grad 0.0564 \n","[2024-04-10 13:05:47,443::train::INFO] [Train] Iter 16080 | Loss 0.321798 | Grad 0.0506 \n","[2024-04-10 13:05:47,562::train::INFO] [Train] Iter 16081 | Loss 0.303437 | Grad 0.0405 \n","[2024-04-10 13:05:47,681::train::INFO] [Train] Iter 16082 | Loss 0.324644 | Grad 0.0549 \n","[2024-04-10 13:05:47,802::train::INFO] [Train] Iter 16083 | Loss 0.289627 | Grad 0.0350 \n","[2024-04-10 13:05:47,922::train::INFO] [Train] Iter 16084 | Loss 0.288785 | Grad 0.0440 \n","[2024-04-10 13:05:48,041::train::INFO] [Train] Iter 16085 | Loss 0.317233 | Grad 0.0599 \n","[2024-04-10 13:05:48,163::train::INFO] [Train] Iter 16086 | Loss 0.330228 | Grad 0.0576 \n","[2024-04-10 13:05:48,282::train::INFO] [Train] Iter 16087 | Loss 0.321776 | Grad 0.0640 \n","[2024-04-10 13:05:48,404::train::INFO] [Train] Iter 16088 | Loss 0.277772 | Grad 0.0336 \n","[2024-04-10 13:05:48,524::train::INFO] [Train] Iter 16089 | Loss 0.347553 | Grad 0.0447 \n","[2024-04-10 13:05:48,644::train::INFO] [Train] Iter 16090 | Loss 0.286782 | Grad 0.0359 \n","[2024-04-10 13:05:48,766::train::INFO] [Train] Iter 16091 | Loss 0.360389 | Grad 0.0684 \n","[2024-04-10 13:05:48,887::train::INFO] [Train] Iter 16092 | Loss 0.300494 | Grad 0.0540 \n","[2024-04-10 13:05:49,008::train::INFO] [Train] Iter 16093 | Loss 0.306339 | Grad 0.0436 \n","[2024-04-10 13:05:49,128::train::INFO] [Train] Iter 16094 | Loss 0.286049 | Grad 0.0447 \n","[2024-04-10 13:05:49,249::train::INFO] [Train] Iter 16095 | Loss 0.302908 | Grad 0.0369 \n","[2024-04-10 13:05:49,370::train::INFO] [Train] Iter 16096 | Loss 0.364830 | Grad 0.0536 \n","[2024-04-10 13:05:49,492::train::INFO] [Train] Iter 16097 | Loss 0.299372 | Grad 0.0464 \n","[2024-04-10 13:05:49,611::train::INFO] [Train] Iter 16098 | Loss 0.297050 | Grad 0.0410 \n","[2024-04-10 13:05:49,731::train::INFO] [Train] Iter 16099 | Loss 0.314387 | Grad 0.0474 \n","[2024-04-10 13:05:49,851::train::INFO] [Train] Iter 16100 | Loss 0.297872 | Grad 0.0266 \n","[2024-04-10 13:05:49,970::train::INFO] [Train] Iter 16101 | Loss 0.290352 | Grad 0.0310 \n","[2024-04-10 13:05:50,089::train::INFO] [Train] Iter 16102 | Loss 0.274048 | Grad 0.0396 \n","[2024-04-10 13:05:50,208::train::INFO] [Train] Iter 16103 | Loss 0.303687 | Grad 0.0497 \n","[2024-04-10 13:05:50,326::train::INFO] [Train] Iter 16104 | Loss 0.290208 | Grad 0.0530 \n","[2024-04-10 13:05:50,445::train::INFO] [Train] Iter 16105 | Loss 0.332169 | Grad 0.0631 \n","[2024-04-10 13:05:50,564::train::INFO] [Train] Iter 16106 | Loss 0.309326 | Grad 0.0345 \n","[2024-04-10 13:05:50,683::train::INFO] [Train] Iter 16107 | Loss 0.321939 | Grad 0.0488 \n","[2024-04-10 13:05:50,803::train::INFO] [Train] Iter 16108 | Loss 0.291090 | Grad 0.0476 \n","[2024-04-10 13:05:50,922::train::INFO] [Train] Iter 16109 | Loss 0.352736 | Grad 0.0664 \n","[2024-04-10 13:05:50,997::train::INFO] [Train] Iter 16110 | Loss 0.296264 | Grad 0.0448 \n","[2024-04-10 13:05:51,115::train::INFO] [Train] Iter 16111 | Loss 0.333281 | Grad 0.0697 \n","[2024-04-10 13:05:51,235::train::INFO] [Train] Iter 16112 | Loss 0.317601 | Grad 0.0677 \n","[2024-04-10 13:05:51,353::train::INFO] [Train] Iter 16113 | Loss 0.327327 | Grad 0.0587 \n","[2024-04-10 13:05:51,471::train::INFO] [Train] Iter 16114 | Loss 0.298689 | Grad 0.0381 \n","[2024-04-10 13:05:51,589::train::INFO] [Train] Iter 16115 | Loss 0.310792 | Grad 0.0471 \n","[2024-04-10 13:05:51,708::train::INFO] [Train] Iter 16116 | Loss 0.292664 | Grad 0.0395 \n","[2024-04-10 13:05:51,827::train::INFO] [Train] Iter 16117 | Loss 0.311349 | Grad 0.0649 \n","[2024-04-10 13:05:51,947::train::INFO] [Train] Iter 16118 | Loss 0.276537 | Grad 0.0440 \n","[2024-04-10 13:05:52,067::train::INFO] [Train] Iter 16119 | Loss 0.333800 | Grad 0.0556 \n","[2024-04-10 13:05:52,187::train::INFO] [Train] Iter 16120 | Loss 0.280374 | Grad 0.0275 \n","[2024-04-10 13:05:52,308::train::INFO] [Train] Iter 16121 | Loss 0.298946 | Grad 0.0450 \n","[2024-04-10 13:05:52,429::train::INFO] [Train] Iter 16122 | Loss 0.331918 | Grad 0.0455 \n","[2024-04-10 13:05:52,548::train::INFO] [Train] Iter 16123 | Loss 0.295796 | Grad 0.0312 \n","[2024-04-10 13:05:52,668::train::INFO] [Train] Iter 16124 | Loss 0.317858 | Grad 0.0354 \n","[2024-04-10 13:05:52,788::train::INFO] [Train] Iter 16125 | Loss 0.354403 | Grad 0.0404 \n","[2024-04-10 13:05:52,907::train::INFO] [Train] Iter 16126 | Loss 0.295576 | Grad 0.0225 \n","[2024-04-10 13:05:53,027::train::INFO] [Train] Iter 16127 | Loss 0.296316 | Grad 0.0223 \n","[2024-04-10 13:05:53,149::train::INFO] [Train] Iter 16128 | Loss 0.317571 | Grad 0.0570 \n","[2024-04-10 13:05:53,272::train::INFO] [Train] Iter 16129 | Loss 0.338757 | Grad 0.0353 \n","[2024-04-10 13:05:53,394::train::INFO] [Train] Iter 16130 | Loss 0.292313 | Grad 0.0392 \n","[2024-04-10 13:05:53,514::train::INFO] [Train] Iter 16131 | Loss 0.335813 | Grad 0.0638 \n","[2024-04-10 13:05:53,632::train::INFO] [Train] Iter 16132 | Loss 0.346819 | Grad 0.0450 \n","[2024-04-10 13:05:53,752::train::INFO] [Train] Iter 16133 | Loss 0.356963 | Grad 0.0463 \n","[2024-04-10 13:05:53,872::train::INFO] [Train] Iter 16134 | Loss 0.269787 | Grad 0.0278 \n","[2024-04-10 13:05:54,004::train::INFO] [Train] Iter 16135 | Loss 0.328189 | Grad 0.0483 \n","[2024-04-10 13:05:54,127::train::INFO] [Train] Iter 16136 | Loss 0.312216 | Grad 0.0516 \n","[2024-04-10 13:05:54,248::train::INFO] [Train] Iter 16137 | Loss 0.365015 | Grad 0.0745 \n","[2024-04-10 13:05:54,369::train::INFO] [Train] Iter 16138 | Loss 0.301374 | Grad 0.0283 \n","[2024-04-10 13:05:54,490::train::INFO] [Train] Iter 16139 | Loss 0.291973 | Grad 0.0402 \n","[2024-04-10 13:05:54,616::train::INFO] [Train] Iter 16140 | Loss 0.336224 | Grad 0.0511 \n","[2024-04-10 13:05:54,738::train::INFO] [Train] Iter 16141 | Loss 0.344312 | Grad 0.0391 \n","[2024-04-10 13:05:54,859::train::INFO] [Train] Iter 16142 | Loss 0.325736 | Grad 0.0526 \n","[2024-04-10 13:05:54,983::train::INFO] [Train] Iter 16143 | Loss 0.318934 | Grad 0.0328 \n","[2024-04-10 13:05:55,106::train::INFO] [Train] Iter 16144 | Loss 0.355470 | Grad 0.0481 \n","[2024-04-10 13:05:55,229::train::INFO] [Train] Iter 16145 | Loss 0.356903 | Grad 0.0504 \n","[2024-04-10 13:05:55,352::train::INFO] [Train] Iter 16146 | Loss 0.283287 | Grad 0.0247 \n","[2024-04-10 13:05:55,476::train::INFO] [Train] Iter 16147 | Loss 0.303522 | Grad 0.0297 \n","[2024-04-10 13:05:55,597::train::INFO] [Train] Iter 16148 | Loss 0.315088 | Grad 0.0571 \n","[2024-04-10 13:05:55,716::train::INFO] [Train] Iter 16149 | Loss 0.358199 | Grad 0.0415 \n","[2024-04-10 13:05:55,837::train::INFO] [Train] Iter 16150 | Loss 0.315534 | Grad 0.0361 \n","[2024-04-10 13:05:55,959::train::INFO] [Train] Iter 16151 | Loss 0.318038 | Grad 0.0378 \n","[2024-04-10 13:05:56,080::train::INFO] [Train] Iter 16152 | Loss 0.339274 | Grad 0.0512 \n","[2024-04-10 13:05:56,201::train::INFO] [Train] Iter 16153 | Loss 0.294702 | Grad 0.0352 \n","[2024-04-10 13:05:56,323::train::INFO] [Train] Iter 16154 | Loss 0.343317 | Grad 0.0398 \n","[2024-04-10 13:05:56,400::train::INFO] [Train] Iter 16155 | Loss 0.255862 | Grad 0.0434 \n","[2024-04-10 13:05:56,522::train::INFO] [Train] Iter 16156 | Loss 0.307131 | Grad 0.0441 \n","[2024-04-10 13:05:56,643::train::INFO] [Train] Iter 16157 | Loss 0.314049 | Grad 0.0498 \n","[2024-04-10 13:05:56,765::train::INFO] [Train] Iter 16158 | Loss 0.331426 | Grad 0.0580 \n","[2024-04-10 13:05:56,888::train::INFO] [Train] Iter 16159 | Loss 0.345294 | Grad 0.0597 \n","[2024-04-10 13:05:57,012::train::INFO] [Train] Iter 16160 | Loss 0.277976 | Grad 0.0425 \n","[2024-04-10 13:05:57,137::train::INFO] [Train] Iter 16161 | Loss 0.318611 | Grad 0.0422 \n","[2024-04-10 13:05:57,257::train::INFO] [Train] Iter 16162 | Loss 0.255437 | Grad 0.0418 \n","[2024-04-10 13:05:57,376::train::INFO] [Train] Iter 16163 | Loss 0.290728 | Grad 0.0382 \n","[2024-04-10 13:05:57,495::train::INFO] [Train] Iter 16164 | Loss 0.290126 | Grad 0.0388 \n","[2024-04-10 13:05:57,616::train::INFO] [Train] Iter 16165 | Loss 0.297961 | Grad 0.0639 \n","[2024-04-10 13:05:57,735::train::INFO] [Train] Iter 16166 | Loss 0.323965 | Grad 0.0573 \n","[2024-04-10 13:05:57,855::train::INFO] [Train] Iter 16167 | Loss 0.289390 | Grad 0.0381 \n","[2024-04-10 13:05:57,974::train::INFO] [Train] Iter 16168 | Loss 0.297800 | Grad 0.0654 \n","[2024-04-10 13:05:58,092::train::INFO] [Train] Iter 16169 | Loss 0.319538 | Grad 0.0430 \n","[2024-04-10 13:05:58,218::train::INFO] [Train] Iter 16170 | Loss 0.390577 | Grad 0.0497 \n","[2024-04-10 13:05:58,340::train::INFO] [Train] Iter 16171 | Loss 0.283458 | Grad 0.0413 \n","[2024-04-10 13:05:58,459::train::INFO] [Train] Iter 16172 | Loss 0.338790 | Grad 0.0574 \n","[2024-04-10 13:05:58,577::train::INFO] [Train] Iter 16173 | Loss 0.297457 | Grad 0.0607 \n","[2024-04-10 13:05:58,696::train::INFO] [Train] Iter 16174 | Loss 0.320329 | Grad 0.0664 \n","[2024-04-10 13:05:58,815::train::INFO] [Train] Iter 16175 | Loss 0.336446 | Grad 0.0576 \n","[2024-04-10 13:05:58,935::train::INFO] [Train] Iter 16176 | Loss 0.351297 | Grad 0.0558 \n","[2024-04-10 13:05:59,055::train::INFO] [Train] Iter 16177 | Loss 0.325378 | Grad 0.0593 \n","[2024-04-10 13:05:59,174::train::INFO] [Train] Iter 16178 | Loss 0.277766 | Grad 0.0367 \n","[2024-04-10 13:05:59,293::train::INFO] [Train] Iter 16179 | Loss 0.292027 | Grad 0.0372 \n","[2024-04-10 13:05:59,412::train::INFO] [Train] Iter 16180 | Loss 0.319298 | Grad 0.0435 \n","[2024-04-10 13:05:59,531::train::INFO] [Train] Iter 16181 | Loss 0.276826 | Grad 0.0292 \n","[2024-04-10 13:05:59,650::train::INFO] [Train] Iter 16182 | Loss 0.305715 | Grad 0.0695 \n","[2024-04-10 13:05:59,769::train::INFO] [Train] Iter 16183 | Loss 0.294226 | Grad 0.0590 \n","[2024-04-10 13:05:59,889::train::INFO] [Train] Iter 16184 | Loss 0.295753 | Grad 0.0445 \n","[2024-04-10 13:06:00,009::train::INFO] [Train] Iter 16185 | Loss 0.276932 | Grad 0.0335 \n","[2024-04-10 13:06:00,129::train::INFO] [Train] Iter 16186 | Loss 0.336027 | Grad 0.0727 \n","[2024-04-10 13:06:00,248::train::INFO] [Train] Iter 16187 | Loss 0.300928 | Grad 0.0383 \n","[2024-04-10 13:06:00,368::train::INFO] [Train] Iter 16188 | Loss 0.298369 | Grad 0.0439 \n","[2024-04-10 13:06:00,487::train::INFO] [Train] Iter 16189 | Loss 0.323307 | Grad 0.0580 \n","[2024-04-10 13:06:00,606::train::INFO] [Train] Iter 16190 | Loss 0.340342 | Grad 0.0532 \n","[2024-04-10 13:06:00,726::train::INFO] [Train] Iter 16191 | Loss 0.317170 | Grad 0.0450 \n","[2024-04-10 13:06:00,845::train::INFO] [Train] Iter 16192 | Loss 0.326576 | Grad 0.0493 \n","[2024-04-10 13:06:00,964::train::INFO] [Train] Iter 16193 | Loss 0.298492 | Grad 0.0503 \n","[2024-04-10 13:06:01,083::train::INFO] [Train] Iter 16194 | Loss 0.307517 | Grad 0.0349 \n","[2024-04-10 13:06:01,203::train::INFO] [Train] Iter 16195 | Loss 0.309979 | Grad 0.0496 \n","[2024-04-10 13:06:01,322::train::INFO] [Train] Iter 16196 | Loss 0.310650 | Grad 0.0546 \n","[2024-04-10 13:06:01,441::train::INFO] [Train] Iter 16197 | Loss 0.286273 | Grad 0.0477 \n","[2024-04-10 13:06:01,559::train::INFO] [Train] Iter 16198 | Loss 0.308765 | Grad 0.0488 \n","[2024-04-10 13:06:01,678::train::INFO] [Train] Iter 16199 | Loss 0.330511 | Grad 0.0451 \n","[2024-04-10 13:06:01,753::train::INFO] [Train] Iter 16200 | Loss 0.327059 | Grad 0.0618 \n","[2024-04-10 13:06:01,871::train::INFO] [Train] Iter 16201 | Loss 0.318743 | Grad 0.0743 \n","[2024-04-10 13:06:01,990::train::INFO] [Train] Iter 16202 | Loss 0.288918 | Grad 0.0453 \n","[2024-04-10 13:06:02,110::train::INFO] [Train] Iter 16203 | Loss 0.351930 | Grad 0.0490 \n","[2024-04-10 13:06:02,231::train::INFO] [Train] Iter 16204 | Loss 0.288763 | Grad 0.0529 \n","[2024-04-10 13:06:02,354::train::INFO] [Train] Iter 16205 | Loss 0.292893 | Grad 0.0424 \n","[2024-04-10 13:06:02,474::train::INFO] [Train] Iter 16206 | Loss 0.344989 | Grad 0.0411 \n","[2024-04-10 13:06:02,593::train::INFO] [Train] Iter 16207 | Loss 0.272264 | Grad 0.0247 \n","[2024-04-10 13:06:02,712::train::INFO] [Train] Iter 16208 | Loss 0.304122 | Grad 0.0356 \n","[2024-04-10 13:06:02,830::train::INFO] [Train] Iter 16209 | Loss 0.337836 | Grad 0.0487 \n","[2024-04-10 13:06:02,951::train::INFO] [Train] Iter 16210 | Loss 0.303120 | Grad 0.0302 \n","[2024-04-10 13:06:03,069::train::INFO] [Train] Iter 16211 | Loss 0.301709 | Grad 0.0314 \n","[2024-04-10 13:06:03,189::train::INFO] [Train] Iter 16212 | Loss 0.311599 | Grad 0.0561 \n","[2024-04-10 13:06:03,308::train::INFO] [Train] Iter 16213 | Loss 0.283819 | Grad 0.0374 \n","[2024-04-10 13:06:03,428::train::INFO] [Train] Iter 16214 | Loss 0.323047 | Grad 0.0621 \n","[2024-04-10 13:06:03,548::train::INFO] [Train] Iter 16215 | Loss 0.303120 | Grad 0.0293 \n","[2024-04-10 13:06:03,667::train::INFO] [Train] Iter 16216 | Loss 0.292010 | Grad 0.0516 \n","[2024-04-10 13:06:03,786::train::INFO] [Train] Iter 16217 | Loss 0.297832 | Grad 0.0437 \n","[2024-04-10 13:06:03,907::train::INFO] [Train] Iter 16218 | Loss 0.320112 | Grad 0.0639 \n","[2024-04-10 13:06:04,028::train::INFO] [Train] Iter 16219 | Loss 0.269220 | Grad 0.0285 \n","[2024-04-10 13:06:04,148::train::INFO] [Train] Iter 16220 | Loss 0.312472 | Grad 0.0606 \n","[2024-04-10 13:06:04,271::train::INFO] [Train] Iter 16221 | Loss 0.339346 | Grad 0.0538 \n","[2024-04-10 13:06:04,391::train::INFO] [Train] Iter 16222 | Loss 0.334005 | Grad 0.0452 \n","[2024-04-10 13:06:04,510::train::INFO] [Train] Iter 16223 | Loss 0.340548 | Grad 0.0623 \n","[2024-04-10 13:06:04,630::train::INFO] [Train] Iter 16224 | Loss 0.281142 | Grad 0.0342 \n","[2024-04-10 13:06:04,750::train::INFO] [Train] Iter 16225 | Loss 0.341815 | Grad 0.0741 \n","[2024-04-10 13:06:04,872::train::INFO] [Train] Iter 16226 | Loss 0.343599 | Grad 0.0436 \n","[2024-04-10 13:06:04,992::train::INFO] [Train] Iter 16227 | Loss 0.283359 | Grad 0.0460 \n","[2024-04-10 13:06:05,111::train::INFO] [Train] Iter 16228 | Loss 0.277170 | Grad 0.0602 \n","[2024-04-10 13:06:05,232::train::INFO] [Train] Iter 16229 | Loss 0.269495 | Grad 0.0441 \n","[2024-04-10 13:06:05,352::train::INFO] [Train] Iter 16230 | Loss 0.296605 | Grad 0.0420 \n","[2024-04-10 13:06:05,472::train::INFO] [Train] Iter 16231 | Loss 0.301596 | Grad 0.0382 \n","[2024-04-10 13:06:05,593::train::INFO] [Train] Iter 16232 | Loss 0.306011 | Grad 0.0393 \n","[2024-04-10 13:06:05,713::train::INFO] [Train] Iter 16233 | Loss 0.299152 | Grad 0.0583 \n","[2024-04-10 13:06:05,833::train::INFO] [Train] Iter 16234 | Loss 0.317152 | Grad 0.0483 \n","[2024-04-10 13:06:05,954::train::INFO] [Train] Iter 16235 | Loss 0.315900 | Grad 0.0587 \n","[2024-04-10 13:06:06,073::train::INFO] [Train] Iter 16236 | Loss 0.259734 | Grad 0.0462 \n","[2024-04-10 13:06:06,193::train::INFO] [Train] Iter 16237 | Loss 0.274545 | Grad 0.0327 \n","[2024-04-10 13:06:06,313::train::INFO] [Train] Iter 16238 | Loss 0.329575 | Grad 0.0666 \n","[2024-04-10 13:06:06,432::train::INFO] [Train] Iter 16239 | Loss 0.343191 | Grad 0.0847 \n","[2024-04-10 13:06:06,551::train::INFO] [Train] Iter 16240 | Loss 0.353765 | Grad 0.0706 \n","[2024-04-10 13:06:06,669::train::INFO] [Train] Iter 16241 | Loss 0.289726 | Grad 0.0377 \n","[2024-04-10 13:06:06,791::train::INFO] [Train] Iter 16242 | Loss 0.299728 | Grad 0.0614 \n","[2024-04-10 13:06:06,911::train::INFO] [Train] Iter 16243 | Loss 0.299718 | Grad 0.0463 \n","[2024-04-10 13:06:07,030::train::INFO] [Train] Iter 16244 | Loss 0.331610 | Grad 0.0491 \n","[2024-04-10 13:06:07,108::train::INFO] [Train] Iter 16245 | Loss 0.298057 | Grad 0.0613 \n","[2024-04-10 13:06:07,230::train::INFO] [Train] Iter 16246 | Loss 0.341510 | Grad 0.0627 \n","[2024-04-10 13:06:07,352::train::INFO] [Train] Iter 16247 | Loss 0.331734 | Grad 0.0477 \n","[2024-04-10 13:06:07,477::train::INFO] [Train] Iter 16248 | Loss 0.314469 | Grad 0.0545 \n","[2024-04-10 13:06:07,601::train::INFO] [Train] Iter 16249 | Loss 0.348946 | Grad 0.0793 \n","[2024-04-10 13:06:07,722::train::INFO] [Train] Iter 16250 | Loss 0.346082 | Grad 0.0517 \n","[2024-04-10 13:06:07,841::train::INFO] [Train] Iter 16251 | Loss 0.352946 | Grad 0.0441 \n","[2024-04-10 13:06:07,961::train::INFO] [Train] Iter 16252 | Loss 0.325905 | Grad 0.0572 \n","[2024-04-10 13:06:08,082::train::INFO] [Train] Iter 16253 | Loss 0.318537 | Grad 0.0484 \n","[2024-04-10 13:06:08,203::train::INFO] [Train] Iter 16254 | Loss 0.314243 | Grad 0.0576 \n","[2024-04-10 13:06:08,324::train::INFO] [Train] Iter 16255 | Loss 0.314422 | Grad 0.0454 \n","[2024-04-10 13:06:08,446::train::INFO] [Train] Iter 16256 | Loss 0.345844 | Grad 0.0503 \n","[2024-04-10 13:06:08,567::train::INFO] [Train] Iter 16257 | Loss 0.312915 | Grad 0.0369 \n","[2024-04-10 13:06:08,687::train::INFO] [Train] Iter 16258 | Loss 0.307471 | Grad 0.0460 \n","[2024-04-10 13:06:08,812::train::INFO] [Train] Iter 16259 | Loss 0.308114 | Grad 0.0411 \n","[2024-04-10 13:06:08,932::train::INFO] [Train] Iter 16260 | Loss 0.309803 | Grad 0.0355 \n","[2024-04-10 13:06:09,054::train::INFO] [Train] Iter 16261 | Loss 0.334066 | Grad 0.0565 \n","[2024-04-10 13:06:09,174::train::INFO] [Train] Iter 16262 | Loss 0.288732 | Grad 0.0356 \n","[2024-04-10 13:06:09,294::train::INFO] [Train] Iter 16263 | Loss 0.305036 | Grad 0.0483 \n","[2024-04-10 13:06:09,415::train::INFO] [Train] Iter 16264 | Loss 0.352209 | Grad 0.0647 \n","[2024-04-10 13:06:09,535::train::INFO] [Train] Iter 16265 | Loss 0.280270 | Grad 0.0394 \n","[2024-04-10 13:06:09,659::train::INFO] [Train] Iter 16266 | Loss 0.340803 | Grad 0.0558 \n","[2024-04-10 13:06:09,780::train::INFO] [Train] Iter 16267 | Loss 0.255470 | Grad 0.0238 \n","[2024-04-10 13:06:09,901::train::INFO] [Train] Iter 16268 | Loss 0.304399 | Grad 0.0313 \n","[2024-04-10 13:06:10,023::train::INFO] [Train] Iter 16269 | Loss 0.276789 | Grad 0.0294 \n","[2024-04-10 13:06:10,146::train::INFO] [Train] Iter 16270 | Loss 0.337831 | Grad 0.0578 \n","[2024-04-10 13:06:10,268::train::INFO] [Train] Iter 16271 | Loss 0.361072 | Grad 0.0501 \n","[2024-04-10 13:06:10,389::train::INFO] [Train] Iter 16272 | Loss 0.306328 | Grad 0.0400 \n","[2024-04-10 13:06:10,508::train::INFO] [Train] Iter 16273 | Loss 0.298042 | Grad 0.0465 \n","[2024-04-10 13:06:10,629::train::INFO] [Train] Iter 16274 | Loss 0.327289 | Grad 0.0501 \n","[2024-04-10 13:06:10,749::train::INFO] [Train] Iter 16275 | Loss 0.315595 | Grad 0.0432 \n","[2024-04-10 13:06:10,869::train::INFO] [Train] Iter 16276 | Loss 0.300130 | Grad 0.0444 \n","[2024-04-10 13:06:10,988::train::INFO] [Train] Iter 16277 | Loss 0.347436 | Grad 0.0542 \n","[2024-04-10 13:06:11,107::train::INFO] [Train] Iter 16278 | Loss 0.374059 | Grad 0.0495 \n","[2024-04-10 13:06:11,226::train::INFO] [Train] Iter 16279 | Loss 0.325505 | Grad 0.0639 \n","[2024-04-10 13:06:11,345::train::INFO] [Train] Iter 16280 | Loss 0.301299 | Grad 0.0362 \n","[2024-04-10 13:06:11,463::train::INFO] [Train] Iter 16281 | Loss 0.330509 | Grad 0.0405 \n","[2024-04-10 13:06:11,583::train::INFO] [Train] Iter 16282 | Loss 0.339592 | Grad 0.0538 \n","[2024-04-10 13:06:11,704::train::INFO] [Train] Iter 16283 | Loss 0.306321 | Grad 0.0431 \n","[2024-04-10 13:06:11,822::train::INFO] [Train] Iter 16284 | Loss 0.303029 | Grad 0.0485 \n","[2024-04-10 13:06:11,942::train::INFO] [Train] Iter 16285 | Loss 0.277927 | Grad 0.0455 \n","[2024-04-10 13:06:12,061::train::INFO] [Train] Iter 16286 | Loss 0.324250 | Grad 0.0350 \n","[2024-04-10 13:06:12,179::train::INFO] [Train] Iter 16287 | Loss 0.308140 | Grad 0.0630 \n","[2024-04-10 13:06:12,298::train::INFO] [Train] Iter 16288 | Loss 0.344792 | Grad 0.0465 \n","[2024-04-10 13:06:12,417::train::INFO] [Train] Iter 16289 | Loss 0.306909 | Grad 0.0384 \n","[2024-04-10 13:06:12,492::train::INFO] [Train] Iter 16290 | Loss 0.256497 | Grad 0.0414 \n","[2024-04-10 13:06:12,611::train::INFO] [Train] Iter 16291 | Loss 0.326238 | Grad 0.0547 \n","[2024-04-10 13:06:12,733::train::INFO] [Train] Iter 16292 | Loss 0.269341 | Grad 0.0319 \n","[2024-04-10 13:06:12,850::train::INFO] [Train] Iter 16293 | Loss 0.289026 | Grad 0.0521 \n","[2024-04-10 13:06:12,969::train::INFO] [Train] Iter 16294 | Loss 0.320142 | Grad 0.0406 \n","[2024-04-10 13:06:13,088::train::INFO] [Train] Iter 16295 | Loss 0.305788 | Grad 0.0580 \n","[2024-04-10 13:06:13,207::train::INFO] [Train] Iter 16296 | Loss 0.343625 | Grad 0.0466 \n","[2024-04-10 13:06:13,331::train::INFO] [Train] Iter 16297 | Loss 0.331730 | Grad 0.0420 \n","[2024-04-10 13:06:13,451::train::INFO] [Train] Iter 16298 | Loss 0.267707 | Grad 0.0318 \n","[2024-04-10 13:06:13,571::train::INFO] [Train] Iter 16299 | Loss 0.308790 | Grad 0.0533 \n","[2024-04-10 13:06:13,690::train::INFO] [Train] Iter 16300 | Loss 0.331095 | Grad 0.0336 \n","[2024-04-10 13:06:13,810::train::INFO] [Train] Iter 16301 | Loss 0.342135 | Grad 0.0558 \n","[2024-04-10 13:06:13,932::train::INFO] [Train] Iter 16302 | Loss 0.283759 | Grad 0.0423 \n","[2024-04-10 13:06:14,052::train::INFO] [Train] Iter 16303 | Loss 0.310403 | Grad 0.0551 \n","[2024-04-10 13:06:14,173::train::INFO] [Train] Iter 16304 | Loss 0.300077 | Grad 0.0402 \n","[2024-04-10 13:06:14,293::train::INFO] [Train] Iter 16305 | Loss 0.324003 | Grad 0.0403 \n","[2024-04-10 13:06:14,413::train::INFO] [Train] Iter 16306 | Loss 0.341878 | Grad 0.0436 \n","[2024-04-10 13:06:14,535::train::INFO] [Train] Iter 16307 | Loss 0.297951 | Grad 0.0381 \n","[2024-04-10 13:06:14,655::train::INFO] [Train] Iter 16308 | Loss 0.286091 | Grad 0.0331 \n","[2024-04-10 13:06:14,776::train::INFO] [Train] Iter 16309 | Loss 0.294980 | Grad 0.0334 \n","[2024-04-10 13:06:14,895::train::INFO] [Train] Iter 16310 | Loss 0.334622 | Grad 0.0708 \n","[2024-04-10 13:06:15,016::train::INFO] [Train] Iter 16311 | Loss 0.363347 | Grad 0.0738 \n","[2024-04-10 13:06:15,142::train::INFO] [Train] Iter 16312 | Loss 0.305983 | Grad 0.0415 \n","[2024-04-10 13:06:15,261::train::INFO] [Train] Iter 16313 | Loss 0.283806 | Grad 0.0395 \n","[2024-04-10 13:06:15,384::train::INFO] [Train] Iter 16314 | Loss 0.320314 | Grad 0.0418 \n","[2024-04-10 13:06:15,504::train::INFO] [Train] Iter 16315 | Loss 0.342090 | Grad 0.0434 \n","[2024-04-10 13:06:15,623::train::INFO] [Train] Iter 16316 | Loss 0.322578 | Grad 0.0282 \n","[2024-04-10 13:06:15,744::train::INFO] [Train] Iter 16317 | Loss 0.284296 | Grad 0.0457 \n","[2024-04-10 13:06:15,866::train::INFO] [Train] Iter 16318 | Loss 0.333489 | Grad 0.0753 \n","[2024-04-10 13:06:15,987::train::INFO] [Train] Iter 16319 | Loss 0.324454 | Grad 0.0591 \n","[2024-04-10 13:06:16,108::train::INFO] [Train] Iter 16320 | Loss 0.323610 | Grad 0.0376 \n","[2024-04-10 13:06:16,229::train::INFO] [Train] Iter 16321 | Loss 0.304287 | Grad 0.0573 \n","[2024-04-10 13:06:16,350::train::INFO] [Train] Iter 16322 | Loss 0.347816 | Grad 0.0654 \n","[2024-04-10 13:06:16,471::train::INFO] [Train] Iter 16323 | Loss 0.295376 | Grad 0.0255 \n","[2024-04-10 13:06:16,591::train::INFO] [Train] Iter 16324 | Loss 0.315478 | Grad 0.0602 \n","[2024-04-10 13:06:16,710::train::INFO] [Train] Iter 16325 | Loss 0.287612 | Grad 0.0414 \n","[2024-04-10 13:06:16,832::train::INFO] [Train] Iter 16326 | Loss 0.362870 | Grad 0.0652 \n","[2024-04-10 13:06:16,953::train::INFO] [Train] Iter 16327 | Loss 0.290297 | Grad 0.0431 \n","[2024-04-10 13:06:17,073::train::INFO] [Train] Iter 16328 | Loss 0.317505 | Grad 0.0551 \n","[2024-04-10 13:06:17,193::train::INFO] [Train] Iter 16329 | Loss 0.319444 | Grad 0.0607 \n","[2024-04-10 13:06:17,313::train::INFO] [Train] Iter 16330 | Loss 0.330880 | Grad 0.0664 \n","[2024-04-10 13:06:17,433::train::INFO] [Train] Iter 16331 | Loss 0.369168 | Grad 0.0680 \n","[2024-04-10 13:06:17,556::train::INFO] [Train] Iter 16332 | Loss 0.282745 | Grad 0.0521 \n","[2024-04-10 13:06:17,676::train::INFO] [Train] Iter 16333 | Loss 0.304610 | Grad 0.0550 \n","[2024-04-10 13:06:17,795::train::INFO] [Train] Iter 16334 | Loss 0.283918 | Grad 0.0432 \n","[2024-04-10 13:06:17,870::train::INFO] [Train] Iter 16335 | Loss 0.283193 | Grad 0.0413 \n","[2024-04-10 13:06:17,990::train::INFO] [Train] Iter 16336 | Loss 0.306772 | Grad 0.0459 \n","[2024-04-10 13:06:18,110::train::INFO] [Train] Iter 16337 | Loss 0.342112 | Grad 0.0487 \n","[2024-04-10 13:06:18,231::train::INFO] [Train] Iter 16338 | Loss 0.330827 | Grad 0.0440 \n","[2024-04-10 13:06:18,351::train::INFO] [Train] Iter 16339 | Loss 0.317218 | Grad 0.0384 \n","[2024-04-10 13:06:18,472::train::INFO] [Train] Iter 16340 | Loss 0.310524 | Grad 0.0431 \n","[2024-04-10 13:06:18,593::train::INFO] [Train] Iter 16341 | Loss 0.350025 | Grad 0.0428 \n","[2024-04-10 13:06:18,714::train::INFO] [Train] Iter 16342 | Loss 0.350881 | Grad 0.0417 \n","[2024-04-10 13:06:18,835::train::INFO] [Train] Iter 16343 | Loss 0.285936 | Grad 0.0280 \n","[2024-04-10 13:06:18,955::train::INFO] [Train] Iter 16344 | Loss 0.276014 | Grad 0.0356 \n","[2024-04-10 13:06:19,077::train::INFO] [Train] Iter 16345 | Loss 0.299417 | Grad 0.0312 \n","[2024-04-10 13:06:19,198::train::INFO] [Train] Iter 16346 | Loss 0.317399 | Grad 0.0291 \n","[2024-04-10 13:06:19,318::train::INFO] [Train] Iter 16347 | Loss 0.326376 | Grad 0.0538 \n","[2024-04-10 13:06:19,439::train::INFO] [Train] Iter 16348 | Loss 0.330540 | Grad 0.0382 \n","[2024-04-10 13:06:19,559::train::INFO] [Train] Iter 16349 | Loss 0.273965 | Grad 0.0301 \n","[2024-04-10 13:06:19,679::train::INFO] [Train] Iter 16350 | Loss 0.329433 | Grad 0.0545 \n","[2024-04-10 13:06:19,798::train::INFO] [Train] Iter 16351 | Loss 0.314353 | Grad 0.0506 \n","[2024-04-10 13:06:19,919::train::INFO] [Train] Iter 16352 | Loss 0.294210 | Grad 0.0279 \n","[2024-04-10 13:06:20,038::train::INFO] [Train] Iter 16353 | Loss 0.305087 | Grad 0.0316 \n","[2024-04-10 13:06:20,157::train::INFO] [Train] Iter 16354 | Loss 0.328109 | Grad 0.0329 \n","[2024-04-10 13:06:20,277::train::INFO] [Train] Iter 16355 | Loss 0.294119 | Grad 0.0305 \n","[2024-04-10 13:06:20,406::train::INFO] [Train] Iter 16356 | Loss 0.276157 | Grad 0.0341 \n","[2024-04-10 13:06:20,529::train::INFO] [Train] Iter 16357 | Loss 0.272351 | Grad 0.0357 \n","[2024-04-10 13:06:20,651::train::INFO] [Train] Iter 16358 | Loss 0.358397 | Grad 0.0796 \n","[2024-04-10 13:06:20,778::train::INFO] [Train] Iter 16359 | Loss 0.300105 | Grad 0.0314 \n","[2024-04-10 13:06:20,906::train::INFO] [Train] Iter 16360 | Loss 0.299909 | Grad 0.0285 \n","[2024-04-10 13:06:21,035::train::INFO] [Train] Iter 16361 | Loss 0.380475 | Grad 0.0440 \n","[2024-04-10 13:06:21,155::train::INFO] [Train] Iter 16362 | Loss 0.295794 | Grad 0.0318 \n","[2024-04-10 13:06:21,278::train::INFO] [Train] Iter 16363 | Loss 0.312260 | Grad 0.0644 \n","[2024-04-10 13:06:21,400::train::INFO] [Train] Iter 16364 | Loss 0.308184 | Grad 0.0468 \n","[2024-04-10 13:06:21,524::train::INFO] [Train] Iter 16365 | Loss 0.313591 | Grad 0.0386 \n","[2024-04-10 13:06:21,644::train::INFO] [Train] Iter 16366 | Loss 0.303934 | Grad 0.0481 \n","[2024-04-10 13:06:21,766::train::INFO] [Train] Iter 16367 | Loss 0.313252 | Grad 0.0466 \n","[2024-04-10 13:06:21,887::train::INFO] [Train] Iter 16368 | Loss 0.319721 | Grad 0.0434 \n","[2024-04-10 13:06:22,014::train::INFO] [Train] Iter 16369 | Loss 0.343994 | Grad 0.0512 \n","[2024-04-10 13:06:22,133::train::INFO] [Train] Iter 16370 | Loss 0.294549 | Grad 0.0369 \n","[2024-04-10 13:06:22,254::train::INFO] [Train] Iter 16371 | Loss 0.283715 | Grad 0.0447 \n","[2024-04-10 13:06:22,373::train::INFO] [Train] Iter 16372 | Loss 0.308154 | Grad 0.0563 \n","[2024-04-10 13:06:22,492::train::INFO] [Train] Iter 16373 | Loss 0.315392 | Grad 0.0332 \n","[2024-04-10 13:06:22,613::train::INFO] [Train] Iter 16374 | Loss 0.313498 | Grad 0.0366 \n","[2024-04-10 13:06:22,737::train::INFO] [Train] Iter 16375 | Loss 0.312959 | Grad 0.0624 \n","[2024-04-10 13:06:22,859::train::INFO] [Train] Iter 16376 | Loss 0.319944 | Grad 0.0332 \n","[2024-04-10 13:06:22,983::train::INFO] [Train] Iter 16377 | Loss 0.329486 | Grad 0.0486 \n","[2024-04-10 13:06:23,105::train::INFO] [Train] Iter 16378 | Loss 0.326393 | Grad 0.0508 \n","[2024-04-10 13:06:23,228::train::INFO] [Train] Iter 16379 | Loss 0.312207 | Grad 0.0429 \n","[2024-04-10 13:06:23,305::train::INFO] [Train] Iter 16380 | Loss 0.258440 | Grad 0.0327 \n","[2024-04-10 13:06:23,426::train::INFO] [Train] Iter 16381 | Loss 0.302318 | Grad 0.0424 \n","[2024-04-10 13:06:23,546::train::INFO] [Train] Iter 16382 | Loss 0.360938 | Grad 0.0480 \n","[2024-04-10 13:06:23,665::train::INFO] [Train] Iter 16383 | Loss 0.333260 | Grad 0.0503 \n","[2024-04-10 13:06:23,786::train::INFO] [Train] Iter 16384 | Loss 0.297804 | Grad 0.0373 \n","[2024-04-10 13:06:23,906::train::INFO] [Train] Iter 16385 | Loss 0.293971 | Grad 0.0459 \n","[2024-04-10 13:06:24,025::train::INFO] [Train] Iter 16386 | Loss 0.283051 | Grad 0.0484 \n","[2024-04-10 13:06:24,145::train::INFO] [Train] Iter 16387 | Loss 0.371010 | Grad 0.0761 \n","[2024-04-10 13:06:24,264::train::INFO] [Train] Iter 16388 | Loss 0.312307 | Grad 0.0268 \n","[2024-04-10 13:06:24,385::train::INFO] [Train] Iter 16389 | Loss 0.303087 | Grad 0.0396 \n","[2024-04-10 13:06:24,505::train::INFO] [Train] Iter 16390 | Loss 0.316360 | Grad 0.0548 \n","[2024-04-10 13:06:24,625::train::INFO] [Train] Iter 16391 | Loss 0.317447 | Grad 0.0372 \n","[2024-04-10 13:06:24,744::train::INFO] [Train] Iter 16392 | Loss 0.306185 | Grad 0.0464 \n","[2024-04-10 13:06:24,863::train::INFO] [Train] Iter 16393 | Loss 0.318734 | Grad 0.0483 \n","[2024-04-10 13:06:24,983::train::INFO] [Train] Iter 16394 | Loss 0.318133 | Grad 0.0346 \n","[2024-04-10 13:06:25,104::train::INFO] [Train] Iter 16395 | Loss 0.290092 | Grad 0.0449 \n","[2024-04-10 13:06:25,222::train::INFO] [Train] Iter 16396 | Loss 0.304155 | Grad 0.0405 \n","[2024-04-10 13:06:25,341::train::INFO] [Train] Iter 16397 | Loss 0.336613 | Grad 0.0477 \n","[2024-04-10 13:06:25,461::train::INFO] [Train] Iter 16398 | Loss 0.296250 | Grad 0.0314 \n","[2024-04-10 13:06:25,581::train::INFO] [Train] Iter 16399 | Loss 0.292651 | Grad 0.0324 \n","[2024-04-10 13:06:25,702::train::INFO] [Train] Iter 16400 | Loss 0.300245 | Grad 0.0508 \n","[2024-04-10 13:06:25,823::train::INFO] [Train] Iter 16401 | Loss 0.311268 | Grad 0.0544 \n","[2024-04-10 13:06:25,942::train::INFO] [Train] Iter 16402 | Loss 0.332055 | Grad 0.0349 \n","[2024-04-10 13:06:26,062::train::INFO] [Train] Iter 16403 | Loss 0.310752 | Grad 0.0436 \n","[2024-04-10 13:06:26,183::train::INFO] [Train] Iter 16404 | Loss 0.345633 | Grad 0.0686 \n","[2024-04-10 13:06:26,303::train::INFO] [Train] Iter 16405 | Loss 0.366194 | Grad 0.0396 \n","[2024-04-10 13:06:26,425::train::INFO] [Train] Iter 16406 | Loss 0.323617 | Grad 0.0403 \n","[2024-04-10 13:06:26,544::train::INFO] [Train] Iter 16407 | Loss 0.313013 | Grad 0.0489 \n","[2024-04-10 13:06:26,664::train::INFO] [Train] Iter 16408 | Loss 0.338101 | Grad 0.0743 \n","[2024-04-10 13:06:26,785::train::INFO] [Train] Iter 16409 | Loss 0.314794 | Grad 0.0354 \n","[2024-04-10 13:06:26,905::train::INFO] [Train] Iter 16410 | Loss 0.327086 | Grad 0.0343 \n","[2024-04-10 13:06:27,025::train::INFO] [Train] Iter 16411 | Loss 0.354702 | Grad 0.0616 \n","[2024-04-10 13:06:27,146::train::INFO] [Train] Iter 16412 | Loss 0.303002 | Grad 0.0407 \n","[2024-04-10 13:06:27,266::train::INFO] [Train] Iter 16413 | Loss 0.328520 | Grad 0.0421 \n","[2024-04-10 13:06:27,386::train::INFO] [Train] Iter 16414 | Loss 0.278068 | Grad 0.0498 \n","[2024-04-10 13:06:27,505::train::INFO] [Train] Iter 16415 | Loss 0.290233 | Grad 0.0336 \n","[2024-04-10 13:06:27,627::train::INFO] [Train] Iter 16416 | Loss 0.340946 | Grad 0.0419 \n","[2024-04-10 13:06:27,748::train::INFO] [Train] Iter 16417 | Loss 0.304730 | Grad 0.0410 \n","[2024-04-10 13:06:27,869::train::INFO] [Train] Iter 16418 | Loss 0.320004 | Grad 0.0489 \n","[2024-04-10 13:06:27,989::train::INFO] [Train] Iter 16419 | Loss 0.313861 | Grad 0.0476 \n","[2024-04-10 13:06:28,111::train::INFO] [Train] Iter 16420 | Loss 0.293742 | Grad 0.0432 \n","[2024-04-10 13:06:28,232::train::INFO] [Train] Iter 16421 | Loss 0.356096 | Grad 0.0393 \n","[2024-04-10 13:06:28,351::train::INFO] [Train] Iter 16422 | Loss 0.323124 | Grad 0.0566 \n","[2024-04-10 13:06:28,471::train::INFO] [Train] Iter 16423 | Loss 0.338099 | Grad 0.0771 \n","[2024-04-10 13:06:28,592::train::INFO] [Train] Iter 16424 | Loss 0.303363 | Grad 0.0362 \n","[2024-04-10 13:06:28,668::train::INFO] [Train] Iter 16425 | Loss 0.358149 | Grad 0.0603 \n","[2024-04-10 13:06:28,787::train::INFO] [Train] Iter 16426 | Loss 0.311522 | Grad 0.0428 \n","[2024-04-10 13:06:28,907::train::INFO] [Train] Iter 16427 | Loss 0.284879 | Grad 0.0368 \n","[2024-04-10 13:06:29,025::train::INFO] [Train] Iter 16428 | Loss 0.300388 | Grad 0.0246 \n","[2024-04-10 13:06:29,144::train::INFO] [Train] Iter 16429 | Loss 0.328956 | Grad 0.0432 \n","[2024-04-10 13:06:29,264::train::INFO] [Train] Iter 16430 | Loss 0.340127 | Grad 0.0854 \n","[2024-04-10 13:06:29,385::train::INFO] [Train] Iter 16431 | Loss 0.291184 | Grad 0.0288 \n","[2024-04-10 13:06:29,504::train::INFO] [Train] Iter 16432 | Loss 0.318723 | Grad 0.0395 \n","[2024-04-10 13:06:29,624::train::INFO] [Train] Iter 16433 | Loss 0.287689 | Grad 0.0316 \n","[2024-04-10 13:06:29,744::train::INFO] [Train] Iter 16434 | Loss 0.314179 | Grad 0.0560 \n","[2024-04-10 13:06:29,863::train::INFO] [Train] Iter 16435 | Loss 0.313624 | Grad 0.0406 \n","[2024-04-10 13:06:29,983::train::INFO] [Train] Iter 16436 | Loss 0.283564 | Grad 0.0409 \n","[2024-04-10 13:06:30,103::train::INFO] [Train] Iter 16437 | Loss 0.293701 | Grad 0.0461 \n","[2024-04-10 13:06:30,223::train::INFO] [Train] Iter 16438 | Loss 0.285046 | Grad 0.0520 \n","[2024-04-10 13:06:30,343::train::INFO] [Train] Iter 16439 | Loss 0.312652 | Grad 0.0408 \n","[2024-04-10 13:06:30,464::train::INFO] [Train] Iter 16440 | Loss 0.333317 | Grad 0.0651 \n","[2024-04-10 13:06:30,585::train::INFO] [Train] Iter 16441 | Loss 0.330468 | Grad 0.0436 \n","[2024-04-10 13:06:30,705::train::INFO] [Train] Iter 16442 | Loss 0.338705 | Grad 0.0463 \n","[2024-04-10 13:06:30,826::train::INFO] [Train] Iter 16443 | Loss 0.296959 | Grad 0.0354 \n","[2024-04-10 13:06:30,947::train::INFO] [Train] Iter 16444 | Loss 0.330397 | Grad 0.0315 \n","[2024-04-10 13:06:31,068::train::INFO] [Train] Iter 16445 | Loss 0.311280 | Grad 0.0393 \n","[2024-04-10 13:06:31,188::train::INFO] [Train] Iter 16446 | Loss 0.332452 | Grad 0.0402 \n","[2024-04-10 13:06:31,308::train::INFO] [Train] Iter 16447 | Loss 0.328726 | Grad 0.0458 \n","[2024-04-10 13:06:31,427::train::INFO] [Train] Iter 16448 | Loss 0.310600 | Grad 0.0329 \n","[2024-04-10 13:06:31,548::train::INFO] [Train] Iter 16449 | Loss 0.322684 | Grad 0.0546 \n","[2024-04-10 13:06:31,668::train::INFO] [Train] Iter 16450 | Loss 0.317581 | Grad 0.0305 \n","[2024-04-10 13:06:31,789::train::INFO] [Train] Iter 16451 | Loss 0.311576 | Grad 0.0362 \n","[2024-04-10 13:06:31,909::train::INFO] [Train] Iter 16452 | Loss 0.327187 | Grad 0.0356 \n","[2024-04-10 13:06:32,029::train::INFO] [Train] Iter 16453 | Loss 0.310831 | Grad 0.0335 \n","[2024-04-10 13:06:32,149::train::INFO] [Train] Iter 16454 | Loss 0.362116 | Grad 0.0381 \n","[2024-04-10 13:06:32,269::train::INFO] [Train] Iter 16455 | Loss 0.329590 | Grad 0.0425 \n","[2024-04-10 13:06:32,388::train::INFO] [Train] Iter 16456 | Loss 0.321831 | Grad 0.0523 \n","[2024-04-10 13:06:32,507::train::INFO] [Train] Iter 16457 | Loss 0.315254 | Grad 0.0447 \n","[2024-04-10 13:06:32,627::train::INFO] [Train] Iter 16458 | Loss 0.309501 | Grad 0.0437 \n","[2024-04-10 13:06:32,746::train::INFO] [Train] Iter 16459 | Loss 0.330517 | Grad 0.0292 \n","[2024-04-10 13:06:32,865::train::INFO] [Train] Iter 16460 | Loss 0.304258 | Grad 0.0272 \n","[2024-04-10 13:06:32,984::train::INFO] [Train] Iter 16461 | Loss 0.321260 | Grad 0.0440 \n","[2024-04-10 13:06:33,105::train::INFO] [Train] Iter 16462 | Loss 0.291947 | Grad 0.0459 \n","[2024-04-10 13:06:33,226::train::INFO] [Train] Iter 16463 | Loss 0.345263 | Grad 0.0515 \n","[2024-04-10 13:06:33,347::train::INFO] [Train] Iter 16464 | Loss 0.277577 | Grad 0.0258 \n","[2024-04-10 13:06:33,468::train::INFO] [Train] Iter 16465 | Loss 0.299426 | Grad 0.0526 \n","[2024-04-10 13:06:33,592::train::INFO] [Train] Iter 16466 | Loss 0.306013 | Grad 0.0449 \n","[2024-04-10 13:06:33,716::train::INFO] [Train] Iter 16467 | Loss 0.292486 | Grad 0.0455 \n","[2024-04-10 13:06:33,837::train::INFO] [Train] Iter 16468 | Loss 0.262011 | Grad 0.0369 \n","[2024-04-10 13:06:33,956::train::INFO] [Train] Iter 16469 | Loss 0.319852 | Grad 0.0616 \n","[2024-04-10 13:06:34,034::train::INFO] [Train] Iter 16470 | Loss 0.364925 | Grad 0.0511 \n","[2024-04-10 13:06:34,154::train::INFO] [Train] Iter 16471 | Loss 0.334030 | Grad 0.0468 \n","[2024-04-10 13:06:34,278::train::INFO] [Train] Iter 16472 | Loss 0.325983 | Grad 0.0501 \n","[2024-04-10 13:06:34,399::train::INFO] [Train] Iter 16473 | Loss 0.313171 | Grad 0.0534 \n","[2024-04-10 13:06:34,519::train::INFO] [Train] Iter 16474 | Loss 0.299133 | Grad 0.0503 \n","[2024-04-10 13:06:34,642::train::INFO] [Train] Iter 16475 | Loss 0.290250 | Grad 0.0466 \n","[2024-04-10 13:06:34,763::train::INFO] [Train] Iter 16476 | Loss 0.288844 | Grad 0.0454 \n","[2024-04-10 13:06:34,885::train::INFO] [Train] Iter 16477 | Loss 0.326436 | Grad 0.0594 \n","[2024-04-10 13:06:35,006::train::INFO] [Train] Iter 16478 | Loss 0.311887 | Grad 0.0526 \n","[2024-04-10 13:06:35,129::train::INFO] [Train] Iter 16479 | Loss 0.320110 | Grad 0.0390 \n","[2024-04-10 13:06:35,251::train::INFO] [Train] Iter 16480 | Loss 0.331003 | Grad 0.0661 \n","[2024-04-10 13:06:35,373::train::INFO] [Train] Iter 16481 | Loss 0.329137 | Grad 0.0437 \n","[2024-04-10 13:06:35,493::train::INFO] [Train] Iter 16482 | Loss 0.332121 | Grad 0.0582 \n","[2024-04-10 13:06:35,614::train::INFO] [Train] Iter 16483 | Loss 0.300545 | Grad 0.0459 \n","[2024-04-10 13:06:35,734::train::INFO] [Train] Iter 16484 | Loss 0.308009 | Grad 0.0391 \n","[2024-04-10 13:06:35,854::train::INFO] [Train] Iter 16485 | Loss 0.291929 | Grad 0.0374 \n","[2024-04-10 13:06:35,979::train::INFO] [Train] Iter 16486 | Loss 0.322409 | Grad 0.0373 \n","[2024-04-10 13:06:36,103::train::INFO] [Train] Iter 16487 | Loss 0.329887 | Grad 0.0708 \n","[2024-04-10 13:06:36,225::train::INFO] [Train] Iter 16488 | Loss 0.332035 | Grad 0.0494 \n","[2024-04-10 13:06:36,349::train::INFO] [Train] Iter 16489 | Loss 0.315229 | Grad 0.0290 \n","[2024-04-10 13:06:36,472::train::INFO] [Train] Iter 16490 | Loss 0.288694 | Grad 0.0411 \n","[2024-04-10 13:06:36,594::train::INFO] [Train] Iter 16491 | Loss 0.288451 | Grad 0.0896 \n","[2024-04-10 13:06:36,717::train::INFO] [Train] Iter 16492 | Loss 0.340711 | Grad 0.0511 \n","[2024-04-10 13:06:36,836::train::INFO] [Train] Iter 16493 | Loss 0.328607 | Grad 0.0460 \n","[2024-04-10 13:06:36,956::train::INFO] [Train] Iter 16494 | Loss 0.334594 | Grad 0.0370 \n","[2024-04-10 13:06:37,078::train::INFO] [Train] Iter 16495 | Loss 0.316972 | Grad 0.0437 \n","[2024-04-10 13:06:37,199::train::INFO] [Train] Iter 16496 | Loss 0.349860 | Grad 0.0624 \n","[2024-04-10 13:06:37,320::train::INFO] [Train] Iter 16497 | Loss 0.299447 | Grad 0.0447 \n","[2024-04-10 13:06:37,440::train::INFO] [Train] Iter 16498 | Loss 0.323380 | Grad 0.0521 \n","[2024-04-10 13:06:37,560::train::INFO] [Train] Iter 16499 | Loss 0.281386 | Grad 0.0394 \n","[2024-04-10 13:06:37,682::train::INFO] [Train] Iter 16500 | Loss 0.271197 | Grad 0.0427 \n","[2024-04-10 13:06:37,803::train::INFO] [Train] Iter 16501 | Loss 0.352588 | Grad 0.0792 \n","[2024-04-10 13:06:37,922::train::INFO] [Train] Iter 16502 | Loss 0.291494 | Grad 0.0315 \n","[2024-04-10 13:06:38,041::train::INFO] [Train] Iter 16503 | Loss 0.281519 | Grad 0.0254 \n","[2024-04-10 13:06:38,161::train::INFO] [Train] Iter 16504 | Loss 0.319381 | Grad 0.0627 \n","[2024-04-10 13:06:38,281::train::INFO] [Train] Iter 16505 | Loss 0.309548 | Grad 0.0324 \n","[2024-04-10 13:06:38,400::train::INFO] [Train] Iter 16506 | Loss 0.313500 | Grad 0.0410 \n","[2024-04-10 13:06:38,522::train::INFO] [Train] Iter 16507 | Loss 0.323872 | Grad 0.0348 \n","[2024-04-10 13:06:38,641::train::INFO] [Train] Iter 16508 | Loss 0.385188 | Grad 0.0634 \n","[2024-04-10 13:06:38,760::train::INFO] [Train] Iter 16509 | Loss 0.306597 | Grad 0.0426 \n","[2024-04-10 13:06:38,881::train::INFO] [Train] Iter 16510 | Loss 0.323977 | Grad 0.0512 \n","[2024-04-10 13:06:38,999::train::INFO] [Train] Iter 16511 | Loss 0.275963 | Grad 0.0402 \n","[2024-04-10 13:06:39,119::train::INFO] [Train] Iter 16512 | Loss 0.309844 | Grad 0.0449 \n","[2024-04-10 13:06:39,238::train::INFO] [Train] Iter 16513 | Loss 0.297584 | Grad 0.0451 \n","[2024-04-10 13:06:39,357::train::INFO] [Train] Iter 16514 | Loss 0.335348 | Grad 0.0448 \n","[2024-04-10 13:06:39,431::train::INFO] [Train] Iter 16515 | Loss 0.255947 | Grad 0.0293 \n","[2024-04-10 13:06:39,553::train::INFO] [Train] Iter 16516 | Loss 0.286054 | Grad 0.0262 \n","[2024-04-10 13:06:39,673::train::INFO] [Train] Iter 16517 | Loss 0.319347 | Grad 0.0337 \n","[2024-04-10 13:06:39,793::train::INFO] [Train] Iter 16518 | Loss 0.335388 | Grad 0.0437 \n","[2024-04-10 13:06:39,914::train::INFO] [Train] Iter 16519 | Loss 0.297075 | Grad 0.0545 \n","[2024-04-10 13:06:40,034::train::INFO] [Train] Iter 16520 | Loss 0.300258 | Grad 0.0401 \n","[2024-04-10 13:06:40,155::train::INFO] [Train] Iter 16521 | Loss 0.340486 | Grad 0.0584 \n","[2024-04-10 13:06:40,275::train::INFO] [Train] Iter 16522 | Loss 0.320225 | Grad 0.0448 \n","[2024-04-10 13:06:40,396::train::INFO] [Train] Iter 16523 | Loss 0.298683 | Grad 0.0410 \n","[2024-04-10 13:06:40,517::train::INFO] [Train] Iter 16524 | Loss 0.350953 | Grad 0.0578 \n","[2024-04-10 13:06:40,636::train::INFO] [Train] Iter 16525 | Loss 0.344614 | Grad 0.0536 \n","[2024-04-10 13:06:40,756::train::INFO] [Train] Iter 16526 | Loss 0.323483 | Grad 0.0381 \n","[2024-04-10 13:06:40,878::train::INFO] [Train] Iter 16527 | Loss 0.282956 | Grad 0.0436 \n","[2024-04-10 13:06:40,998::train::INFO] [Train] Iter 16528 | Loss 0.322523 | Grad 0.0653 \n","[2024-04-10 13:06:41,118::train::INFO] [Train] Iter 16529 | Loss 0.283313 | Grad 0.0472 \n","[2024-04-10 13:06:41,239::train::INFO] [Train] Iter 16530 | Loss 0.334075 | Grad 0.0557 \n","[2024-04-10 13:06:41,360::train::INFO] [Train] Iter 16531 | Loss 0.293692 | Grad 0.0344 \n","[2024-04-10 13:06:41,479::train::INFO] [Train] Iter 16532 | Loss 0.338445 | Grad 0.0390 \n","[2024-04-10 13:06:41,605::train::INFO] [Train] Iter 16533 | Loss 0.320646 | Grad 0.0280 \n","[2024-04-10 13:06:41,725::train::INFO] [Train] Iter 16534 | Loss 0.292288 | Grad 0.0285 \n","[2024-04-10 13:06:41,847::train::INFO] [Train] Iter 16535 | Loss 0.348838 | Grad 0.0438 \n","[2024-04-10 13:06:41,966::train::INFO] [Train] Iter 16536 | Loss 0.347864 | Grad 0.0563 \n","[2024-04-10 13:06:42,085::train::INFO] [Train] Iter 16537 | Loss 0.284847 | Grad 0.0353 \n","[2024-04-10 13:06:42,206::train::INFO] [Train] Iter 16538 | Loss 0.295667 | Grad 0.0425 \n","[2024-04-10 13:06:42,324::train::INFO] [Train] Iter 16539 | Loss 0.297835 | Grad 0.0480 \n","[2024-04-10 13:06:42,443::train::INFO] [Train] Iter 16540 | Loss 0.364578 | Grad 0.0507 \n","[2024-04-10 13:06:42,562::train::INFO] [Train] Iter 16541 | Loss 0.283078 | Grad 0.0397 \n","[2024-04-10 13:06:42,682::train::INFO] [Train] Iter 16542 | Loss 0.309516 | Grad 0.0331 \n","[2024-04-10 13:06:42,802::train::INFO] [Train] Iter 16543 | Loss 0.346539 | Grad 0.0514 \n","[2024-04-10 13:06:42,920::train::INFO] [Train] Iter 16544 | Loss 0.312320 | Grad 0.0391 \n","[2024-04-10 13:06:43,039::train::INFO] [Train] Iter 16545 | Loss 0.299161 | Grad 0.0375 \n","[2024-04-10 13:06:43,157::train::INFO] [Train] Iter 16546 | Loss 0.292140 | Grad 0.0329 \n","[2024-04-10 13:06:43,277::train::INFO] [Train] Iter 16547 | Loss 0.355169 | Grad 0.0574 \n","[2024-04-10 13:06:43,398::train::INFO] [Train] Iter 16548 | Loss 0.330352 | Grad 0.0348 \n","[2024-04-10 13:06:43,519::train::INFO] [Train] Iter 16549 | Loss 0.299797 | Grad 0.0398 \n","[2024-04-10 13:06:43,641::train::INFO] [Train] Iter 16550 | Loss 0.319378 | Grad 0.0431 \n","[2024-04-10 13:06:43,760::train::INFO] [Train] Iter 16551 | Loss 0.310044 | Grad 0.0386 \n","[2024-04-10 13:06:43,880::train::INFO] [Train] Iter 16552 | Loss 0.250784 | Grad 0.0419 \n","[2024-04-10 13:06:43,999::train::INFO] [Train] Iter 16553 | Loss 0.281771 | Grad 0.0422 \n","[2024-04-10 13:06:44,121::train::INFO] [Train] Iter 16554 | Loss 0.321078 | Grad 0.0498 \n","[2024-04-10 13:06:44,240::train::INFO] [Train] Iter 16555 | Loss 0.326318 | Grad 0.0350 \n","[2024-04-10 13:06:44,362::train::INFO] [Train] Iter 16556 | Loss 0.310057 | Grad 0.0367 \n","[2024-04-10 13:06:44,482::train::INFO] [Train] Iter 16557 | Loss 0.335337 | Grad 0.0421 \n","[2024-04-10 13:06:44,601::train::INFO] [Train] Iter 16558 | Loss 0.304890 | Grad 0.0361 \n","[2024-04-10 13:06:44,725::train::INFO] [Train] Iter 16559 | Loss 0.325362 | Grad 0.0531 \n","[2024-04-10 13:06:44,799::train::INFO] [Train] Iter 16560 | Loss 0.322730 | Grad 0.0368 \n","[2024-04-10 13:06:44,918::train::INFO] [Train] Iter 16561 | Loss 0.325205 | Grad 0.0509 \n","[2024-04-10 13:06:45,037::train::INFO] [Train] Iter 16562 | Loss 0.314890 | Grad 0.0483 \n","[2024-04-10 13:06:45,157::train::INFO] [Train] Iter 16563 | Loss 0.311460 | Grad 0.0417 \n","[2024-04-10 13:06:45,275::train::INFO] [Train] Iter 16564 | Loss 0.313889 | Grad 0.0411 \n","[2024-04-10 13:06:45,393::train::INFO] [Train] Iter 16565 | Loss 0.367850 | Grad 0.0548 \n","[2024-04-10 13:06:45,512::train::INFO] [Train] Iter 16566 | Loss 0.346898 | Grad 0.0390 \n","[2024-04-10 13:06:45,631::train::INFO] [Train] Iter 16567 | Loss 0.324747 | Grad 0.0492 \n","[2024-04-10 13:06:45,751::train::INFO] [Train] Iter 16568 | Loss 0.353500 | Grad 0.0797 \n","[2024-04-10 13:06:45,872::train::INFO] [Train] Iter 16569 | Loss 0.297126 | Grad 0.0576 \n","[2024-04-10 13:06:45,992::train::INFO] [Train] Iter 16570 | Loss 0.307072 | Grad 0.0469 \n","[2024-04-10 13:06:46,112::train::INFO] [Train] Iter 16571 | Loss 0.279965 | Grad 0.0392 \n","[2024-04-10 13:06:46,232::train::INFO] [Train] Iter 16572 | Loss 0.336150 | Grad 0.0429 \n","[2024-04-10 13:06:46,351::train::INFO] [Train] Iter 16573 | Loss 0.295594 | Grad 0.0577 \n","[2024-04-10 13:06:46,470::train::INFO] [Train] Iter 16574 | Loss 0.319397 | Grad 0.0747 \n","[2024-04-10 13:06:46,589::train::INFO] [Train] Iter 16575 | Loss 0.329769 | Grad 0.0321 \n","[2024-04-10 13:06:46,710::train::INFO] [Train] Iter 16576 | Loss 0.330832 | Grad 0.0443 \n","[2024-04-10 13:06:46,833::train::INFO] [Train] Iter 16577 | Loss 0.315879 | Grad 0.0421 \n","[2024-04-10 13:06:46,953::train::INFO] [Train] Iter 16578 | Loss 0.265067 | Grad 0.0412 \n","[2024-04-10 13:06:47,075::train::INFO] [Train] Iter 16579 | Loss 0.305718 | Grad 0.0344 \n","[2024-04-10 13:06:47,195::train::INFO] [Train] Iter 16580 | Loss 0.308564 | Grad 0.0301 \n","[2024-04-10 13:06:47,315::train::INFO] [Train] Iter 16581 | Loss 0.328983 | Grad 0.0414 \n","[2024-04-10 13:06:47,441::train::INFO] [Train] Iter 16582 | Loss 0.347745 | Grad 0.0442 \n","[2024-04-10 13:06:47,562::train::INFO] [Train] Iter 16583 | Loss 0.354031 | Grad 0.0468 \n","[2024-04-10 13:06:47,685::train::INFO] [Train] Iter 16584 | Loss 0.320417 | Grad 0.0680 \n","[2024-04-10 13:06:47,810::train::INFO] [Train] Iter 16585 | Loss 0.315653 | Grad 0.0306 \n","[2024-04-10 13:06:47,930::train::INFO] [Train] Iter 16586 | Loss 0.310358 | Grad 0.0309 \n","[2024-04-10 13:06:48,053::train::INFO] [Train] Iter 16587 | Loss 0.326706 | Grad 0.0356 \n","[2024-04-10 13:06:48,172::train::INFO] [Train] Iter 16588 | Loss 0.314298 | Grad 0.0463 \n","[2024-04-10 13:06:48,295::train::INFO] [Train] Iter 16589 | Loss 0.312154 | Grad 0.0363 \n","[2024-04-10 13:06:48,415::train::INFO] [Train] Iter 16590 | Loss 0.314788 | Grad 0.0413 \n","[2024-04-10 13:06:48,535::train::INFO] [Train] Iter 16591 | Loss 0.322357 | Grad 0.0555 \n","[2024-04-10 13:06:48,656::train::INFO] [Train] Iter 16592 | Loss 0.332288 | Grad 0.0635 \n","[2024-04-10 13:06:48,777::train::INFO] [Train] Iter 16593 | Loss 0.291838 | Grad 0.0316 \n","[2024-04-10 13:06:48,898::train::INFO] [Train] Iter 16594 | Loss 0.293856 | Grad 0.0325 \n","[2024-04-10 13:06:49,019::train::INFO] [Train] Iter 16595 | Loss 0.319491 | Grad 0.0473 \n","[2024-04-10 13:06:49,143::train::INFO] [Train] Iter 16596 | Loss 0.313920 | Grad 0.0578 \n","[2024-04-10 13:06:49,264::train::INFO] [Train] Iter 16597 | Loss 0.344804 | Grad 0.0473 \n","[2024-04-10 13:06:49,386::train::INFO] [Train] Iter 16598 | Loss 0.295423 | Grad 0.0502 \n","[2024-04-10 13:06:49,509::train::INFO] [Train] Iter 16599 | Loss 0.323965 | Grad 0.0402 \n","[2024-04-10 13:06:49,630::train::INFO] [Train] Iter 16600 | Loss 0.307056 | Grad 0.0470 \n","[2024-04-10 13:06:49,751::train::INFO] [Train] Iter 16601 | Loss 0.282677 | Grad 0.0376 \n","[2024-04-10 13:06:49,874::train::INFO] [Train] Iter 16602 | Loss 0.307169 | Grad 0.0542 \n","[2024-04-10 13:06:49,993::train::INFO] [Train] Iter 16603 | Loss 0.342668 | Grad 0.0461 \n","[2024-04-10 13:06:50,112::train::INFO] [Train] Iter 16604 | Loss 0.290932 | Grad 0.0410 \n","[2024-04-10 13:06:50,187::train::INFO] [Train] Iter 16605 | Loss 0.285000 | Grad 0.0321 \n","[2024-04-10 13:06:50,307::train::INFO] [Train] Iter 16606 | Loss 0.315480 | Grad 0.0373 \n","[2024-04-10 13:06:50,427::train::INFO] [Train] Iter 16607 | Loss 0.285174 | Grad 0.0345 \n","[2024-04-10 13:06:50,547::train::INFO] [Train] Iter 16608 | Loss 0.316935 | Grad 0.0556 \n","[2024-04-10 13:06:50,667::train::INFO] [Train] Iter 16609 | Loss 0.280180 | Grad 0.0378 \n","[2024-04-10 13:06:50,794::train::INFO] [Train] Iter 16610 | Loss 0.312905 | Grad 0.0418 \n","[2024-04-10 13:06:50,914::train::INFO] [Train] Iter 16611 | Loss 0.251914 | Grad 0.0451 \n","[2024-04-10 13:06:51,034::train::INFO] [Train] Iter 16612 | Loss 0.360231 | Grad 0.0574 \n","[2024-04-10 13:06:51,154::train::INFO] [Train] Iter 16613 | Loss 0.350048 | Grad 0.0487 \n","[2024-04-10 13:06:51,274::train::INFO] [Train] Iter 16614 | Loss 0.342807 | Grad 0.0625 \n","[2024-04-10 13:06:51,393::train::INFO] [Train] Iter 16615 | Loss 0.309372 | Grad 0.0381 \n","[2024-04-10 13:06:51,511::train::INFO] [Train] Iter 16616 | Loss 0.334011 | Grad 0.0587 \n","[2024-04-10 13:06:51,630::train::INFO] [Train] Iter 16617 | Loss 0.401551 | Grad 0.0717 \n","[2024-04-10 13:06:51,750::train::INFO] [Train] Iter 16618 | Loss 0.345197 | Grad 0.0569 \n","[2024-04-10 13:06:51,869::train::INFO] [Train] Iter 16619 | Loss 0.317102 | Grad 0.0553 \n","[2024-04-10 13:06:51,989::train::INFO] [Train] Iter 16620 | Loss 0.309146 | Grad 0.0587 \n","[2024-04-10 13:06:52,109::train::INFO] [Train] Iter 16621 | Loss 0.311770 | Grad 0.0339 \n","[2024-04-10 13:06:52,230::train::INFO] [Train] Iter 16622 | Loss 0.333760 | Grad 0.0403 \n","[2024-04-10 13:06:52,351::train::INFO] [Train] Iter 16623 | Loss 0.361617 | Grad 0.0593 \n","[2024-04-10 13:06:52,471::train::INFO] [Train] Iter 16624 | Loss 0.314212 | Grad 0.0337 \n","[2024-04-10 13:06:52,592::train::INFO] [Train] Iter 16625 | Loss 0.263417 | Grad 0.0364 \n","[2024-04-10 13:06:52,713::train::INFO] [Train] Iter 16626 | Loss 0.291052 | Grad 0.0595 \n","[2024-04-10 13:06:52,831::train::INFO] [Train] Iter 16627 | Loss 0.304791 | Grad 0.0413 \n","[2024-04-10 13:06:52,954::train::INFO] [Train] Iter 16628 | Loss 0.308004 | Grad 0.0388 \n","[2024-04-10 13:06:53,073::train::INFO] [Train] Iter 16629 | Loss 0.363865 | Grad 0.0674 \n","[2024-04-10 13:06:53,193::train::INFO] [Train] Iter 16630 | Loss 0.317505 | Grad 0.0428 \n","[2024-04-10 13:06:53,313::train::INFO] [Train] Iter 16631 | Loss 0.296292 | Grad 0.0478 \n","[2024-04-10 13:06:53,434::train::INFO] [Train] Iter 16632 | Loss 0.311629 | Grad 0.0404 \n","[2024-04-10 13:06:53,555::train::INFO] [Train] Iter 16633 | Loss 0.268352 | Grad 0.0446 \n","[2024-04-10 13:06:53,674::train::INFO] [Train] Iter 16634 | Loss 0.251344 | Grad 0.0433 \n","[2024-04-10 13:06:53,795::train::INFO] [Train] Iter 16635 | Loss 0.258646 | Grad 0.0369 \n","[2024-04-10 13:06:53,916::train::INFO] [Train] Iter 16636 | Loss 0.295642 | Grad 0.0666 \n","[2024-04-10 13:06:54,035::train::INFO] [Train] Iter 16637 | Loss 0.296133 | Grad 0.0456 \n","[2024-04-10 13:06:54,156::train::INFO] [Train] Iter 16638 | Loss 0.323633 | Grad 0.0484 \n","[2024-04-10 13:06:54,276::train::INFO] [Train] Iter 16639 | Loss 0.318393 | Grad 0.0314 \n","[2024-04-10 13:06:54,397::train::INFO] [Train] Iter 16640 | Loss 0.318208 | Grad 0.0474 \n","[2024-04-10 13:06:54,518::train::INFO] [Train] Iter 16641 | Loss 0.327217 | Grad 0.0650 \n","[2024-04-10 13:06:54,639::train::INFO] [Train] Iter 16642 | Loss 0.352726 | Grad 0.0539 \n","[2024-04-10 13:06:54,759::train::INFO] [Train] Iter 16643 | Loss 0.347545 | Grad 0.0689 \n","[2024-04-10 13:06:54,879::train::INFO] [Train] Iter 16644 | Loss 0.263403 | Grad 0.0425 \n","[2024-04-10 13:06:55,005::train::INFO] [Train] Iter 16645 | Loss 0.241148 | Grad 0.0472 \n","[2024-04-10 13:06:55,125::train::INFO] [Train] Iter 16646 | Loss 0.295531 | Grad 0.0473 \n","[2024-04-10 13:06:55,244::train::INFO] [Train] Iter 16647 | Loss 0.353295 | Grad 0.0621 \n","[2024-04-10 13:06:55,363::train::INFO] [Train] Iter 16648 | Loss 0.306844 | Grad 0.0445 \n","[2024-04-10 13:06:55,483::train::INFO] [Train] Iter 16649 | Loss 0.274940 | Grad 0.0504 \n","[2024-04-10 13:06:55,558::train::INFO] [Train] Iter 16650 | Loss 0.298652 | Grad 0.0443 \n","[2024-04-10 13:06:55,678::train::INFO] [Train] Iter 16651 | Loss 0.300273 | Grad 0.0494 \n","[2024-04-10 13:06:55,799::train::INFO] [Train] Iter 16652 | Loss 0.319955 | Grad 0.0502 \n","[2024-04-10 13:06:55,919::train::INFO] [Train] Iter 16653 | Loss 0.317575 | Grad 0.0515 \n","[2024-04-10 13:06:56,041::train::INFO] [Train] Iter 16654 | Loss 0.315229 | Grad 0.0453 \n","[2024-04-10 13:06:56,162::train::INFO] [Train] Iter 16655 | Loss 0.292333 | Grad 0.0345 \n","[2024-04-10 13:06:56,283::train::INFO] [Train] Iter 16656 | Loss 0.310436 | Grad 0.0528 \n","[2024-04-10 13:06:56,402::train::INFO] [Train] Iter 16657 | Loss 0.313932 | Grad 0.0639 \n","[2024-04-10 13:06:56,523::train::INFO] [Train] Iter 16658 | Loss 0.323211 | Grad 0.0606 \n","[2024-04-10 13:06:56,644::train::INFO] [Train] Iter 16659 | Loss 0.292574 | Grad 0.0335 \n","[2024-04-10 13:06:56,767::train::INFO] [Train] Iter 16660 | Loss 0.351089 | Grad 0.0514 \n","[2024-04-10 13:06:56,887::train::INFO] [Train] Iter 16661 | Loss 0.297033 | Grad 0.0504 \n","[2024-04-10 13:06:57,008::train::INFO] [Train] Iter 16662 | Loss 0.362530 | Grad 0.0373 \n","[2024-04-10 13:06:57,130::train::INFO] [Train] Iter 16663 | Loss 0.352321 | Grad 0.0586 \n","[2024-04-10 13:06:57,249::train::INFO] [Train] Iter 16664 | Loss 0.302012 | Grad 0.0587 \n","[2024-04-10 13:06:57,369::train::INFO] [Train] Iter 16665 | Loss 0.325496 | Grad 0.0466 \n","[2024-04-10 13:06:57,489::train::INFO] [Train] Iter 16666 | Loss 0.328141 | Grad 0.0716 \n","[2024-04-10 13:06:57,612::train::INFO] [Train] Iter 16667 | Loss 0.293069 | Grad 0.0503 \n","[2024-04-10 13:06:57,732::train::INFO] [Train] Iter 16668 | Loss 0.302106 | Grad 0.0542 \n","[2024-04-10 13:06:57,853::train::INFO] [Train] Iter 16669 | Loss 0.325392 | Grad 0.0449 \n","[2024-04-10 13:06:57,978::train::INFO] [Train] Iter 16670 | Loss 0.358430 | Grad 0.0467 \n","[2024-04-10 13:06:58,098::train::INFO] [Train] Iter 16671 | Loss 0.352794 | Grad 0.1002 \n","[2024-04-10 13:06:58,217::train::INFO] [Train] Iter 16672 | Loss 0.329093 | Grad 0.0480 \n","[2024-04-10 13:06:58,337::train::INFO] [Train] Iter 16673 | Loss 0.326253 | Grad 0.0437 \n","[2024-04-10 13:06:58,457::train::INFO] [Train] Iter 16674 | Loss 0.270195 | Grad 0.0426 \n","[2024-04-10 13:06:58,577::train::INFO] [Train] Iter 16675 | Loss 0.305923 | Grad 0.0517 \n","[2024-04-10 13:06:58,698::train::INFO] [Train] Iter 16676 | Loss 0.305116 | Grad 0.0352 \n","[2024-04-10 13:06:58,819::train::INFO] [Train] Iter 16677 | Loss 0.297263 | Grad 0.0387 \n","[2024-04-10 13:06:58,938::train::INFO] [Train] Iter 16678 | Loss 0.338089 | Grad 0.0586 \n","[2024-04-10 13:06:59,057::train::INFO] [Train] Iter 16679 | Loss 0.353087 | Grad 0.0558 \n","[2024-04-10 13:06:59,177::train::INFO] [Train] Iter 16680 | Loss 0.314171 | Grad 0.0381 \n","[2024-04-10 13:06:59,297::train::INFO] [Train] Iter 16681 | Loss 0.296102 | Grad 0.0360 \n","[2024-04-10 13:06:59,418::train::INFO] [Train] Iter 16682 | Loss 0.305189 | Grad 0.0529 \n","[2024-04-10 13:06:59,537::train::INFO] [Train] Iter 16683 | Loss 0.318916 | Grad 0.0311 \n","[2024-04-10 13:06:59,655::train::INFO] [Train] Iter 16684 | Loss 0.319095 | Grad 0.0356 \n","[2024-04-10 13:06:59,775::train::INFO] [Train] Iter 16685 | Loss 0.350802 | Grad 0.0475 \n","[2024-04-10 13:06:59,897::train::INFO] [Train] Iter 16686 | Loss 0.327201 | Grad 0.0360 \n","[2024-04-10 13:07:00,018::train::INFO] [Train] Iter 16687 | Loss 0.321988 | Grad 0.0430 \n","[2024-04-10 13:07:00,143::train::INFO] [Train] Iter 16688 | Loss 0.358653 | Grad 0.1116 \n","[2024-04-10 13:07:00,263::train::INFO] [Train] Iter 16689 | Loss 0.285343 | Grad 0.0574 \n","[2024-04-10 13:07:00,384::train::INFO] [Train] Iter 16690 | Loss 0.350852 | Grad 0.0413 \n","[2024-04-10 13:07:00,505::train::INFO] [Train] Iter 16691 | Loss 0.353599 | Grad 0.0566 \n","[2024-04-10 13:07:00,626::train::INFO] [Train] Iter 16692 | Loss 0.270800 | Grad 0.0384 \n","[2024-04-10 13:07:00,747::train::INFO] [Train] Iter 16693 | Loss 0.311054 | Grad 0.0459 \n","[2024-04-10 13:07:00,867::train::INFO] [Train] Iter 16694 | Loss 0.287948 | Grad 0.0477 \n","[2024-04-10 13:07:00,944::train::INFO] [Train] Iter 16695 | Loss 0.312404 | Grad 0.0636 \n","[2024-04-10 13:07:01,067::train::INFO] [Train] Iter 16696 | Loss 0.280335 | Grad 0.0316 \n","[2024-04-10 13:07:01,194::train::INFO] [Train] Iter 16697 | Loss 0.316161 | Grad 0.0411 \n","[2024-04-10 13:07:01,318::train::INFO] [Train] Iter 16698 | Loss 0.353309 | Grad 0.0676 \n","[2024-04-10 13:07:01,438::train::INFO] [Train] Iter 16699 | Loss 0.318354 | Grad 0.0418 \n","[2024-04-10 13:07:01,559::train::INFO] [Train] Iter 16700 | Loss 0.291633 | Grad 0.0514 \n","[2024-04-10 13:07:01,680::train::INFO] [Train] Iter 16701 | Loss 0.320181 | Grad 0.0617 \n","[2024-04-10 13:07:01,801::train::INFO] [Train] Iter 16702 | Loss 0.325096 | Grad 0.0575 \n","[2024-04-10 13:07:01,921::train::INFO] [Train] Iter 16703 | Loss 0.290789 | Grad 0.0371 \n","[2024-04-10 13:07:02,041::train::INFO] [Train] Iter 16704 | Loss 0.374753 | Grad 0.0588 \n","[2024-04-10 13:07:02,163::train::INFO] [Train] Iter 16705 | Loss 0.348536 | Grad 0.0471 \n","[2024-04-10 13:07:02,284::train::INFO] [Train] Iter 16706 | Loss 0.336480 | Grad 0.0473 \n","[2024-04-10 13:07:02,406::train::INFO] [Train] Iter 16707 | Loss 0.317734 | Grad 0.0465 \n","[2024-04-10 13:07:02,526::train::INFO] [Train] Iter 16708 | Loss 0.280670 | Grad 0.0308 \n","[2024-04-10 13:07:02,647::train::INFO] [Train] Iter 16709 | Loss 0.294028 | Grad 0.0379 \n","[2024-04-10 13:07:02,773::train::INFO] [Train] Iter 16710 | Loss 0.308644 | Grad 0.0398 \n","[2024-04-10 13:07:02,895::train::INFO] [Train] Iter 16711 | Loss 0.310758 | Grad 0.0405 \n","[2024-04-10 13:07:03,015::train::INFO] [Train] Iter 16712 | Loss 0.288991 | Grad 0.0636 \n","[2024-04-10 13:07:03,135::train::INFO] [Train] Iter 16713 | Loss 0.373781 | Grad 0.0817 \n","[2024-04-10 13:07:03,271::train::INFO] [Train] Iter 16714 | Loss 0.282774 | Grad 0.0315 \n","[2024-04-10 13:07:03,392::train::INFO] [Train] Iter 16715 | Loss 0.340564 | Grad 0.0458 \n","[2024-04-10 13:07:03,513::train::INFO] [Train] Iter 16716 | Loss 0.317310 | Grad 0.0550 \n","[2024-04-10 13:07:03,633::train::INFO] [Train] Iter 16717 | Loss 0.299300 | Grad 0.0423 \n","[2024-04-10 13:07:03,753::train::INFO] [Train] Iter 16718 | Loss 0.357279 | Grad 0.0700 \n","[2024-04-10 13:07:03,875::train::INFO] [Train] Iter 16719 | Loss 0.308514 | Grad 0.0576 \n","[2024-04-10 13:07:03,996::train::INFO] [Train] Iter 16720 | Loss 0.296104 | Grad 0.0444 \n","[2024-04-10 13:07:04,115::train::INFO] [Train] Iter 16721 | Loss 0.333398 | Grad 0.0610 \n","[2024-04-10 13:07:04,236::train::INFO] [Train] Iter 16722 | Loss 0.280743 | Grad 0.0364 \n","[2024-04-10 13:07:04,355::train::INFO] [Train] Iter 16723 | Loss 0.295013 | Grad 0.0350 \n","[2024-04-10 13:07:04,476::train::INFO] [Train] Iter 16724 | Loss 0.337060 | Grad 0.0580 \n","[2024-04-10 13:07:04,597::train::INFO] [Train] Iter 16725 | Loss 0.316638 | Grad 0.0658 \n","[2024-04-10 13:07:04,718::train::INFO] [Train] Iter 16726 | Loss 0.260810 | Grad 0.0589 \n","[2024-04-10 13:07:04,838::train::INFO] [Train] Iter 16727 | Loss 0.281368 | Grad 0.0375 \n","[2024-04-10 13:07:04,959::train::INFO] [Train] Iter 16728 | Loss 0.314364 | Grad 0.0512 \n","[2024-04-10 13:07:05,080::train::INFO] [Train] Iter 16729 | Loss 0.294692 | Grad 0.0487 \n","[2024-04-10 13:07:05,200::train::INFO] [Train] Iter 16730 | Loss 0.296143 | Grad 0.0341 \n","[2024-04-10 13:07:05,322::train::INFO] [Train] Iter 16731 | Loss 0.300625 | Grad 0.0511 \n","[2024-04-10 13:07:05,442::train::INFO] [Train] Iter 16732 | Loss 0.307385 | Grad 0.0489 \n","[2024-04-10 13:07:05,562::train::INFO] [Train] Iter 16733 | Loss 0.312951 | Grad 0.0515 \n","[2024-04-10 13:07:05,682::train::INFO] [Train] Iter 16734 | Loss 0.335743 | Grad 0.0662 \n","[2024-04-10 13:07:05,801::train::INFO] [Train] Iter 16735 | Loss 0.320689 | Grad 0.0554 \n","[2024-04-10 13:07:05,920::train::INFO] [Train] Iter 16736 | Loss 0.343240 | Grad 0.0438 \n","[2024-04-10 13:07:06,039::train::INFO] [Train] Iter 16737 | Loss 0.309182 | Grad 0.0552 \n","[2024-04-10 13:07:06,158::train::INFO] [Train] Iter 16738 | Loss 0.295327 | Grad 0.0378 \n","[2024-04-10 13:07:06,278::train::INFO] [Train] Iter 16739 | Loss 0.291861 | Grad 0.0461 \n","[2024-04-10 13:07:06,353::train::INFO] [Train] Iter 16740 | Loss 0.309964 | Grad 0.0655 \n","[2024-04-10 13:07:06,471::train::INFO] [Train] Iter 16741 | Loss 0.338876 | Grad 0.0375 \n","[2024-04-10 13:07:06,590::train::INFO] [Train] Iter 16742 | Loss 0.319512 | Grad 0.0371 \n","[2024-04-10 13:07:06,709::train::INFO] [Train] Iter 16743 | Loss 0.331640 | Grad 0.0517 \n","[2024-04-10 13:07:06,828::train::INFO] [Train] Iter 16744 | Loss 0.277968 | Grad 0.0361 \n","[2024-04-10 13:07:06,947::train::INFO] [Train] Iter 16745 | Loss 0.311503 | Grad 0.0460 \n","[2024-04-10 13:07:07,067::train::INFO] [Train] Iter 16746 | Loss 0.317875 | Grad 0.0520 \n","[2024-04-10 13:07:07,188::train::INFO] [Train] Iter 16747 | Loss 0.269106 | Grad 0.0462 \n","[2024-04-10 13:07:07,307::train::INFO] [Train] Iter 16748 | Loss 0.306545 | Grad 0.0403 \n","[2024-04-10 13:07:07,426::train::INFO] [Train] Iter 16749 | Loss 0.304694 | Grad 0.0517 \n","[2024-04-10 13:07:07,545::train::INFO] [Train] Iter 16750 | Loss 0.315318 | Grad 0.0346 \n","[2024-04-10 13:07:07,665::train::INFO] [Train] Iter 16751 | Loss 0.348199 | Grad 0.0328 \n","[2024-04-10 13:07:07,786::train::INFO] [Train] Iter 16752 | Loss 0.299143 | Grad 0.0347 \n","[2024-04-10 13:07:07,907::train::INFO] [Train] Iter 16753 | Loss 0.334081 | Grad 0.0456 \n","[2024-04-10 13:07:08,026::train::INFO] [Train] Iter 16754 | Loss 0.313523 | Grad 0.0629 \n","[2024-04-10 13:07:08,146::train::INFO] [Train] Iter 16755 | Loss 0.311255 | Grad 0.0463 \n","[2024-04-10 13:07:08,266::train::INFO] [Train] Iter 16756 | Loss 0.349029 | Grad 0.0471 \n","[2024-04-10 13:07:08,394::train::INFO] [Train] Iter 16757 | Loss 0.321525 | Grad 0.0708 \n","[2024-04-10 13:07:08,513::train::INFO] [Train] Iter 16758 | Loss 0.326968 | Grad 0.0376 \n","[2024-04-10 13:07:08,633::train::INFO] [Train] Iter 16759 | Loss 0.341795 | Grad 0.0528 \n","[2024-04-10 13:07:08,754::train::INFO] [Train] Iter 16760 | Loss 0.293891 | Grad 0.0252 \n","[2024-04-10 13:07:08,874::train::INFO] [Train] Iter 16761 | Loss 0.311814 | Grad 0.0544 \n","[2024-04-10 13:07:08,993::train::INFO] [Train] Iter 16762 | Loss 0.372554 | Grad 0.0933 \n","[2024-04-10 13:07:09,112::train::INFO] [Train] Iter 16763 | Loss 0.299144 | Grad 0.0573 \n","[2024-04-10 13:07:09,231::train::INFO] [Train] Iter 16764 | Loss 0.343169 | Grad 0.0612 \n","[2024-04-10 13:07:09,349::train::INFO] [Train] Iter 16765 | Loss 0.311445 | Grad 0.0554 \n","[2024-04-10 13:07:09,468::train::INFO] [Train] Iter 16766 | Loss 0.264848 | Grad 0.0285 \n","[2024-04-10 13:07:09,589::train::INFO] [Train] Iter 16767 | Loss 0.304859 | Grad 0.0306 \n","[2024-04-10 13:07:09,709::train::INFO] [Train] Iter 16768 | Loss 0.316333 | Grad 0.0425 \n","[2024-04-10 13:07:09,828::train::INFO] [Train] Iter 16769 | Loss 0.269212 | Grad 0.0292 \n","[2024-04-10 13:07:09,950::train::INFO] [Train] Iter 16770 | Loss 0.375960 | Grad 0.0624 \n","[2024-04-10 13:07:10,069::train::INFO] [Train] Iter 16771 | Loss 0.303396 | Grad 0.0443 \n","[2024-04-10 13:07:10,191::train::INFO] [Train] Iter 16772 | Loss 0.335774 | Grad 0.0727 \n","[2024-04-10 13:07:10,310::train::INFO] [Train] Iter 16773 | Loss 0.306672 | Grad 0.0387 \n","[2024-04-10 13:07:10,431::train::INFO] [Train] Iter 16774 | Loss 0.339419 | Grad 0.0577 \n","[2024-04-10 13:07:10,550::train::INFO] [Train] Iter 16775 | Loss 0.330332 | Grad 0.0500 \n","[2024-04-10 13:07:10,671::train::INFO] [Train] Iter 16776 | Loss 0.288468 | Grad 0.0370 \n","[2024-04-10 13:07:10,794::train::INFO] [Train] Iter 16777 | Loss 0.324120 | Grad 0.0557 \n","[2024-04-10 13:07:10,915::train::INFO] [Train] Iter 16778 | Loss 0.330618 | Grad 0.0524 \n","[2024-04-10 13:07:11,035::train::INFO] [Train] Iter 16779 | Loss 0.320557 | Grad 0.0814 \n","[2024-04-10 13:07:11,156::train::INFO] [Train] Iter 16780 | Loss 0.364663 | Grad 0.0690 \n","[2024-04-10 13:07:11,277::train::INFO] [Train] Iter 16781 | Loss 0.293886 | Grad 0.0385 \n","[2024-04-10 13:07:11,398::train::INFO] [Train] Iter 16782 | Loss 0.318812 | Grad 0.0429 \n","[2024-04-10 13:07:11,518::train::INFO] [Train] Iter 16783 | Loss 0.323356 | Grad 0.0630 \n","[2024-04-10 13:07:11,638::train::INFO] [Train] Iter 16784 | Loss 0.294796 | Grad 0.0531 \n","[2024-04-10 13:07:11,714::train::INFO] [Train] Iter 16785 | Loss 0.327524 | Grad 0.0546 \n","[2024-04-10 13:07:11,835::train::INFO] [Train] Iter 16786 | Loss 0.291007 | Grad 0.0625 \n","[2024-04-10 13:07:11,954::train::INFO] [Train] Iter 16787 | Loss 0.254681 | Grad 0.0421 \n","[2024-04-10 13:07:12,073::train::INFO] [Train] Iter 16788 | Loss 0.307416 | Grad 0.0555 \n","[2024-04-10 13:07:12,192::train::INFO] [Train] Iter 16789 | Loss 0.310191 | Grad 0.0419 \n","[2024-04-10 13:07:12,311::train::INFO] [Train] Iter 16790 | Loss 0.346988 | Grad 0.0638 \n","[2024-04-10 13:07:12,430::train::INFO] [Train] Iter 16791 | Loss 0.276348 | Grad 0.0428 \n","[2024-04-10 13:07:12,549::train::INFO] [Train] Iter 16792 | Loss 0.349400 | Grad 0.0749 \n","[2024-04-10 13:07:12,670::train::INFO] [Train] Iter 16793 | Loss 0.337600 | Grad 0.0502 \n","[2024-04-10 13:07:12,793::train::INFO] [Train] Iter 16794 | Loss 0.280489 | Grad 0.0438 \n","[2024-04-10 13:07:12,912::train::INFO] [Train] Iter 16795 | Loss 0.320104 | Grad 0.0483 \n","[2024-04-10 13:07:13,031::train::INFO] [Train] Iter 16796 | Loss 0.334660 | Grad 0.0515 \n","[2024-04-10 13:07:13,155::train::INFO] [Train] Iter 16797 | Loss 0.308950 | Grad 0.0419 \n","[2024-04-10 13:07:13,276::train::INFO] [Train] Iter 16798 | Loss 0.265375 | Grad 0.0396 \n","[2024-04-10 13:07:13,400::train::INFO] [Train] Iter 16799 | Loss 0.309508 | Grad 0.0494 \n","[2024-04-10 13:07:13,523::train::INFO] [Train] Iter 16800 | Loss 0.295370 | Grad 0.0275 \n","[2024-04-10 13:07:13,644::train::INFO] [Train] Iter 16801 | Loss 0.326285 | Grad 0.0502 \n","[2024-04-10 13:07:13,764::train::INFO] [Train] Iter 16802 | Loss 0.328830 | Grad 0.0835 \n","[2024-04-10 13:07:13,883::train::INFO] [Train] Iter 16803 | Loss 0.301858 | Grad 0.0411 \n","[2024-04-10 13:07:14,003::train::INFO] [Train] Iter 16804 | Loss 0.325078 | Grad 0.0380 \n","[2024-04-10 13:07:14,123::train::INFO] [Train] Iter 16805 | Loss 0.346105 | Grad 0.0340 \n","[2024-04-10 13:07:14,246::train::INFO] [Train] Iter 16806 | Loss 0.340023 | Grad 0.0553 \n","[2024-04-10 13:07:14,367::train::INFO] [Train] Iter 16807 | Loss 0.327402 | Grad 0.0414 \n","[2024-04-10 13:07:14,489::train::INFO] [Train] Iter 16808 | Loss 0.351697 | Grad 0.0675 \n","[2024-04-10 13:07:14,611::train::INFO] [Train] Iter 16809 | Loss 0.334682 | Grad 0.0598 \n","[2024-04-10 13:07:14,733::train::INFO] [Train] Iter 16810 | Loss 0.360963 | Grad 0.0542 \n","[2024-04-10 13:07:14,858::train::INFO] [Train] Iter 16811 | Loss 0.352991 | Grad 0.0631 \n","[2024-04-10 13:07:14,981::train::INFO] [Train] Iter 16812 | Loss 0.286764 | Grad 0.0637 \n","[2024-04-10 13:07:15,102::train::INFO] [Train] Iter 16813 | Loss 0.247073 | Grad 0.0328 \n","[2024-04-10 13:07:15,223::train::INFO] [Train] Iter 16814 | Loss 0.279408 | Grad 0.0370 \n","[2024-04-10 13:07:15,346::train::INFO] [Train] Iter 16815 | Loss 0.304078 | Grad 0.0519 \n","[2024-04-10 13:07:15,472::train::INFO] [Train] Iter 16816 | Loss 0.323494 | Grad 0.0570 \n","[2024-04-10 13:07:15,592::train::INFO] [Train] Iter 16817 | Loss 0.376680 | Grad 0.0726 \n","[2024-04-10 13:07:15,718::train::INFO] [Train] Iter 16818 | Loss 0.332365 | Grad 0.0381 \n","[2024-04-10 13:07:15,840::train::INFO] [Train] Iter 16819 | Loss 0.327405 | Grad 0.0462 \n","[2024-04-10 13:07:15,962::train::INFO] [Train] Iter 16820 | Loss 0.296344 | Grad 0.0299 \n","[2024-04-10 13:07:16,083::train::INFO] [Train] Iter 16821 | Loss 0.290666 | Grad 0.0286 \n","[2024-04-10 13:07:16,203::train::INFO] [Train] Iter 16822 | Loss 0.291513 | Grad 0.0457 \n","[2024-04-10 13:07:16,323::train::INFO] [Train] Iter 16823 | Loss 0.289582 | Grad 0.0586 \n","[2024-04-10 13:07:16,442::train::INFO] [Train] Iter 16824 | Loss 0.299797 | Grad 0.0600 \n","[2024-04-10 13:07:16,560::train::INFO] [Train] Iter 16825 | Loss 0.338398 | Grad 0.0444 \n","[2024-04-10 13:07:16,679::train::INFO] [Train] Iter 16826 | Loss 0.369814 | Grad 0.0588 \n","[2024-04-10 13:07:16,799::train::INFO] [Train] Iter 16827 | Loss 0.290853 | Grad 0.0313 \n","[2024-04-10 13:07:16,917::train::INFO] [Train] Iter 16828 | Loss 0.314856 | Grad 0.0303 \n","[2024-04-10 13:07:17,037::train::INFO] [Train] Iter 16829 | Loss 0.277647 | Grad 0.0472 \n","[2024-04-10 13:07:17,113::train::INFO] [Train] Iter 16830 | Loss 0.334068 | Grad 0.0527 \n","[2024-04-10 13:07:17,233::train::INFO] [Train] Iter 16831 | Loss 0.310954 | Grad 0.0525 \n","[2024-04-10 13:07:17,353::train::INFO] [Train] Iter 16832 | Loss 0.333040 | Grad 0.0492 \n","[2024-04-10 13:07:17,473::train::INFO] [Train] Iter 16833 | Loss 0.325294 | Grad 0.0555 \n","[2024-04-10 13:07:17,592::train::INFO] [Train] Iter 16834 | Loss 0.292110 | Grad 0.0390 \n","[2024-04-10 13:07:17,711::train::INFO] [Train] Iter 16835 | Loss 0.328641 | Grad 0.0356 \n","[2024-04-10 13:07:17,830::train::INFO] [Train] Iter 16836 | Loss 0.290138 | Grad 0.0368 \n","[2024-04-10 13:07:17,951::train::INFO] [Train] Iter 16837 | Loss 0.296377 | Grad 0.0510 \n","[2024-04-10 13:07:18,070::train::INFO] [Train] Iter 16838 | Loss 0.309012 | Grad 0.0827 \n","[2024-04-10 13:07:18,191::train::INFO] [Train] Iter 16839 | Loss 0.337023 | Grad 0.0732 \n","[2024-04-10 13:07:18,310::train::INFO] [Train] Iter 16840 | Loss 0.331918 | Grad 0.0668 \n","[2024-04-10 13:07:18,429::train::INFO] [Train] Iter 16841 | Loss 0.353546 | Grad 0.0514 \n","[2024-04-10 13:07:18,547::train::INFO] [Train] Iter 16842 | Loss 0.300087 | Grad 0.0339 \n","[2024-04-10 13:07:18,668::train::INFO] [Train] Iter 16843 | Loss 0.322458 | Grad 0.0438 \n","[2024-04-10 13:07:18,787::train::INFO] [Train] Iter 16844 | Loss 0.320189 | Grad 0.0599 \n","[2024-04-10 13:07:18,908::train::INFO] [Train] Iter 16845 | Loss 0.329958 | Grad 0.0534 \n","[2024-04-10 13:07:19,027::train::INFO] [Train] Iter 16846 | Loss 0.304636 | Grad 0.0278 \n","[2024-04-10 13:07:19,147::train::INFO] [Train] Iter 16847 | Loss 0.348971 | Grad 0.0542 \n","[2024-04-10 13:07:19,267::train::INFO] [Train] Iter 16848 | Loss 0.315146 | Grad 0.0580 \n","[2024-04-10 13:07:19,387::train::INFO] [Train] Iter 16849 | Loss 0.342866 | Grad 0.0376 \n","[2024-04-10 13:07:19,507::train::INFO] [Train] Iter 16850 | Loss 0.346826 | Grad 0.0321 \n","[2024-04-10 13:07:19,627::train::INFO] [Train] Iter 16851 | Loss 0.305762 | Grad 0.0306 \n","[2024-04-10 13:07:19,747::train::INFO] [Train] Iter 16852 | Loss 0.316884 | Grad 0.0567 \n","[2024-04-10 13:07:19,869::train::INFO] [Train] Iter 16853 | Loss 0.287958 | Grad 0.0334 \n","[2024-04-10 13:07:19,989::train::INFO] [Train] Iter 16854 | Loss 0.289678 | Grad 0.0496 \n","[2024-04-10 13:07:20,108::train::INFO] [Train] Iter 16855 | Loss 0.351250 | Grad 0.0596 \n","[2024-04-10 13:07:20,227::train::INFO] [Train] Iter 16856 | Loss 0.302911 | Grad 0.0545 \n","[2024-04-10 13:07:20,345::train::INFO] [Train] Iter 16857 | Loss 0.314186 | Grad 0.0633 \n","[2024-04-10 13:07:20,465::train::INFO] [Train] Iter 16858 | Loss 0.334250 | Grad 0.0724 \n","[2024-04-10 13:07:20,582::train::INFO] [Train] Iter 16859 | Loss 0.328407 | Grad 0.0516 \n","[2024-04-10 13:07:20,702::train::INFO] [Train] Iter 16860 | Loss 0.325788 | Grad 0.0497 \n","[2024-04-10 13:07:20,822::train::INFO] [Train] Iter 16861 | Loss 0.353082 | Grad 0.0519 \n","[2024-04-10 13:07:20,941::train::INFO] [Train] Iter 16862 | Loss 0.307168 | Grad 0.0519 \n","[2024-04-10 13:07:21,061::train::INFO] [Train] Iter 16863 | Loss 0.311244 | Grad 0.0438 \n","[2024-04-10 13:07:21,189::train::INFO] [Train] Iter 16864 | Loss 0.301639 | Grad 0.0518 \n","[2024-04-10 13:07:21,308::train::INFO] [Train] Iter 16865 | Loss 0.327377 | Grad 0.0500 \n","[2024-04-10 13:07:21,428::train::INFO] [Train] Iter 16866 | Loss 0.339008 | Grad 0.0494 \n","[2024-04-10 13:07:21,548::train::INFO] [Train] Iter 16867 | Loss 0.330309 | Grad 0.0403 \n","[2024-04-10 13:07:21,667::train::INFO] [Train] Iter 16868 | Loss 0.327768 | Grad 0.0648 \n","[2024-04-10 13:07:21,790::train::INFO] [Train] Iter 16869 | Loss 0.330085 | Grad 0.0624 \n","[2024-04-10 13:07:21,910::train::INFO] [Train] Iter 16870 | Loss 0.322529 | Grad 0.0482 \n","[2024-04-10 13:07:22,031::train::INFO] [Train] Iter 16871 | Loss 0.353715 | Grad 0.0929 \n","[2024-04-10 13:07:22,150::train::INFO] [Train] Iter 16872 | Loss 0.287731 | Grad 0.0355 \n","[2024-04-10 13:07:22,270::train::INFO] [Train] Iter 16873 | Loss 0.345817 | Grad 0.0569 \n","[2024-04-10 13:07:22,391::train::INFO] [Train] Iter 16874 | Loss 0.312418 | Grad 0.0352 \n","[2024-04-10 13:07:22,467::train::INFO] [Train] Iter 16875 | Loss 0.321991 | Grad 0.0507 \n","[2024-04-10 13:07:22,587::train::INFO] [Train] Iter 16876 | Loss 0.338813 | Grad 0.0590 \n","[2024-04-10 13:07:22,708::train::INFO] [Train] Iter 16877 | Loss 0.350630 | Grad 0.0658 \n","[2024-04-10 13:07:22,827::train::INFO] [Train] Iter 16878 | Loss 0.322888 | Grad 0.0552 \n","[2024-04-10 13:07:22,946::train::INFO] [Train] Iter 16879 | Loss 0.319281 | Grad 0.0367 \n","[2024-04-10 13:07:23,064::train::INFO] [Train] Iter 16880 | Loss 0.305183 | Grad 0.0408 \n","[2024-04-10 13:07:23,183::train::INFO] [Train] Iter 16881 | Loss 0.314694 | Grad 0.0533 \n","[2024-04-10 13:07:23,304::train::INFO] [Train] Iter 16882 | Loss 0.348715 | Grad 0.0537 \n","[2024-04-10 13:07:23,422::train::INFO] [Train] Iter 16883 | Loss 0.336038 | Grad 0.0615 \n","[2024-04-10 13:07:23,542::train::INFO] [Train] Iter 16884 | Loss 0.355228 | Grad 0.0573 \n","[2024-04-10 13:07:23,660::train::INFO] [Train] Iter 16885 | Loss 0.280350 | Grad 0.0560 \n","[2024-04-10 13:07:23,779::train::INFO] [Train] Iter 16886 | Loss 0.332172 | Grad 0.0481 \n","[2024-04-10 13:07:23,898::train::INFO] [Train] Iter 16887 | Loss 0.349649 | Grad 0.0522 \n","[2024-04-10 13:07:24,019::train::INFO] [Train] Iter 16888 | Loss 0.329501 | Grad 0.0433 \n","[2024-04-10 13:07:24,138::train::INFO] [Train] Iter 16889 | Loss 0.322898 | Grad 0.0580 \n","[2024-04-10 13:07:24,261::train::INFO] [Train] Iter 16890 | Loss 0.338151 | Grad 0.0348 \n","[2024-04-10 13:07:24,382::train::INFO] [Train] Iter 16891 | Loss 0.363676 | Grad 0.0524 \n","[2024-04-10 13:07:24,502::train::INFO] [Train] Iter 16892 | Loss 0.327998 | Grad 0.0867 \n","[2024-04-10 13:07:24,622::train::INFO] [Train] Iter 16893 | Loss 0.312567 | Grad 0.0644 \n","[2024-04-10 13:07:24,740::train::INFO] [Train] Iter 16894 | Loss 0.311171 | Grad 0.0376 \n","[2024-04-10 13:07:24,858::train::INFO] [Train] Iter 16895 | Loss 0.299011 | Grad 0.0252 \n","[2024-04-10 13:07:24,979::train::INFO] [Train] Iter 16896 | Loss 0.322469 | Grad 0.0294 \n","[2024-04-10 13:07:25,098::train::INFO] [Train] Iter 16897 | Loss 0.328221 | Grad 0.0339 \n","[2024-04-10 13:07:25,216::train::INFO] [Train] Iter 16898 | Loss 0.317379 | Grad 0.0379 \n","[2024-04-10 13:07:25,335::train::INFO] [Train] Iter 16899 | Loss 0.336340 | Grad 0.0699 \n","[2024-04-10 13:07:25,455::train::INFO] [Train] Iter 16900 | Loss 0.306669 | Grad 0.0393 \n","[2024-04-10 13:07:25,575::train::INFO] [Train] Iter 16901 | Loss 0.312025 | Grad 0.0339 \n","[2024-04-10 13:07:25,695::train::INFO] [Train] Iter 16902 | Loss 0.332440 | Grad 0.0499 \n","[2024-04-10 13:07:25,815::train::INFO] [Train] Iter 16903 | Loss 0.297080 | Grad 0.0359 \n","[2024-04-10 13:07:25,941::train::INFO] [Train] Iter 16904 | Loss 0.368174 | Grad 0.0549 \n","[2024-04-10 13:07:26,061::train::INFO] [Train] Iter 16905 | Loss 0.301035 | Grad 0.0444 \n","[2024-04-10 13:07:26,180::train::INFO] [Train] Iter 16906 | Loss 0.341254 | Grad 0.0536 \n","[2024-04-10 13:07:26,304::train::INFO] [Train] Iter 16907 | Loss 0.322290 | Grad 0.0376 \n","[2024-04-10 13:07:26,434::train::INFO] [Train] Iter 16908 | Loss 0.326563 | Grad 0.0424 \n","[2024-04-10 13:07:26,554::train::INFO] [Train] Iter 16909 | Loss 0.325611 | Grad 0.0442 \n","[2024-04-10 13:07:26,674::train::INFO] [Train] Iter 16910 | Loss 0.326740 | Grad 0.0482 \n","[2024-04-10 13:07:26,794::train::INFO] [Train] Iter 16911 | Loss 0.287474 | Grad 0.0311 \n","[2024-04-10 13:07:26,915::train::INFO] [Train] Iter 16912 | Loss 0.367045 | Grad 0.0446 \n","[2024-04-10 13:07:27,035::train::INFO] [Train] Iter 16913 | Loss 0.317284 | Grad 0.0469 \n","[2024-04-10 13:07:27,155::train::INFO] [Train] Iter 16914 | Loss 0.346317 | Grad 0.0503 \n","[2024-04-10 13:07:27,275::train::INFO] [Train] Iter 16915 | Loss 0.303978 | Grad 0.0376 \n","[2024-04-10 13:07:27,397::train::INFO] [Train] Iter 16916 | Loss 0.329210 | Grad 0.0522 \n","[2024-04-10 13:07:27,519::train::INFO] [Train] Iter 16917 | Loss 0.333251 | Grad 0.0386 \n","[2024-04-10 13:07:27,639::train::INFO] [Train] Iter 16918 | Loss 0.289578 | Grad 0.0270 \n","[2024-04-10 13:07:27,761::train::INFO] [Train] Iter 16919 | Loss 0.272934 | Grad 0.0284 \n","[2024-04-10 13:07:27,838::train::INFO] [Train] Iter 16920 | Loss 0.342373 | Grad 0.0503 \n","[2024-04-10 13:07:27,962::train::INFO] [Train] Iter 16921 | Loss 0.311892 | Grad 0.0359 \n","[2024-04-10 13:07:28,088::train::INFO] [Train] Iter 16922 | Loss 0.295707 | Grad 0.0411 \n","[2024-04-10 13:07:28,209::train::INFO] [Train] Iter 16923 | Loss 0.316079 | Grad 0.0473 \n","[2024-04-10 13:07:28,341::train::INFO] [Train] Iter 16924 | Loss 0.309493 | Grad 0.0529 \n","[2024-04-10 13:07:28,462::train::INFO] [Train] Iter 16925 | Loss 0.311074 | Grad 0.0686 \n","[2024-04-10 13:07:28,582::train::INFO] [Train] Iter 16926 | Loss 0.299267 | Grad 0.0497 \n","[2024-04-10 13:07:28,703::train::INFO] [Train] Iter 16927 | Loss 0.358808 | Grad 0.0480 \n","[2024-04-10 13:07:28,827::train::INFO] [Train] Iter 16928 | Loss 0.373499 | Grad 0.0717 \n","[2024-04-10 13:07:28,950::train::INFO] [Train] Iter 16929 | Loss 0.272387 | Grad 0.0622 \n","[2024-04-10 13:07:29,071::train::INFO] [Train] Iter 16930 | Loss 0.321736 | Grad 0.0474 \n","[2024-04-10 13:07:29,196::train::INFO] [Train] Iter 16931 | Loss 0.301684 | Grad 0.0455 \n","[2024-04-10 13:07:29,320::train::INFO] [Train] Iter 16932 | Loss 0.305222 | Grad 0.0416 \n","[2024-04-10 13:07:29,441::train::INFO] [Train] Iter 16933 | Loss 0.300601 | Grad 0.0280 \n","[2024-04-10 13:07:29,561::train::INFO] [Train] Iter 16934 | Loss 0.344318 | Grad 0.0477 \n","[2024-04-10 13:07:29,681::train::INFO] [Train] Iter 16935 | Loss 0.342288 | Grad 0.0353 \n","[2024-04-10 13:07:29,801::train::INFO] [Train] Iter 16936 | Loss 0.328825 | Grad 0.0391 \n","[2024-04-10 13:07:29,921::train::INFO] [Train] Iter 16937 | Loss 0.302882 | Grad 0.0514 \n","[2024-04-10 13:07:30,041::train::INFO] [Train] Iter 16938 | Loss 0.286229 | Grad 0.0312 \n","[2024-04-10 13:07:30,160::train::INFO] [Train] Iter 16939 | Loss 0.345945 | Grad 0.0591 \n","[2024-04-10 13:07:30,279::train::INFO] [Train] Iter 16940 | Loss 0.311105 | Grad 0.0540 \n","[2024-04-10 13:07:30,397::train::INFO] [Train] Iter 16941 | Loss 0.323883 | Grad 0.0450 \n","[2024-04-10 13:07:30,516::train::INFO] [Train] Iter 16942 | Loss 0.309386 | Grad 0.0275 \n","[2024-04-10 13:07:30,635::train::INFO] [Train] Iter 16943 | Loss 0.304743 | Grad 0.0479 \n","[2024-04-10 13:07:30,754::train::INFO] [Train] Iter 16944 | Loss 0.300974 | Grad 0.0434 \n","[2024-04-10 13:07:30,873::train::INFO] [Train] Iter 16945 | Loss 0.327331 | Grad 0.0467 \n","[2024-04-10 13:07:30,992::train::INFO] [Train] Iter 16946 | Loss 0.334599 | Grad 0.0562 \n","[2024-04-10 13:07:31,113::train::INFO] [Train] Iter 16947 | Loss 0.306552 | Grad 0.0332 \n","[2024-04-10 13:07:31,233::train::INFO] [Train] Iter 16948 | Loss 0.284252 | Grad 0.0443 \n","[2024-04-10 13:07:31,354::train::INFO] [Train] Iter 16949 | Loss 0.350080 | Grad 0.0694 \n","[2024-04-10 13:07:31,473::train::INFO] [Train] Iter 16950 | Loss 0.297407 | Grad 0.0366 \n","[2024-04-10 13:07:31,593::train::INFO] [Train] Iter 16951 | Loss 0.330506 | Grad 0.0425 \n","[2024-04-10 13:07:31,712::train::INFO] [Train] Iter 16952 | Loss 0.284835 | Grad 0.0581 \n","[2024-04-10 13:07:31,832::train::INFO] [Train] Iter 16953 | Loss 0.335530 | Grad 0.0459 \n","[2024-04-10 13:07:31,952::train::INFO] [Train] Iter 16954 | Loss 0.328043 | Grad 0.0414 \n","[2024-04-10 13:07:32,074::train::INFO] [Train] Iter 16955 | Loss 0.293673 | Grad 0.0543 \n","[2024-04-10 13:07:32,194::train::INFO] [Train] Iter 16956 | Loss 0.355183 | Grad 0.0576 \n","[2024-04-10 13:07:32,313::train::INFO] [Train] Iter 16957 | Loss 0.327103 | Grad 0.0377 \n","[2024-04-10 13:07:32,434::train::INFO] [Train] Iter 16958 | Loss 0.316230 | Grad 0.0557 \n","[2024-04-10 13:07:32,555::train::INFO] [Train] Iter 16959 | Loss 0.296724 | Grad 0.0440 \n","[2024-04-10 13:07:32,674::train::INFO] [Train] Iter 16960 | Loss 0.312763 | Grad 0.0372 \n","[2024-04-10 13:07:32,794::train::INFO] [Train] Iter 16961 | Loss 0.307732 | Grad 0.0500 \n","[2024-04-10 13:07:32,914::train::INFO] [Train] Iter 16962 | Loss 0.372315 | Grad 0.0793 \n","[2024-04-10 13:07:33,032::train::INFO] [Train] Iter 16963 | Loss 0.330281 | Grad 0.0420 \n","[2024-04-10 13:07:33,151::train::INFO] [Train] Iter 16964 | Loss 0.337433 | Grad 0.0590 \n","[2024-04-10 13:07:33,235::train::INFO] [Train] Iter 16965 | Loss 0.308002 | Grad 0.0414 \n","[2024-04-10 13:07:33,354::train::INFO] [Train] Iter 16966 | Loss 0.327996 | Grad 0.0430 \n","[2024-04-10 13:07:33,474::train::INFO] [Train] Iter 16967 | Loss 0.311646 | Grad 0.0405 \n","[2024-04-10 13:07:33,593::train::INFO] [Train] Iter 16968 | Loss 0.269606 | Grad 0.0409 \n","[2024-04-10 13:07:33,711::train::INFO] [Train] Iter 16969 | Loss 0.276018 | Grad 0.0399 \n","[2024-04-10 13:07:33,831::train::INFO] [Train] Iter 16970 | Loss 0.293958 | Grad 0.0510 \n","[2024-04-10 13:07:33,951::train::INFO] [Train] Iter 16971 | Loss 0.280809 | Grad 0.0359 \n","[2024-04-10 13:07:34,071::train::INFO] [Train] Iter 16972 | Loss 0.295923 | Grad 0.0384 \n","[2024-04-10 13:07:34,192::train::INFO] [Train] Iter 16973 | Loss 0.301645 | Grad 0.0533 \n","[2024-04-10 13:07:34,312::train::INFO] [Train] Iter 16974 | Loss 0.320805 | Grad 0.0565 \n","[2024-04-10 13:07:34,431::train::INFO] [Train] Iter 16975 | Loss 0.340977 | Grad 0.0626 \n","[2024-04-10 13:07:34,550::train::INFO] [Train] Iter 16976 | Loss 0.334509 | Grad 0.0610 \n","[2024-04-10 13:07:34,670::train::INFO] [Train] Iter 16977 | Loss 0.315473 | Grad 0.0567 \n","[2024-04-10 13:07:34,790::train::INFO] [Train] Iter 16978 | Loss 0.326951 | Grad 0.0452 \n","[2024-04-10 13:07:34,908::train::INFO] [Train] Iter 16979 | Loss 0.321801 | Grad 0.0429 \n","[2024-04-10 13:07:35,028::train::INFO] [Train] Iter 16980 | Loss 0.269950 | Grad 0.0253 \n","[2024-04-10 13:07:35,146::train::INFO] [Train] Iter 16981 | Loss 0.275049 | Grad 0.0378 \n","[2024-04-10 13:07:35,264::train::INFO] [Train] Iter 16982 | Loss 0.315802 | Grad 0.0657 \n","[2024-04-10 13:07:35,383::train::INFO] [Train] Iter 16983 | Loss 0.325749 | Grad 0.0635 \n","[2024-04-10 13:07:35,502::train::INFO] [Train] Iter 16984 | Loss 0.346295 | Grad 0.0463 \n","[2024-04-10 13:07:35,621::train::INFO] [Train] Iter 16985 | Loss 0.315854 | Grad 0.0456 \n","[2024-04-10 13:07:35,741::train::INFO] [Train] Iter 16986 | Loss 0.318940 | Grad 0.0407 \n","[2024-04-10 13:07:35,860::train::INFO] [Train] Iter 16987 | Loss 0.324821 | Grad 0.0602 \n","[2024-04-10 13:07:35,981::train::INFO] [Train] Iter 16988 | Loss 0.318755 | Grad 0.0561 \n","[2024-04-10 13:07:36,103::train::INFO] [Train] Iter 16989 | Loss 0.242863 | Grad 0.0245 \n","[2024-04-10 13:07:36,224::train::INFO] [Train] Iter 16990 | Loss 0.359516 | Grad 0.0604 \n","[2024-04-10 13:07:36,347::train::INFO] [Train] Iter 16991 | Loss 0.318773 | Grad 0.0550 \n","[2024-04-10 13:07:36,466::train::INFO] [Train] Iter 16992 | Loss 0.352018 | Grad 0.0753 \n","[2024-04-10 13:07:36,587::train::INFO] [Train] Iter 16993 | Loss 0.349712 | Grad 0.0467 \n","[2024-04-10 13:07:36,709::train::INFO] [Train] Iter 16994 | Loss 0.309331 | Grad 0.0579 \n","[2024-04-10 13:07:36,830::train::INFO] [Train] Iter 16995 | Loss 0.342891 | Grad 0.0381 \n","[2024-04-10 13:07:36,949::train::INFO] [Train] Iter 16996 | Loss 0.295477 | Grad 0.0443 \n","[2024-04-10 13:07:37,069::train::INFO] [Train] Iter 16997 | Loss 0.309098 | Grad 0.0304 \n","[2024-04-10 13:07:37,189::train::INFO] [Train] Iter 16998 | Loss 0.314722 | Grad 0.0282 \n","[2024-04-10 13:07:37,311::train::INFO] [Train] Iter 16999 | Loss 0.297893 | Grad 0.0706 \n","[2024-04-10 13:07:37,430::train::INFO] [Train] Iter 17000 | Loss 0.325050 | Grad 0.0515 \n","Validate: 100% 31/31 [00:48<00:00,  1.57s/it]\n","EMD-CD: 100% 31/31 [00:00<00:00, 136.54it/s]\n","[2024-04-10 13:08:26,368::train::INFO] [Val] Iter 17000 | CD 0.000834 | EMD 0.000000  \n","Inspect:   3% 1/31 [00:03<01:33,  3.12s/it]\n","[2024-04-10 13:08:29,732::train::INFO] [Train] Iter 17001 | Loss 0.301868 | Grad 0.0384 \n","[2024-04-10 13:08:29,860::train::INFO] [Train] Iter 17002 | Loss 0.312215 | Grad 0.0392 \n","[2024-04-10 13:08:29,979::train::INFO] [Train] Iter 17003 | Loss 0.327413 | Grad 0.0627 \n","[2024-04-10 13:08:30,098::train::INFO] [Train] Iter 17004 | Loss 0.312699 | Grad 0.0412 \n","[2024-04-10 13:08:30,217::train::INFO] [Train] Iter 17005 | Loss 0.312342 | Grad 0.0483 \n","[2024-04-10 13:08:30,336::train::INFO] [Train] Iter 17006 | Loss 0.316015 | Grad 0.0402 \n","[2024-04-10 13:08:30,457::train::INFO] [Train] Iter 17007 | Loss 0.307229 | Grad 0.0487 \n","[2024-04-10 13:08:30,575::train::INFO] [Train] Iter 17008 | Loss 0.333415 | Grad 0.0551 \n","[2024-04-10 13:08:30,694::train::INFO] [Train] Iter 17009 | Loss 0.362059 | Grad 0.0585 \n","[2024-04-10 13:08:30,769::train::INFO] [Train] Iter 17010 | Loss 0.285808 | Grad 0.0378 \n","[2024-04-10 13:08:30,888::train::INFO] [Train] Iter 17011 | Loss 0.323239 | Grad 0.0464 \n","[2024-04-10 13:08:31,006::train::INFO] [Train] Iter 17012 | Loss 0.307837 | Grad 0.0417 \n","[2024-04-10 13:08:31,126::train::INFO] [Train] Iter 17013 | Loss 0.325678 | Grad 0.0559 \n","[2024-04-10 13:08:31,246::train::INFO] [Train] Iter 17014 | Loss 0.364402 | Grad 0.0565 \n","[2024-04-10 13:08:31,366::train::INFO] [Train] Iter 17015 | Loss 0.285230 | Grad 0.0385 \n","[2024-04-10 13:08:31,488::train::INFO] [Train] Iter 17016 | Loss 0.340023 | Grad 0.0448 \n","[2024-04-10 13:08:31,609::train::INFO] [Train] Iter 17017 | Loss 0.320592 | Grad 0.0457 \n","[2024-04-10 13:08:31,729::train::INFO] [Train] Iter 17018 | Loss 0.326621 | Grad 0.0551 \n","[2024-04-10 13:08:31,849::train::INFO] [Train] Iter 17019 | Loss 0.355212 | Grad 0.0588 \n","[2024-04-10 13:08:31,973::train::INFO] [Train] Iter 17020 | Loss 0.306602 | Grad 0.0429 \n","[2024-04-10 13:08:32,095::train::INFO] [Train] Iter 17021 | Loss 0.314190 | Grad 0.0383 \n","[2024-04-10 13:08:32,217::train::INFO] [Train] Iter 17022 | Loss 0.318530 | Grad 0.0365 \n","[2024-04-10 13:08:32,339::train::INFO] [Train] Iter 17023 | Loss 0.324712 | Grad 0.0427 \n","[2024-04-10 13:08:32,462::train::INFO] [Train] Iter 17024 | Loss 0.316691 | Grad 0.0317 \n","[2024-04-10 13:08:32,593::train::INFO] [Train] Iter 17025 | Loss 0.374734 | Grad 0.0703 \n","[2024-04-10 13:08:32,718::train::INFO] [Train] Iter 17026 | Loss 0.316241 | Grad 0.0311 \n","[2024-04-10 13:08:32,838::train::INFO] [Train] Iter 17027 | Loss 0.284182 | Grad 0.0449 \n","[2024-04-10 13:08:32,964::train::INFO] [Train] Iter 17028 | Loss 0.290179 | Grad 0.0507 \n","[2024-04-10 13:08:33,086::train::INFO] [Train] Iter 17029 | Loss 0.307429 | Grad 0.0532 \n","[2024-04-10 13:08:33,208::train::INFO] [Train] Iter 17030 | Loss 0.356475 | Grad 0.0469 \n","[2024-04-10 13:08:33,329::train::INFO] [Train] Iter 17031 | Loss 0.334578 | Grad 0.0347 \n","[2024-04-10 13:08:33,451::train::INFO] [Train] Iter 17032 | Loss 0.334150 | Grad 0.0478 \n","[2024-04-10 13:08:33,574::train::INFO] [Train] Iter 17033 | Loss 0.313700 | Grad 0.0450 \n","[2024-04-10 13:08:33,696::train::INFO] [Train] Iter 17034 | Loss 0.338220 | Grad 0.0668 \n","[2024-04-10 13:08:33,817::train::INFO] [Train] Iter 17035 | Loss 0.282638 | Grad 0.0272 \n","[2024-04-10 13:08:33,938::train::INFO] [Train] Iter 17036 | Loss 0.326843 | Grad 0.0316 \n","[2024-04-10 13:08:34,062::train::INFO] [Train] Iter 17037 | Loss 0.313351 | Grad 0.0399 \n","[2024-04-10 13:08:34,183::train::INFO] [Train] Iter 17038 | Loss 0.328603 | Grad 0.0430 \n","[2024-04-10 13:08:34,304::train::INFO] [Train] Iter 17039 | Loss 0.320028 | Grad 0.0550 \n","[2024-04-10 13:08:34,428::train::INFO] [Train] Iter 17040 | Loss 0.356791 | Grad 0.0667 \n","[2024-04-10 13:08:34,549::train::INFO] [Train] Iter 17041 | Loss 0.352237 | Grad 0.0416 \n","[2024-04-10 13:08:34,669::train::INFO] [Train] Iter 17042 | Loss 0.301432 | Grad 0.0500 \n","[2024-04-10 13:08:34,788::train::INFO] [Train] Iter 17043 | Loss 0.313216 | Grad 0.0322 \n","[2024-04-10 13:08:34,905::train::INFO] [Train] Iter 17044 | Loss 0.283293 | Grad 0.0362 \n","[2024-04-10 13:08:35,025::train::INFO] [Train] Iter 17045 | Loss 0.302918 | Grad 0.0749 \n","[2024-04-10 13:08:35,144::train::INFO] [Train] Iter 17046 | Loss 0.330939 | Grad 0.0430 \n","[2024-04-10 13:08:35,263::train::INFO] [Train] Iter 17047 | Loss 0.295470 | Grad 0.0469 \n","[2024-04-10 13:08:35,382::train::INFO] [Train] Iter 17048 | Loss 0.372549 | Grad 0.0727 \n","[2024-04-10 13:08:35,503::train::INFO] [Train] Iter 17049 | Loss 0.291959 | Grad 0.0497 \n","[2024-04-10 13:08:35,622::train::INFO] [Train] Iter 17050 | Loss 0.349758 | Grad 0.0684 \n","[2024-04-10 13:08:35,743::train::INFO] [Train] Iter 17051 | Loss 0.313095 | Grad 0.0598 \n","[2024-04-10 13:08:35,869::train::INFO] [Train] Iter 17052 | Loss 0.314839 | Grad 0.0433 \n","[2024-04-10 13:08:35,996::train::INFO] [Train] Iter 17053 | Loss 0.360614 | Grad 0.0769 \n","[2024-04-10 13:08:36,118::train::INFO] [Train] Iter 17054 | Loss 0.287819 | Grad 0.0505 \n","[2024-04-10 13:08:36,194::train::INFO] [Train] Iter 17055 | Loss 0.329449 | Grad 0.0394 \n","[2024-04-10 13:08:36,314::train::INFO] [Train] Iter 17056 | Loss 0.325507 | Grad 0.0384 \n","[2024-04-10 13:08:36,434::train::INFO] [Train] Iter 17057 | Loss 0.344772 | Grad 0.0450 \n","[2024-04-10 13:08:36,554::train::INFO] [Train] Iter 17058 | Loss 0.340600 | Grad 0.0554 \n","[2024-04-10 13:08:36,674::train::INFO] [Train] Iter 17059 | Loss 0.349311 | Grad 0.0488 \n","[2024-04-10 13:08:36,793::train::INFO] [Train] Iter 17060 | Loss 0.394522 | Grad 0.0809 \n","[2024-04-10 13:08:36,911::train::INFO] [Train] Iter 17061 | Loss 0.348581 | Grad 0.0586 \n","[2024-04-10 13:08:37,030::train::INFO] [Train] Iter 17062 | Loss 0.282452 | Grad 0.0237 \n","[2024-04-10 13:08:37,149::train::INFO] [Train] Iter 17063 | Loss 0.295093 | Grad 0.0333 \n","[2024-04-10 13:08:37,269::train::INFO] [Train] Iter 17064 | Loss 0.339139 | Grad 0.0369 \n","[2024-04-10 13:08:37,388::train::INFO] [Train] Iter 17065 | Loss 0.323986 | Grad 0.0620 \n","[2024-04-10 13:08:37,507::train::INFO] [Train] Iter 17066 | Loss 0.298015 | Grad 0.0298 \n","[2024-04-10 13:08:37,626::train::INFO] [Train] Iter 17067 | Loss 0.330060 | Grad 0.0469 \n","[2024-04-10 13:08:37,746::train::INFO] [Train] Iter 17068 | Loss 0.280559 | Grad 0.0348 \n","[2024-04-10 13:08:37,866::train::INFO] [Train] Iter 17069 | Loss 0.329579 | Grad 0.0391 \n","[2024-04-10 13:08:37,985::train::INFO] [Train] Iter 17070 | Loss 0.336822 | Grad 0.0467 \n","[2024-04-10 13:08:38,103::train::INFO] [Train] Iter 17071 | Loss 0.305059 | Grad 0.0307 \n","[2024-04-10 13:08:38,222::train::INFO] [Train] Iter 17072 | Loss 0.334273 | Grad 0.0558 \n","[2024-04-10 13:08:38,340::train::INFO] [Train] Iter 17073 | Loss 0.366175 | Grad 0.0651 \n","[2024-04-10 13:08:38,460::train::INFO] [Train] Iter 17074 | Loss 0.304390 | Grad 0.0338 \n","[2024-04-10 13:08:38,580::train::INFO] [Train] Iter 17075 | Loss 0.295110 | Grad 0.0356 \n","[2024-04-10 13:08:38,699::train::INFO] [Train] Iter 17076 | Loss 0.297166 | Grad 0.0363 \n","[2024-04-10 13:08:38,817::train::INFO] [Train] Iter 17077 | Loss 0.338878 | Grad 0.0274 \n","[2024-04-10 13:08:38,938::train::INFO] [Train] Iter 17078 | Loss 0.330871 | Grad 0.0300 \n","[2024-04-10 13:08:39,058::train::INFO] [Train] Iter 17079 | Loss 0.339302 | Grad 0.0404 \n","[2024-04-10 13:08:39,179::train::INFO] [Train] Iter 17080 | Loss 0.281236 | Grad 0.0221 \n","[2024-04-10 13:08:39,303::train::INFO] [Train] Iter 17081 | Loss 0.277155 | Grad 0.0271 \n","[2024-04-10 13:08:39,423::train::INFO] [Train] Iter 17082 | Loss 0.299961 | Grad 0.0442 \n","[2024-04-10 13:08:39,542::train::INFO] [Train] Iter 17083 | Loss 0.322156 | Grad 0.0647 \n","[2024-04-10 13:08:39,661::train::INFO] [Train] Iter 17084 | Loss 0.286183 | Grad 0.0565 \n","[2024-04-10 13:08:39,783::train::INFO] [Train] Iter 17085 | Loss 0.285861 | Grad 0.0539 \n","[2024-04-10 13:08:39,904::train::INFO] [Train] Iter 17086 | Loss 0.309291 | Grad 0.0463 \n","[2024-04-10 13:08:40,023::train::INFO] [Train] Iter 17087 | Loss 0.305789 | Grad 0.0688 \n","[2024-04-10 13:08:40,143::train::INFO] [Train] Iter 17088 | Loss 0.316465 | Grad 0.0325 \n","[2024-04-10 13:08:40,264::train::INFO] [Train] Iter 17089 | Loss 0.320526 | Grad 0.0490 \n","[2024-04-10 13:08:40,384::train::INFO] [Train] Iter 17090 | Loss 0.292630 | Grad 0.0449 \n","[2024-04-10 13:08:40,507::train::INFO] [Train] Iter 17091 | Loss 0.328766 | Grad 0.0412 \n","[2024-04-10 13:08:40,626::train::INFO] [Train] Iter 17092 | Loss 0.269971 | Grad 0.0325 \n","[2024-04-10 13:08:40,745::train::INFO] [Train] Iter 17093 | Loss 0.302567 | Grad 0.0447 \n","[2024-04-10 13:08:40,866::train::INFO] [Train] Iter 17094 | Loss 0.308709 | Grad 0.0469 \n","[2024-04-10 13:08:40,992::train::INFO] [Train] Iter 17095 | Loss 0.319160 | Grad 0.0381 \n","[2024-04-10 13:08:41,114::train::INFO] [Train] Iter 17096 | Loss 0.315080 | Grad 0.0399 \n","[2024-04-10 13:08:41,235::train::INFO] [Train] Iter 17097 | Loss 0.292891 | Grad 0.0346 \n","[2024-04-10 13:08:41,355::train::INFO] [Train] Iter 17098 | Loss 0.326074 | Grad 0.0458 \n","[2024-04-10 13:08:41,475::train::INFO] [Train] Iter 17099 | Loss 0.305336 | Grad 0.0441 \n","[2024-04-10 13:08:41,550::train::INFO] [Train] Iter 17100 | Loss 0.307620 | Grad 0.0484 \n","[2024-04-10 13:08:41,669::train::INFO] [Train] Iter 17101 | Loss 0.314317 | Grad 0.0474 \n","[2024-04-10 13:08:41,789::train::INFO] [Train] Iter 17102 | Loss 0.319837 | Grad 0.0476 \n","[2024-04-10 13:08:41,908::train::INFO] [Train] Iter 17103 | Loss 0.315154 | Grad 0.0391 \n","[2024-04-10 13:08:42,026::train::INFO] [Train] Iter 17104 | Loss 0.339349 | Grad 0.0664 \n","[2024-04-10 13:08:42,144::train::INFO] [Train] Iter 17105 | Loss 0.322631 | Grad 0.0350 \n","[2024-04-10 13:08:42,263::train::INFO] [Train] Iter 17106 | Loss 0.301134 | Grad 0.0393 \n","[2024-04-10 13:08:42,382::train::INFO] [Train] Iter 17107 | Loss 0.281713 | Grad 0.0422 \n","[2024-04-10 13:08:42,501::train::INFO] [Train] Iter 17108 | Loss 0.325502 | Grad 0.0529 \n","[2024-04-10 13:08:42,621::train::INFO] [Train] Iter 17109 | Loss 0.339925 | Grad 0.0628 \n","[2024-04-10 13:08:42,740::train::INFO] [Train] Iter 17110 | Loss 0.309817 | Grad 0.0395 \n","[2024-04-10 13:08:42,860::train::INFO] [Train] Iter 17111 | Loss 0.324151 | Grad 0.0532 \n","[2024-04-10 13:08:42,981::train::INFO] [Train] Iter 17112 | Loss 0.306614 | Grad 0.0430 \n","[2024-04-10 13:08:43,101::train::INFO] [Train] Iter 17113 | Loss 0.297654 | Grad 0.0621 \n","[2024-04-10 13:08:43,222::train::INFO] [Train] Iter 17114 | Loss 0.294453 | Grad 0.0395 \n","[2024-04-10 13:08:43,342::train::INFO] [Train] Iter 17115 | Loss 0.366074 | Grad 0.0471 \n","[2024-04-10 13:08:43,463::train::INFO] [Train] Iter 17116 | Loss 0.303874 | Grad 0.0286 \n","[2024-04-10 13:08:43,582::train::INFO] [Train] Iter 17117 | Loss 0.310968 | Grad 0.0318 \n","[2024-04-10 13:08:43,700::train::INFO] [Train] Iter 17118 | Loss 0.297479 | Grad 0.0367 \n","[2024-04-10 13:08:43,820::train::INFO] [Train] Iter 17119 | Loss 0.345996 | Grad 0.0421 \n","[2024-04-10 13:08:43,938::train::INFO] [Train] Iter 17120 | Loss 0.294400 | Grad 0.0320 \n","[2024-04-10 13:08:44,056::train::INFO] [Train] Iter 17121 | Loss 0.320373 | Grad 0.0298 \n","[2024-04-10 13:08:44,174::train::INFO] [Train] Iter 17122 | Loss 0.300990 | Grad 0.0489 \n","[2024-04-10 13:08:44,293::train::INFO] [Train] Iter 17123 | Loss 0.325913 | Grad 0.0337 \n","[2024-04-10 13:08:44,413::train::INFO] [Train] Iter 17124 | Loss 0.290913 | Grad 0.0273 \n","[2024-04-10 13:08:44,532::train::INFO] [Train] Iter 17125 | Loss 0.301793 | Grad 0.0333 \n","[2024-04-10 13:08:44,662::train::INFO] [Train] Iter 17126 | Loss 0.297726 | Grad 0.0387 \n","[2024-04-10 13:08:44,787::train::INFO] [Train] Iter 17127 | Loss 0.281221 | Grad 0.0464 \n","[2024-04-10 13:08:44,907::train::INFO] [Train] Iter 17128 | Loss 0.304807 | Grad 0.0316 \n","[2024-04-10 13:08:45,028::train::INFO] [Train] Iter 17129 | Loss 0.349741 | Grad 0.0454 \n","[2024-04-10 13:08:45,154::train::INFO] [Train] Iter 17130 | Loss 0.349763 | Grad 0.0416 \n","[2024-04-10 13:08:45,275::train::INFO] [Train] Iter 17131 | Loss 0.341215 | Grad 0.0450 \n","[2024-04-10 13:08:45,397::train::INFO] [Train] Iter 17132 | Loss 0.318559 | Grad 0.0394 \n","[2024-04-10 13:08:45,518::train::INFO] [Train] Iter 17133 | Loss 0.319773 | Grad 0.0377 \n","[2024-04-10 13:08:45,639::train::INFO] [Train] Iter 17134 | Loss 0.302929 | Grad 0.0303 \n","[2024-04-10 13:08:45,764::train::INFO] [Train] Iter 17135 | Loss 0.285220 | Grad 0.0333 \n","[2024-04-10 13:08:45,886::train::INFO] [Train] Iter 17136 | Loss 0.321874 | Grad 0.0496 \n","[2024-04-10 13:08:46,009::train::INFO] [Train] Iter 17137 | Loss 0.262542 | Grad 0.0340 \n","[2024-04-10 13:08:46,129::train::INFO] [Train] Iter 17138 | Loss 0.296787 | Grad 0.0716 \n","[2024-04-10 13:08:46,249::train::INFO] [Train] Iter 17139 | Loss 0.304162 | Grad 0.0687 \n","[2024-04-10 13:08:46,370::train::INFO] [Train] Iter 17140 | Loss 0.345023 | Grad 0.0523 \n","[2024-04-10 13:08:46,491::train::INFO] [Train] Iter 17141 | Loss 0.275403 | Grad 0.0275 \n","[2024-04-10 13:08:46,611::train::INFO] [Train] Iter 17142 | Loss 0.305937 | Grad 0.0488 \n","[2024-04-10 13:08:46,731::train::INFO] [Train] Iter 17143 | Loss 0.314986 | Grad 0.0399 \n","[2024-04-10 13:08:46,853::train::INFO] [Train] Iter 17144 | Loss 0.292353 | Grad 0.0351 \n","[2024-04-10 13:08:46,929::train::INFO] [Train] Iter 17145 | Loss 0.285120 | Grad 0.0774 \n","[2024-04-10 13:08:47,051::train::INFO] [Train] Iter 17146 | Loss 0.293179 | Grad 0.0702 \n","[2024-04-10 13:08:47,173::train::INFO] [Train] Iter 17147 | Loss 0.318199 | Grad 0.0509 \n","[2024-04-10 13:08:47,295::train::INFO] [Train] Iter 17148 | Loss 0.293923 | Grad 0.0444 \n","[2024-04-10 13:08:47,417::train::INFO] [Train] Iter 17149 | Loss 0.304541 | Grad 0.0370 \n","[2024-04-10 13:08:47,542::train::INFO] [Train] Iter 17150 | Loss 0.337906 | Grad 0.0487 \n","[2024-04-10 13:08:47,662::train::INFO] [Train] Iter 17151 | Loss 0.331721 | Grad 0.0492 \n","[2024-04-10 13:08:47,782::train::INFO] [Train] Iter 17152 | Loss 0.307143 | Grad 0.0443 \n","[2024-04-10 13:08:47,900::train::INFO] [Train] Iter 17153 | Loss 0.300731 | Grad 0.0529 \n","[2024-04-10 13:08:48,019::train::INFO] [Train] Iter 17154 | Loss 0.329376 | Grad 0.0384 \n","[2024-04-10 13:08:48,138::train::INFO] [Train] Iter 17155 | Loss 0.307446 | Grad 0.0362 \n","[2024-04-10 13:08:48,258::train::INFO] [Train] Iter 17156 | Loss 0.323116 | Grad 0.0508 \n","[2024-04-10 13:08:48,378::train::INFO] [Train] Iter 17157 | Loss 0.321227 | Grad 0.0364 \n","[2024-04-10 13:08:48,500::train::INFO] [Train] Iter 17158 | Loss 0.278714 | Grad 0.0350 \n","[2024-04-10 13:08:48,619::train::INFO] [Train] Iter 17159 | Loss 0.325637 | Grad 0.0473 \n","[2024-04-10 13:08:48,740::train::INFO] [Train] Iter 17160 | Loss 0.339051 | Grad 0.0385 \n","[2024-04-10 13:08:48,861::train::INFO] [Train] Iter 17161 | Loss 0.290920 | Grad 0.0309 \n","[2024-04-10 13:08:48,980::train::INFO] [Train] Iter 17162 | Loss 0.318768 | Grad 0.0381 \n","[2024-04-10 13:08:49,101::train::INFO] [Train] Iter 17163 | Loss 0.307533 | Grad 0.0476 \n","[2024-04-10 13:08:49,221::train::INFO] [Train] Iter 17164 | Loss 0.356254 | Grad 0.0367 \n","[2024-04-10 13:08:49,341::train::INFO] [Train] Iter 17165 | Loss 0.243091 | Grad 0.0358 \n","[2024-04-10 13:08:49,464::train::INFO] [Train] Iter 17166 | Loss 0.308951 | Grad 0.0477 \n","[2024-04-10 13:08:49,583::train::INFO] [Train] Iter 17167 | Loss 0.331876 | Grad 0.0546 \n","[2024-04-10 13:08:49,705::train::INFO] [Train] Iter 17168 | Loss 0.278471 | Grad 0.0287 \n","[2024-04-10 13:08:49,823::train::INFO] [Train] Iter 17169 | Loss 0.300728 | Grad 0.0381 \n","[2024-04-10 13:08:49,943::train::INFO] [Train] Iter 17170 | Loss 0.337477 | Grad 0.0341 \n","[2024-04-10 13:08:50,065::train::INFO] [Train] Iter 17171 | Loss 0.245461 | Grad 0.0256 \n","[2024-04-10 13:08:50,186::train::INFO] [Train] Iter 17172 | Loss 0.309790 | Grad 0.0717 \n","[2024-04-10 13:08:50,305::train::INFO] [Train] Iter 17173 | Loss 0.310426 | Grad 0.0467 \n","[2024-04-10 13:08:50,424::train::INFO] [Train] Iter 17174 | Loss 0.313103 | Grad 0.0459 \n","[2024-04-10 13:08:50,544::train::INFO] [Train] Iter 17175 | Loss 0.326970 | Grad 0.0373 \n","[2024-04-10 13:08:50,665::train::INFO] [Train] Iter 17176 | Loss 0.318090 | Grad 0.0357 \n","[2024-04-10 13:08:50,783::train::INFO] [Train] Iter 17177 | Loss 0.318785 | Grad 0.0776 \n","[2024-04-10 13:08:50,901::train::INFO] [Train] Iter 17178 | Loss 0.321629 | Grad 0.0368 \n","[2024-04-10 13:08:51,019::train::INFO] [Train] Iter 17179 | Loss 0.316345 | Grad 0.0355 \n","[2024-04-10 13:08:51,138::train::INFO] [Train] Iter 17180 | Loss 0.371206 | Grad 0.0696 \n","[2024-04-10 13:08:51,258::train::INFO] [Train] Iter 17181 | Loss 0.324673 | Grad 0.0363 \n","[2024-04-10 13:08:51,378::train::INFO] [Train] Iter 17182 | Loss 0.334892 | Grad 0.0560 \n","[2024-04-10 13:08:51,497::train::INFO] [Train] Iter 17183 | Loss 0.302612 | Grad 0.0599 \n","[2024-04-10 13:08:51,618::train::INFO] [Train] Iter 17184 | Loss 0.321161 | Grad 0.0444 \n","[2024-04-10 13:08:51,739::train::INFO] [Train] Iter 17185 | Loss 0.282537 | Grad 0.0328 \n","[2024-04-10 13:08:51,860::train::INFO] [Train] Iter 17186 | Loss 0.325026 | Grad 0.0372 \n","[2024-04-10 13:08:51,978::train::INFO] [Train] Iter 17187 | Loss 0.275087 | Grad 0.0292 \n","[2024-04-10 13:08:52,099::train::INFO] [Train] Iter 17188 | Loss 0.312924 | Grad 0.0423 \n","[2024-04-10 13:08:52,219::train::INFO] [Train] Iter 17189 | Loss 0.324995 | Grad 0.0577 \n","[2024-04-10 13:08:52,294::train::INFO] [Train] Iter 17190 | Loss 0.296884 | Grad 0.0390 \n","[2024-04-10 13:08:52,414::train::INFO] [Train] Iter 17191 | Loss 0.308108 | Grad 0.0936 \n","[2024-04-10 13:08:52,533::train::INFO] [Train] Iter 17192 | Loss 0.307264 | Grad 0.0504 \n","[2024-04-10 13:08:52,652::train::INFO] [Train] Iter 17193 | Loss 0.333170 | Grad 0.0619 \n","[2024-04-10 13:08:52,770::train::INFO] [Train] Iter 17194 | Loss 0.332078 | Grad 0.0460 \n","[2024-04-10 13:08:52,890::train::INFO] [Train] Iter 17195 | Loss 0.295653 | Grad 0.0272 \n","[2024-04-10 13:08:53,009::train::INFO] [Train] Iter 17196 | Loss 0.362542 | Grad 0.0446 \n","[2024-04-10 13:08:53,127::train::INFO] [Train] Iter 17197 | Loss 0.287387 | Grad 0.0345 \n","[2024-04-10 13:08:53,248::train::INFO] [Train] Iter 17198 | Loss 0.284849 | Grad 0.0404 \n","[2024-04-10 13:08:53,368::train::INFO] [Train] Iter 17199 | Loss 0.367442 | Grad 0.0537 \n","[2024-04-10 13:08:53,491::train::INFO] [Train] Iter 17200 | Loss 0.304948 | Grad 0.0414 \n","[2024-04-10 13:08:53,612::train::INFO] [Train] Iter 17201 | Loss 0.320817 | Grad 0.0384 \n","[2024-04-10 13:08:53,730::train::INFO] [Train] Iter 17202 | Loss 0.318953 | Grad 0.0449 \n","[2024-04-10 13:08:53,848::train::INFO] [Train] Iter 17203 | Loss 0.300434 | Grad 0.0400 \n","[2024-04-10 13:08:53,967::train::INFO] [Train] Iter 17204 | Loss 0.308530 | Grad 0.0363 \n","[2024-04-10 13:08:54,089::train::INFO] [Train] Iter 17205 | Loss 0.311495 | Grad 0.0405 \n","[2024-04-10 13:08:54,210::train::INFO] [Train] Iter 17206 | Loss 0.290322 | Grad 0.0373 \n","[2024-04-10 13:08:54,329::train::INFO] [Train] Iter 17207 | Loss 0.288746 | Grad 0.0318 \n","[2024-04-10 13:08:54,448::train::INFO] [Train] Iter 17208 | Loss 0.292702 | Grad 0.0584 \n","[2024-04-10 13:08:54,567::train::INFO] [Train] Iter 17209 | Loss 0.327869 | Grad 0.0446 \n","[2024-04-10 13:08:54,687::train::INFO] [Train] Iter 17210 | Loss 0.302864 | Grad 0.0381 \n","[2024-04-10 13:08:54,806::train::INFO] [Train] Iter 17211 | Loss 0.337789 | Grad 0.0506 \n","[2024-04-10 13:08:54,924::train::INFO] [Train] Iter 17212 | Loss 0.310714 | Grad 0.0357 \n","[2024-04-10 13:08:55,044::train::INFO] [Train] Iter 17213 | Loss 0.327580 | Grad 0.0280 \n","[2024-04-10 13:08:55,164::train::INFO] [Train] Iter 17214 | Loss 0.378261 | Grad 0.0435 \n","[2024-04-10 13:08:55,284::train::INFO] [Train] Iter 17215 | Loss 0.355300 | Grad 0.0387 \n","[2024-04-10 13:08:55,405::train::INFO] [Train] Iter 17216 | Loss 0.277698 | Grad 0.0306 \n","[2024-04-10 13:08:55,525::train::INFO] [Train] Iter 17217 | Loss 0.295592 | Grad 0.0427 \n","[2024-04-10 13:08:55,645::train::INFO] [Train] Iter 17218 | Loss 0.369012 | Grad 0.0501 \n","[2024-04-10 13:08:55,765::train::INFO] [Train] Iter 17219 | Loss 0.346145 | Grad 0.0567 \n","[2024-04-10 13:08:55,886::train::INFO] [Train] Iter 17220 | Loss 0.332743 | Grad 0.0449 \n","[2024-04-10 13:08:56,005::train::INFO] [Train] Iter 17221 | Loss 0.307644 | Grad 0.0394 \n","[2024-04-10 13:08:56,124::train::INFO] [Train] Iter 17222 | Loss 0.332253 | Grad 0.0366 \n","[2024-04-10 13:08:56,244::train::INFO] [Train] Iter 17223 | Loss 0.290754 | Grad 0.0366 \n","[2024-04-10 13:08:56,365::train::INFO] [Train] Iter 17224 | Loss 0.320275 | Grad 0.0315 \n","[2024-04-10 13:08:56,485::train::INFO] [Train] Iter 17225 | Loss 0.287875 | Grad 0.0289 \n","[2024-04-10 13:08:56,605::train::INFO] [Train] Iter 17226 | Loss 0.288449 | Grad 0.0414 \n","[2024-04-10 13:08:56,724::train::INFO] [Train] Iter 17227 | Loss 0.358395 | Grad 0.0404 \n","[2024-04-10 13:08:56,843::train::INFO] [Train] Iter 17228 | Loss 0.328969 | Grad 0.0330 \n","[2024-04-10 13:08:56,962::train::INFO] [Train] Iter 17229 | Loss 0.330966 | Grad 0.0374 \n","[2024-04-10 13:08:57,080::train::INFO] [Train] Iter 17230 | Loss 0.306015 | Grad 0.0497 \n","[2024-04-10 13:08:57,199::train::INFO] [Train] Iter 17231 | Loss 0.272334 | Grad 0.0380 \n","[2024-04-10 13:08:57,318::train::INFO] [Train] Iter 17232 | Loss 0.308364 | Grad 0.0491 \n","[2024-04-10 13:08:57,437::train::INFO] [Train] Iter 17233 | Loss 0.297711 | Grad 0.0348 \n","[2024-04-10 13:08:57,557::train::INFO] [Train] Iter 17234 | Loss 0.319318 | Grad 0.0418 \n","[2024-04-10 13:08:57,631::train::INFO] [Train] Iter 17235 | Loss 0.319647 | Grad 0.0501 \n","[2024-04-10 13:08:57,750::train::INFO] [Train] Iter 17236 | Loss 0.298161 | Grad 0.0527 \n","[2024-04-10 13:08:57,872::train::INFO] [Train] Iter 17237 | Loss 0.299245 | Grad 0.0518 \n","[2024-04-10 13:08:57,994::train::INFO] [Train] Iter 17238 | Loss 0.350570 | Grad 0.0583 \n","[2024-04-10 13:08:58,121::train::INFO] [Train] Iter 17239 | Loss 0.329018 | Grad 0.0673 \n","[2024-04-10 13:08:58,242::train::INFO] [Train] Iter 17240 | Loss 0.306612 | Grad 0.0513 \n","[2024-04-10 13:08:58,362::train::INFO] [Train] Iter 17241 | Loss 0.311123 | Grad 0.0508 \n","[2024-04-10 13:08:58,486::train::INFO] [Train] Iter 17242 | Loss 0.312112 | Grad 0.0546 \n","[2024-04-10 13:08:58,608::train::INFO] [Train] Iter 17243 | Loss 0.255588 | Grad 0.0451 \n","[2024-04-10 13:08:58,732::train::INFO] [Train] Iter 17244 | Loss 0.320149 | Grad 0.0443 \n","[2024-04-10 13:08:58,856::train::INFO] [Train] Iter 17245 | Loss 0.317730 | Grad 0.0513 \n","[2024-04-10 13:08:58,979::train::INFO] [Train] Iter 17246 | Loss 0.354863 | Grad 0.0630 \n","[2024-04-10 13:08:59,100::train::INFO] [Train] Iter 17247 | Loss 0.330219 | Grad 0.0506 \n","[2024-04-10 13:08:59,221::train::INFO] [Train] Iter 17248 | Loss 0.315926 | Grad 0.0352 \n","[2024-04-10 13:08:59,342::train::INFO] [Train] Iter 17249 | Loss 0.306365 | Grad 0.0391 \n","[2024-04-10 13:08:59,462::train::INFO] [Train] Iter 17250 | Loss 0.324484 | Grad 0.0606 \n","[2024-04-10 13:08:59,582::train::INFO] [Train] Iter 17251 | Loss 0.283373 | Grad 0.0385 \n","[2024-04-10 13:08:59,703::train::INFO] [Train] Iter 17252 | Loss 0.312474 | Grad 0.0355 \n","[2024-04-10 13:08:59,823::train::INFO] [Train] Iter 17253 | Loss 0.334700 | Grad 0.0849 \n","[2024-04-10 13:08:59,943::train::INFO] [Train] Iter 17254 | Loss 0.292880 | Grad 0.0433 \n","[2024-04-10 13:09:00,063::train::INFO] [Train] Iter 17255 | Loss 0.336927 | Grad 0.0464 \n","[2024-04-10 13:09:00,186::train::INFO] [Train] Iter 17256 | Loss 0.342694 | Grad 0.0741 \n","[2024-04-10 13:09:00,307::train::INFO] [Train] Iter 17257 | Loss 0.356278 | Grad 0.0497 \n","[2024-04-10 13:09:00,430::train::INFO] [Train] Iter 17258 | Loss 0.284627 | Grad 0.0408 \n","[2024-04-10 13:09:00,552::train::INFO] [Train] Iter 17259 | Loss 0.322291 | Grad 0.0368 \n","[2024-04-10 13:09:00,675::train::INFO] [Train] Iter 17260 | Loss 0.308704 | Grad 0.0405 \n","[2024-04-10 13:09:00,799::train::INFO] [Train] Iter 17261 | Loss 0.287899 | Grad 0.0379 \n","[2024-04-10 13:09:00,920::train::INFO] [Train] Iter 17262 | Loss 0.286318 | Grad 0.0422 \n","[2024-04-10 13:09:01,039::train::INFO] [Train] Iter 17263 | Loss 0.288901 | Grad 0.0333 \n","[2024-04-10 13:09:01,159::train::INFO] [Train] Iter 17264 | Loss 0.364314 | Grad 0.0529 \n","[2024-04-10 13:09:01,280::train::INFO] [Train] Iter 17265 | Loss 0.346987 | Grad 0.0555 \n","[2024-04-10 13:09:01,401::train::INFO] [Train] Iter 17266 | Loss 0.311738 | Grad 0.0422 \n","[2024-04-10 13:09:01,520::train::INFO] [Train] Iter 17267 | Loss 0.293610 | Grad 0.0321 \n","[2024-04-10 13:09:01,640::train::INFO] [Train] Iter 17268 | Loss 0.325624 | Grad 0.0513 \n","[2024-04-10 13:09:01,761::train::INFO] [Train] Iter 17269 | Loss 0.290545 | Grad 0.0290 \n","[2024-04-10 13:09:01,880::train::INFO] [Train] Iter 17270 | Loss 0.326900 | Grad 0.0607 \n","[2024-04-10 13:09:02,000::train::INFO] [Train] Iter 17271 | Loss 0.306759 | Grad 0.0300 \n","[2024-04-10 13:09:02,119::train::INFO] [Train] Iter 17272 | Loss 0.330764 | Grad 0.0418 \n","[2024-04-10 13:09:02,243::train::INFO] [Train] Iter 17273 | Loss 0.321678 | Grad 0.0342 \n","[2024-04-10 13:09:02,364::train::INFO] [Train] Iter 17274 | Loss 0.318030 | Grad 0.0386 \n","[2024-04-10 13:09:02,485::train::INFO] [Train] Iter 17275 | Loss 0.326260 | Grad 0.0477 \n","[2024-04-10 13:09:02,606::train::INFO] [Train] Iter 17276 | Loss 0.326731 | Grad 0.0473 \n","[2024-04-10 13:09:02,726::train::INFO] [Train] Iter 17277 | Loss 0.349124 | Grad 0.0493 \n","[2024-04-10 13:09:02,846::train::INFO] [Train] Iter 17278 | Loss 0.329464 | Grad 0.0506 \n","[2024-04-10 13:09:02,972::train::INFO] [Train] Iter 17279 | Loss 0.334907 | Grad 0.0522 \n","[2024-04-10 13:09:03,047::train::INFO] [Train] Iter 17280 | Loss 0.333917 | Grad 0.0509 \n","[2024-04-10 13:09:03,165::train::INFO] [Train] Iter 17281 | Loss 0.313613 | Grad 0.0479 \n","[2024-04-10 13:09:03,285::train::INFO] [Train] Iter 17282 | Loss 0.341625 | Grad 0.1071 \n","[2024-04-10 13:09:03,404::train::INFO] [Train] Iter 17283 | Loss 0.291143 | Grad 0.0430 \n","[2024-04-10 13:09:03,524::train::INFO] [Train] Iter 17284 | Loss 0.277207 | Grad 0.0373 \n","[2024-04-10 13:09:03,644::train::INFO] [Train] Iter 17285 | Loss 0.303266 | Grad 0.0347 \n","[2024-04-10 13:09:03,763::train::INFO] [Train] Iter 17286 | Loss 0.316912 | Grad 0.0718 \n","[2024-04-10 13:09:03,881::train::INFO] [Train] Iter 17287 | Loss 0.299714 | Grad 0.0326 \n","[2024-04-10 13:09:04,000::train::INFO] [Train] Iter 17288 | Loss 0.317817 | Grad 0.0533 \n","[2024-04-10 13:09:04,119::train::INFO] [Train] Iter 17289 | Loss 0.341809 | Grad 0.0484 \n","[2024-04-10 13:09:04,237::train::INFO] [Train] Iter 17290 | Loss 0.312516 | Grad 0.0351 \n","[2024-04-10 13:09:04,358::train::INFO] [Train] Iter 17291 | Loss 0.302095 | Grad 0.0431 \n","[2024-04-10 13:09:04,477::train::INFO] [Train] Iter 17292 | Loss 0.282918 | Grad 0.0441 \n","[2024-04-10 13:09:04,596::train::INFO] [Train] Iter 17293 | Loss 0.369707 | Grad 0.0429 \n","[2024-04-10 13:09:04,715::train::INFO] [Train] Iter 17294 | Loss 0.343089 | Grad 0.0424 \n","[2024-04-10 13:09:04,836::train::INFO] [Train] Iter 17295 | Loss 0.334425 | Grad 0.0491 \n","[2024-04-10 13:09:04,959::train::INFO] [Train] Iter 17296 | Loss 0.276441 | Grad 0.0394 \n","[2024-04-10 13:09:05,078::train::INFO] [Train] Iter 17297 | Loss 0.288673 | Grad 0.0435 \n","[2024-04-10 13:09:05,198::train::INFO] [Train] Iter 17298 | Loss 0.288838 | Grad 0.0371 \n","[2024-04-10 13:09:05,318::train::INFO] [Train] Iter 17299 | Loss 0.350214 | Grad 0.0508 \n","[2024-04-10 13:09:05,437::train::INFO] [Train] Iter 17300 | Loss 0.333061 | Grad 0.0444 \n","[2024-04-10 13:09:05,561::train::INFO] [Train] Iter 17301 | Loss 0.279351 | Grad 0.0387 \n","[2024-04-10 13:09:05,679::train::INFO] [Train] Iter 17302 | Loss 0.298034 | Grad 0.0492 \n","[2024-04-10 13:09:05,799::train::INFO] [Train] Iter 17303 | Loss 0.337270 | Grad 0.0370 \n","[2024-04-10 13:09:05,918::train::INFO] [Train] Iter 17304 | Loss 0.360135 | Grad 0.0444 \n","[2024-04-10 13:09:06,041::train::INFO] [Train] Iter 17305 | Loss 0.333459 | Grad 0.0359 \n","[2024-04-10 13:09:06,160::train::INFO] [Train] Iter 17306 | Loss 0.320222 | Grad 0.0394 \n","[2024-04-10 13:09:06,280::train::INFO] [Train] Iter 17307 | Loss 0.332488 | Grad 0.0484 \n","[2024-04-10 13:09:06,400::train::INFO] [Train] Iter 17308 | Loss 0.341324 | Grad 0.0604 \n","[2024-04-10 13:09:06,520::train::INFO] [Train] Iter 17309 | Loss 0.321558 | Grad 0.0501 \n","[2024-04-10 13:09:06,640::train::INFO] [Train] Iter 17310 | Loss 0.314129 | Grad 0.0437 \n","[2024-04-10 13:09:06,761::train::INFO] [Train] Iter 17311 | Loss 0.274063 | Grad 0.0408 \n","[2024-04-10 13:09:06,880::train::INFO] [Train] Iter 17312 | Loss 0.304071 | Grad 0.0323 \n","[2024-04-10 13:09:07,000::train::INFO] [Train] Iter 17313 | Loss 0.321742 | Grad 0.0514 \n","[2024-04-10 13:09:07,121::train::INFO] [Train] Iter 17314 | Loss 0.308433 | Grad 0.0434 \n","[2024-04-10 13:09:07,241::train::INFO] [Train] Iter 17315 | Loss 0.314643 | Grad 0.0403 \n","[2024-04-10 13:09:07,359::train::INFO] [Train] Iter 17316 | Loss 0.327002 | Grad 0.0409 \n","[2024-04-10 13:09:07,483::train::INFO] [Train] Iter 17317 | Loss 0.295412 | Grad 0.0318 \n","[2024-04-10 13:09:07,601::train::INFO] [Train] Iter 17318 | Loss 0.337581 | Grad 0.0999 \n","[2024-04-10 13:09:07,719::train::INFO] [Train] Iter 17319 | Loss 0.342492 | Grad 0.0698 \n","[2024-04-10 13:09:07,838::train::INFO] [Train] Iter 17320 | Loss 0.291891 | Grad 0.0577 \n","[2024-04-10 13:09:07,957::train::INFO] [Train] Iter 17321 | Loss 0.284921 | Grad 0.0563 \n","[2024-04-10 13:09:08,084::train::INFO] [Train] Iter 17322 | Loss 0.304724 | Grad 0.0457 \n","[2024-04-10 13:09:08,202::train::INFO] [Train] Iter 17323 | Loss 0.283154 | Grad 0.0247 \n","[2024-04-10 13:09:08,321::train::INFO] [Train] Iter 17324 | Loss 0.290298 | Grad 0.0373 \n","[2024-04-10 13:09:08,395::train::INFO] [Train] Iter 17325 | Loss 0.332318 | Grad 0.0420 \n","[2024-04-10 13:09:08,513::train::INFO] [Train] Iter 17326 | Loss 0.312777 | Grad 0.0506 \n","[2024-04-10 13:09:08,633::train::INFO] [Train] Iter 17327 | Loss 0.318487 | Grad 0.0445 \n","[2024-04-10 13:09:08,753::train::INFO] [Train] Iter 17328 | Loss 0.360030 | Grad 0.0668 \n","[2024-04-10 13:09:08,871::train::INFO] [Train] Iter 17329 | Loss 0.324680 | Grad 0.0610 \n","[2024-04-10 13:09:08,991::train::INFO] [Train] Iter 17330 | Loss 0.321835 | Grad 0.0409 \n","[2024-04-10 13:09:09,112::train::INFO] [Train] Iter 17331 | Loss 0.316436 | Grad 0.0395 \n","[2024-04-10 13:09:09,231::train::INFO] [Train] Iter 17332 | Loss 0.315183 | Grad 0.0567 \n","[2024-04-10 13:09:09,348::train::INFO] [Train] Iter 17333 | Loss 0.325832 | Grad 0.0517 \n","[2024-04-10 13:09:09,467::train::INFO] [Train] Iter 17334 | Loss 0.277866 | Grad 0.0484 \n","[2024-04-10 13:09:09,586::train::INFO] [Train] Iter 17335 | Loss 0.335808 | Grad 0.0574 \n","[2024-04-10 13:09:09,705::train::INFO] [Train] Iter 17336 | Loss 0.294091 | Grad 0.0273 \n","[2024-04-10 13:09:09,823::train::INFO] [Train] Iter 17337 | Loss 0.339453 | Grad 0.0508 \n","[2024-04-10 13:09:09,942::train::INFO] [Train] Iter 17338 | Loss 0.294939 | Grad 0.0380 \n","[2024-04-10 13:09:10,062::train::INFO] [Train] Iter 17339 | Loss 0.311287 | Grad 0.0554 \n","[2024-04-10 13:09:10,184::train::INFO] [Train] Iter 17340 | Loss 0.348189 | Grad 0.0509 \n","[2024-04-10 13:09:10,303::train::INFO] [Train] Iter 17341 | Loss 0.295849 | Grad 0.0355 \n","[2024-04-10 13:09:10,422::train::INFO] [Train] Iter 17342 | Loss 0.261594 | Grad 0.0272 \n","[2024-04-10 13:09:10,542::train::INFO] [Train] Iter 17343 | Loss 0.266160 | Grad 0.0258 \n","[2024-04-10 13:09:10,664::train::INFO] [Train] Iter 17344 | Loss 0.313238 | Grad 0.0447 \n","[2024-04-10 13:09:10,792::train::INFO] [Train] Iter 17345 | Loss 0.297980 | Grad 0.0538 \n","[2024-04-10 13:09:10,910::train::INFO] [Train] Iter 17346 | Loss 0.310299 | Grad 0.0271 \n","[2024-04-10 13:09:11,033::train::INFO] [Train] Iter 17347 | Loss 0.296899 | Grad 0.0548 \n","[2024-04-10 13:09:11,161::train::INFO] [Train] Iter 17348 | Loss 0.364041 | Grad 0.0472 \n","[2024-04-10 13:09:11,282::train::INFO] [Train] Iter 17349 | Loss 0.316694 | Grad 0.0350 \n","[2024-04-10 13:09:11,406::train::INFO] [Train] Iter 17350 | Loss 0.290110 | Grad 0.0283 \n","[2024-04-10 13:09:11,527::train::INFO] [Train] Iter 17351 | Loss 0.276450 | Grad 0.0281 \n","[2024-04-10 13:09:11,648::train::INFO] [Train] Iter 17352 | Loss 0.318622 | Grad 0.0548 \n","[2024-04-10 13:09:11,769::train::INFO] [Train] Iter 17353 | Loss 0.286356 | Grad 0.0301 \n","[2024-04-10 13:09:11,892::train::INFO] [Train] Iter 17354 | Loss 0.310019 | Grad 0.0421 \n","[2024-04-10 13:09:12,017::train::INFO] [Train] Iter 17355 | Loss 0.315429 | Grad 0.0319 \n","[2024-04-10 13:09:12,142::train::INFO] [Train] Iter 17356 | Loss 0.271256 | Grad 0.0381 \n","[2024-04-10 13:09:12,262::train::INFO] [Train] Iter 17357 | Loss 0.320730 | Grad 0.0592 \n","[2024-04-10 13:09:12,386::train::INFO] [Train] Iter 17358 | Loss 0.333508 | Grad 0.0532 \n","[2024-04-10 13:09:12,506::train::INFO] [Train] Iter 17359 | Loss 0.322608 | Grad 0.0343 \n","[2024-04-10 13:09:12,625::train::INFO] [Train] Iter 17360 | Loss 0.321185 | Grad 0.0341 \n","[2024-04-10 13:09:12,746::train::INFO] [Train] Iter 17361 | Loss 0.304067 | Grad 0.0430 \n","[2024-04-10 13:09:12,867::train::INFO] [Train] Iter 17362 | Loss 0.273438 | Grad 0.0268 \n","[2024-04-10 13:09:12,987::train::INFO] [Train] Iter 17363 | Loss 0.333198 | Grad 0.0335 \n","[2024-04-10 13:09:13,107::train::INFO] [Train] Iter 17364 | Loss 0.344815 | Grad 0.0487 \n","[2024-04-10 13:09:13,228::train::INFO] [Train] Iter 17365 | Loss 0.300721 | Grad 0.0444 \n","[2024-04-10 13:09:13,348::train::INFO] [Train] Iter 17366 | Loss 0.337761 | Grad 0.0564 \n","[2024-04-10 13:09:13,469::train::INFO] [Train] Iter 17367 | Loss 0.332097 | Grad 0.0533 \n","[2024-04-10 13:09:13,590::train::INFO] [Train] Iter 17368 | Loss 0.300399 | Grad 0.0524 \n","[2024-04-10 13:09:13,713::train::INFO] [Train] Iter 17369 | Loss 0.335946 | Grad 0.0559 \n","[2024-04-10 13:09:13,790::train::INFO] [Train] Iter 17370 | Loss 0.319486 | Grad 0.0767 \n","[2024-04-10 13:09:13,913::train::INFO] [Train] Iter 17371 | Loss 0.335124 | Grad 0.0477 \n","[2024-04-10 13:09:14,035::train::INFO] [Train] Iter 17372 | Loss 0.298463 | Grad 0.0518 \n","[2024-04-10 13:09:14,158::train::INFO] [Train] Iter 17373 | Loss 0.352088 | Grad 0.0871 \n","[2024-04-10 13:09:14,278::train::INFO] [Train] Iter 17374 | Loss 0.311358 | Grad 0.0496 \n","[2024-04-10 13:09:14,400::train::INFO] [Train] Iter 17375 | Loss 0.330631 | Grad 0.0582 \n","[2024-04-10 13:09:14,520::train::INFO] [Train] Iter 17376 | Loss 0.315213 | Grad 0.0666 \n","[2024-04-10 13:09:14,639::train::INFO] [Train] Iter 17377 | Loss 0.289389 | Grad 0.0384 \n","[2024-04-10 13:09:14,759::train::INFO] [Train] Iter 17378 | Loss 0.282898 | Grad 0.0389 \n","[2024-04-10 13:09:14,880::train::INFO] [Train] Iter 17379 | Loss 0.298981 | Grad 0.0527 \n","[2024-04-10 13:09:15,006::train::INFO] [Train] Iter 17380 | Loss 0.323543 | Grad 0.0566 \n","[2024-04-10 13:09:15,125::train::INFO] [Train] Iter 17381 | Loss 0.311589 | Grad 0.0503 \n","[2024-04-10 13:09:15,244::train::INFO] [Train] Iter 17382 | Loss 0.301500 | Grad 0.0418 \n","[2024-04-10 13:09:15,365::train::INFO] [Train] Iter 17383 | Loss 0.336595 | Grad 0.0444 \n","[2024-04-10 13:09:15,485::train::INFO] [Train] Iter 17384 | Loss 0.328838 | Grad 0.0387 \n","[2024-04-10 13:09:15,605::train::INFO] [Train] Iter 17385 | Loss 0.301505 | Grad 0.0483 \n","[2024-04-10 13:09:15,724::train::INFO] [Train] Iter 17386 | Loss 0.320550 | Grad 0.0519 \n","[2024-04-10 13:09:15,845::train::INFO] [Train] Iter 17387 | Loss 0.343706 | Grad 0.0522 \n","[2024-04-10 13:09:15,965::train::INFO] [Train] Iter 17388 | Loss 0.324212 | Grad 0.0504 \n","[2024-04-10 13:09:16,085::train::INFO] [Train] Iter 17389 | Loss 0.313253 | Grad 0.0384 \n","[2024-04-10 13:09:16,204::train::INFO] [Train] Iter 17390 | Loss 0.264778 | Grad 0.0322 \n","[2024-04-10 13:09:16,323::train::INFO] [Train] Iter 17391 | Loss 0.360043 | Grad 0.0487 \n","[2024-04-10 13:09:16,442::train::INFO] [Train] Iter 17392 | Loss 0.293782 | Grad 0.0280 \n","[2024-04-10 13:09:16,560::train::INFO] [Train] Iter 17393 | Loss 0.359821 | Grad 0.0379 \n","[2024-04-10 13:09:16,680::train::INFO] [Train] Iter 17394 | Loss 0.342794 | Grad 0.0414 \n","[2024-04-10 13:09:16,799::train::INFO] [Train] Iter 17395 | Loss 0.285385 | Grad 0.0338 \n","[2024-04-10 13:09:16,922::train::INFO] [Train] Iter 17396 | Loss 0.305660 | Grad 0.0367 \n","[2024-04-10 13:09:17,042::train::INFO] [Train] Iter 17397 | Loss 0.313311 | Grad 0.0396 \n","[2024-04-10 13:09:17,162::train::INFO] [Train] Iter 17398 | Loss 0.316389 | Grad 0.0330 \n","[2024-04-10 13:09:17,283::train::INFO] [Train] Iter 17399 | Loss 0.347807 | Grad 0.0472 \n","[2024-04-10 13:09:17,405::train::INFO] [Train] Iter 17400 | Loss 0.243610 | Grad 0.0309 \n","[2024-04-10 13:09:17,525::train::INFO] [Train] Iter 17401 | Loss 0.289576 | Grad 0.0292 \n","[2024-04-10 13:09:17,644::train::INFO] [Train] Iter 17402 | Loss 0.337731 | Grad 0.0477 \n","[2024-04-10 13:09:17,763::train::INFO] [Train] Iter 17403 | Loss 0.297343 | Grad 0.0311 \n","[2024-04-10 13:09:17,885::train::INFO] [Train] Iter 17404 | Loss 0.297226 | Grad 0.0341 \n","[2024-04-10 13:09:18,003::train::INFO] [Train] Iter 17405 | Loss 0.263185 | Grad 0.0264 \n","[2024-04-10 13:09:18,122::train::INFO] [Train] Iter 17406 | Loss 0.316246 | Grad 0.0412 \n","[2024-04-10 13:09:18,240::train::INFO] [Train] Iter 17407 | Loss 0.296532 | Grad 0.0293 \n","[2024-04-10 13:09:18,359::train::INFO] [Train] Iter 17408 | Loss 0.317487 | Grad 0.0366 \n","[2024-04-10 13:09:18,478::train::INFO] [Train] Iter 17409 | Loss 0.287930 | Grad 0.0296 \n","[2024-04-10 13:09:18,598::train::INFO] [Train] Iter 17410 | Loss 0.316727 | Grad 0.0571 \n","[2024-04-10 13:09:18,724::train::INFO] [Train] Iter 17411 | Loss 0.305742 | Grad 0.0439 \n","[2024-04-10 13:09:18,844::train::INFO] [Train] Iter 17412 | Loss 0.322838 | Grad 0.0717 \n","[2024-04-10 13:09:18,963::train::INFO] [Train] Iter 17413 | Loss 0.261045 | Grad 0.0445 \n","[2024-04-10 13:09:19,083::train::INFO] [Train] Iter 17414 | Loss 0.284749 | Grad 0.0322 \n","[2024-04-10 13:09:19,157::train::INFO] [Train] Iter 17415 | Loss 0.314989 | Grad 0.0428 \n","[2024-04-10 13:09:19,276::train::INFO] [Train] Iter 17416 | Loss 0.324385 | Grad 0.0530 \n","[2024-04-10 13:09:19,394::train::INFO] [Train] Iter 17417 | Loss 0.302184 | Grad 0.0472 \n","[2024-04-10 13:09:19,513::train::INFO] [Train] Iter 17418 | Loss 0.303663 | Grad 0.0434 \n","[2024-04-10 13:09:19,632::train::INFO] [Train] Iter 17419 | Loss 0.323344 | Grad 0.0678 \n","[2024-04-10 13:09:19,750::train::INFO] [Train] Iter 17420 | Loss 0.321681 | Grad 0.0703 \n","[2024-04-10 13:09:19,869::train::INFO] [Train] Iter 17421 | Loss 0.375951 | Grad 0.0575 \n","[2024-04-10 13:09:19,989::train::INFO] [Train] Iter 17422 | Loss 0.313555 | Grad 0.0357 \n","[2024-04-10 13:09:20,110::train::INFO] [Train] Iter 17423 | Loss 0.384710 | Grad 0.0503 \n","[2024-04-10 13:09:20,230::train::INFO] [Train] Iter 17424 | Loss 0.348117 | Grad 0.0718 \n","[2024-04-10 13:09:20,349::train::INFO] [Train] Iter 17425 | Loss 0.290440 | Grad 0.0404 \n","[2024-04-10 13:09:20,472::train::INFO] [Train] Iter 17426 | Loss 0.353849 | Grad 0.0614 \n","[2024-04-10 13:09:20,592::train::INFO] [Train] Iter 17427 | Loss 0.326319 | Grad 0.0428 \n","[2024-04-10 13:09:20,712::train::INFO] [Train] Iter 17428 | Loss 0.289351 | Grad 0.0338 \n","[2024-04-10 13:09:20,831::train::INFO] [Train] Iter 17429 | Loss 0.316847 | Grad 0.0426 \n","[2024-04-10 13:09:20,951::train::INFO] [Train] Iter 17430 | Loss 0.328276 | Grad 0.0422 \n","[2024-04-10 13:09:21,069::train::INFO] [Train] Iter 17431 | Loss 0.357086 | Grad 0.0649 \n","[2024-04-10 13:09:21,189::train::INFO] [Train] Iter 17432 | Loss 0.301207 | Grad 0.0350 \n","[2024-04-10 13:09:21,307::train::INFO] [Train] Iter 17433 | Loss 0.331261 | Grad 0.0437 \n","[2024-04-10 13:09:21,427::train::INFO] [Train] Iter 17434 | Loss 0.277610 | Grad 0.0222 \n","[2024-04-10 13:09:21,548::train::INFO] [Train] Iter 17435 | Loss 0.300989 | Grad 0.0376 \n","[2024-04-10 13:09:21,667::train::INFO] [Train] Iter 17436 | Loss 0.341685 | Grad 0.0463 \n","[2024-04-10 13:09:21,788::train::INFO] [Train] Iter 17437 | Loss 0.326106 | Grad 0.0346 \n","[2024-04-10 13:09:21,907::train::INFO] [Train] Iter 17438 | Loss 0.322942 | Grad 0.0369 \n","[2024-04-10 13:09:22,025::train::INFO] [Train] Iter 17439 | Loss 0.285697 | Grad 0.0283 \n","[2024-04-10 13:09:22,146::train::INFO] [Train] Iter 17440 | Loss 0.302756 | Grad 0.0365 \n","[2024-04-10 13:09:22,265::train::INFO] [Train] Iter 17441 | Loss 0.313358 | Grad 0.0342 \n","[2024-04-10 13:09:22,384::train::INFO] [Train] Iter 17442 | Loss 0.310896 | Grad 0.0372 \n","[2024-04-10 13:09:22,502::train::INFO] [Train] Iter 17443 | Loss 0.326670 | Grad 0.0399 \n","[2024-04-10 13:09:22,621::train::INFO] [Train] Iter 17444 | Loss 0.283379 | Grad 0.0319 \n","[2024-04-10 13:09:22,745::train::INFO] [Train] Iter 17445 | Loss 0.325952 | Grad 0.0573 \n","[2024-04-10 13:09:22,864::train::INFO] [Train] Iter 17446 | Loss 0.321073 | Grad 0.0317 \n","[2024-04-10 13:09:22,984::train::INFO] [Train] Iter 17447 | Loss 0.301211 | Grad 0.0499 \n","[2024-04-10 13:09:23,104::train::INFO] [Train] Iter 17448 | Loss 0.342408 | Grad 0.0366 \n","[2024-04-10 13:09:23,223::train::INFO] [Train] Iter 17449 | Loss 0.323357 | Grad 0.0340 \n","[2024-04-10 13:09:23,342::train::INFO] [Train] Iter 17450 | Loss 0.332086 | Grad 0.0317 \n","[2024-04-10 13:09:23,461::train::INFO] [Train] Iter 17451 | Loss 0.276242 | Grad 0.0316 \n","[2024-04-10 13:09:23,582::train::INFO] [Train] Iter 17452 | Loss 0.349293 | Grad 0.0553 \n","[2024-04-10 13:09:23,700::train::INFO] [Train] Iter 17453 | Loss 0.312999 | Grad 0.0404 \n","[2024-04-10 13:09:23,819::train::INFO] [Train] Iter 17454 | Loss 0.323508 | Grad 0.0321 \n","[2024-04-10 13:09:23,937::train::INFO] [Train] Iter 17455 | Loss 0.331823 | Grad 0.0488 \n","[2024-04-10 13:09:24,058::train::INFO] [Train] Iter 17456 | Loss 0.310749 | Grad 0.0454 \n","[2024-04-10 13:09:24,179::train::INFO] [Train] Iter 17457 | Loss 0.310957 | Grad 0.0333 \n","[2024-04-10 13:09:24,302::train::INFO] [Train] Iter 17458 | Loss 0.338743 | Grad 0.0456 \n","[2024-04-10 13:09:24,426::train::INFO] [Train] Iter 17459 | Loss 0.335565 | Grad 0.0654 \n","[2024-04-10 13:09:24,503::train::INFO] [Train] Iter 17460 | Loss 0.338662 | Grad 0.0427 \n","[2024-04-10 13:09:24,626::train::INFO] [Train] Iter 17461 | Loss 0.312424 | Grad 0.0376 \n","[2024-04-10 13:09:24,747::train::INFO] [Train] Iter 17462 | Loss 0.318546 | Grad 0.0460 \n","[2024-04-10 13:09:24,869::train::INFO] [Train] Iter 17463 | Loss 0.306926 | Grad 0.0440 \n","[2024-04-10 13:09:24,992::train::INFO] [Train] Iter 17464 | Loss 0.343817 | Grad 0.0768 \n","[2024-04-10 13:09:25,112::train::INFO] [Train] Iter 17465 | Loss 0.276253 | Grad 0.0378 \n","[2024-04-10 13:09:25,235::train::INFO] [Train] Iter 17466 | Loss 0.339218 | Grad 0.0431 \n","[2024-04-10 13:09:25,355::train::INFO] [Train] Iter 17467 | Loss 0.332061 | Grad 0.0391 \n","[2024-04-10 13:09:25,477::train::INFO] [Train] Iter 17468 | Loss 0.330848 | Grad 0.0469 \n","[2024-04-10 13:09:25,597::train::INFO] [Train] Iter 17469 | Loss 0.330641 | Grad 0.0511 \n","[2024-04-10 13:09:25,717::train::INFO] [Train] Iter 17470 | Loss 0.315352 | Grad 0.0493 \n","[2024-04-10 13:09:25,837::train::INFO] [Train] Iter 17471 | Loss 0.351881 | Grad 0.0611 \n","[2024-04-10 13:09:25,958::train::INFO] [Train] Iter 17472 | Loss 0.327539 | Grad 0.0530 \n","[2024-04-10 13:09:26,078::train::INFO] [Train] Iter 17473 | Loss 0.327785 | Grad 0.0352 \n","[2024-04-10 13:09:26,198::train::INFO] [Train] Iter 17474 | Loss 0.294271 | Grad 0.0292 \n","[2024-04-10 13:09:26,319::train::INFO] [Train] Iter 17475 | Loss 0.279474 | Grad 0.0296 \n","[2024-04-10 13:09:26,439::train::INFO] [Train] Iter 17476 | Loss 0.332015 | Grad 0.0621 \n","[2024-04-10 13:09:26,559::train::INFO] [Train] Iter 17477 | Loss 0.340800 | Grad 0.0509 \n","[2024-04-10 13:09:26,683::train::INFO] [Train] Iter 17478 | Loss 0.308278 | Grad 0.0322 \n","[2024-04-10 13:09:26,804::train::INFO] [Train] Iter 17479 | Loss 0.320114 | Grad 0.0354 \n","[2024-04-10 13:09:26,925::train::INFO] [Train] Iter 17480 | Loss 0.355709 | Grad 0.0409 \n","[2024-04-10 13:09:27,047::train::INFO] [Train] Iter 17481 | Loss 0.292414 | Grad 0.0396 \n","[2024-04-10 13:09:27,170::train::INFO] [Train] Iter 17482 | Loss 0.328983 | Grad 0.0409 \n","[2024-04-10 13:09:27,290::train::INFO] [Train] Iter 17483 | Loss 0.313691 | Grad 0.0274 \n","[2024-04-10 13:09:27,410::train::INFO] [Train] Iter 17484 | Loss 0.301130 | Grad 0.0375 \n","[2024-04-10 13:09:27,530::train::INFO] [Train] Iter 17485 | Loss 0.364563 | Grad 0.0487 \n","[2024-04-10 13:09:27,650::train::INFO] [Train] Iter 17486 | Loss 0.304118 | Grad 0.0406 \n","[2024-04-10 13:09:27,770::train::INFO] [Train] Iter 17487 | Loss 0.294292 | Grad 0.0403 \n","[2024-04-10 13:09:27,891::train::INFO] [Train] Iter 17488 | Loss 0.322624 | Grad 0.0681 \n","[2024-04-10 13:09:28,011::train::INFO] [Train] Iter 17489 | Loss 0.334977 | Grad 0.0427 \n","[2024-04-10 13:09:28,131::train::INFO] [Train] Iter 17490 | Loss 0.317454 | Grad 0.0460 \n","[2024-04-10 13:09:28,251::train::INFO] [Train] Iter 17491 | Loss 0.346859 | Grad 0.0461 \n","[2024-04-10 13:09:28,370::train::INFO] [Train] Iter 17492 | Loss 0.320121 | Grad 0.0453 \n","[2024-04-10 13:09:28,493::train::INFO] [Train] Iter 17493 | Loss 0.287440 | Grad 0.0275 \n","[2024-04-10 13:09:28,612::train::INFO] [Train] Iter 17494 | Loss 0.312950 | Grad 0.0315 \n","[2024-04-10 13:09:28,733::train::INFO] [Train] Iter 17495 | Loss 0.330221 | Grad 0.0321 \n","[2024-04-10 13:09:28,852::train::INFO] [Train] Iter 17496 | Loss 0.305018 | Grad 0.0298 \n","[2024-04-10 13:09:28,973::train::INFO] [Train] Iter 17497 | Loss 0.314196 | Grad 0.0447 \n","[2024-04-10 13:09:29,093::train::INFO] [Train] Iter 17498 | Loss 0.347761 | Grad 0.0556 \n","[2024-04-10 13:09:29,212::train::INFO] [Train] Iter 17499 | Loss 0.298992 | Grad 0.0330 \n","[2024-04-10 13:09:29,330::train::INFO] [Train] Iter 17500 | Loss 0.311169 | Grad 0.0466 \n","[2024-04-10 13:09:29,449::train::INFO] [Train] Iter 17501 | Loss 0.292125 | Grad 0.0376 \n","[2024-04-10 13:09:29,568::train::INFO] [Train] Iter 17502 | Loss 0.307522 | Grad 0.0316 \n","[2024-04-10 13:09:29,688::train::INFO] [Train] Iter 17503 | Loss 0.305565 | Grad 0.0267 \n","[2024-04-10 13:09:29,807::train::INFO] [Train] Iter 17504 | Loss 0.351968 | Grad 0.0525 \n","[2024-04-10 13:09:29,883::train::INFO] [Train] Iter 17505 | Loss 0.312351 | Grad 0.0369 \n","[2024-04-10 13:09:30,002::train::INFO] [Train] Iter 17506 | Loss 0.343108 | Grad 0.0609 \n","[2024-04-10 13:09:30,120::train::INFO] [Train] Iter 17507 | Loss 0.318724 | Grad 0.0335 \n","[2024-04-10 13:09:30,238::train::INFO] [Train] Iter 17508 | Loss 0.323403 | Grad 0.0467 \n","[2024-04-10 13:09:30,357::train::INFO] [Train] Iter 17509 | Loss 0.317780 | Grad 0.0433 \n","[2024-04-10 13:09:30,475::train::INFO] [Train] Iter 17510 | Loss 0.332557 | Grad 0.0513 \n","[2024-04-10 13:09:30,593::train::INFO] [Train] Iter 17511 | Loss 0.361976 | Grad 0.0463 \n","[2024-04-10 13:09:30,711::train::INFO] [Train] Iter 17512 | Loss 0.314391 | Grad 0.0539 \n","[2024-04-10 13:09:30,830::train::INFO] [Train] Iter 17513 | Loss 0.254627 | Grad 0.0276 \n","[2024-04-10 13:09:30,949::train::INFO] [Train] Iter 17514 | Loss 0.311534 | Grad 0.0457 \n","[2024-04-10 13:09:31,068::train::INFO] [Train] Iter 17515 | Loss 0.289893 | Grad 0.0384 \n","[2024-04-10 13:09:31,192::train::INFO] [Train] Iter 17516 | Loss 0.291703 | Grad 0.0269 \n","[2024-04-10 13:09:31,311::train::INFO] [Train] Iter 17517 | Loss 0.306211 | Grad 0.0495 \n","[2024-04-10 13:09:31,432::train::INFO] [Train] Iter 17518 | Loss 0.341712 | Grad 0.0496 \n","[2024-04-10 13:09:31,552::train::INFO] [Train] Iter 17519 | Loss 0.350424 | Grad 0.0822 \n","[2024-04-10 13:09:31,671::train::INFO] [Train] Iter 17520 | Loss 0.294413 | Grad 0.0519 \n","[2024-04-10 13:09:31,792::train::INFO] [Train] Iter 17521 | Loss 0.335601 | Grad 0.0384 \n","[2024-04-10 13:09:31,913::train::INFO] [Train] Iter 17522 | Loss 0.341069 | Grad 0.0559 \n","[2024-04-10 13:09:32,033::train::INFO] [Train] Iter 17523 | Loss 0.334067 | Grad 0.0528 \n","[2024-04-10 13:09:32,152::train::INFO] [Train] Iter 17524 | Loss 0.285856 | Grad 0.0426 \n","[2024-04-10 13:09:32,271::train::INFO] [Train] Iter 17525 | Loss 0.326190 | Grad 0.0555 \n","[2024-04-10 13:09:32,392::train::INFO] [Train] Iter 17526 | Loss 0.273842 | Grad 0.0369 \n","[2024-04-10 13:09:32,512::train::INFO] [Train] Iter 17527 | Loss 0.254395 | Grad 0.0299 \n","[2024-04-10 13:09:32,631::train::INFO] [Train] Iter 17528 | Loss 0.296784 | Grad 0.0496 \n","[2024-04-10 13:09:32,752::train::INFO] [Train] Iter 17529 | Loss 0.347378 | Grad 0.0933 \n","[2024-04-10 13:09:32,873::train::INFO] [Train] Iter 17530 | Loss 0.310921 | Grad 0.0456 \n","[2024-04-10 13:09:32,996::train::INFO] [Train] Iter 17531 | Loss 0.300765 | Grad 0.0341 \n","[2024-04-10 13:09:33,115::train::INFO] [Train] Iter 17532 | Loss 0.338458 | Grad 0.0514 \n","[2024-04-10 13:09:33,235::train::INFO] [Train] Iter 17533 | Loss 0.297576 | Grad 0.0312 \n","[2024-04-10 13:09:33,356::train::INFO] [Train] Iter 17534 | Loss 0.298194 | Grad 0.0474 \n","[2024-04-10 13:09:33,475::train::INFO] [Train] Iter 17535 | Loss 0.284627 | Grad 0.0471 \n","[2024-04-10 13:09:33,595::train::INFO] [Train] Iter 17536 | Loss 0.310196 | Grad 0.0483 \n","[2024-04-10 13:09:33,712::train::INFO] [Train] Iter 17537 | Loss 0.291042 | Grad 0.0275 \n","[2024-04-10 13:09:33,831::train::INFO] [Train] Iter 17538 | Loss 0.305982 | Grad 0.0332 \n","[2024-04-10 13:09:33,950::train::INFO] [Train] Iter 17539 | Loss 0.265295 | Grad 0.0493 \n","[2024-04-10 13:09:34,069::train::INFO] [Train] Iter 17540 | Loss 0.326585 | Grad 0.0426 \n","[2024-04-10 13:09:34,189::train::INFO] [Train] Iter 17541 | Loss 0.303807 | Grad 0.0493 \n","[2024-04-10 13:09:34,308::train::INFO] [Train] Iter 17542 | Loss 0.307320 | Grad 0.0451 \n","[2024-04-10 13:09:34,428::train::INFO] [Train] Iter 17543 | Loss 0.284680 | Grad 0.0579 \n","[2024-04-10 13:09:34,548::train::INFO] [Train] Iter 17544 | Loss 0.325483 | Grad 0.0397 \n","[2024-04-10 13:09:34,669::train::INFO] [Train] Iter 17545 | Loss 0.267677 | Grad 0.0396 \n","[2024-04-10 13:09:34,790::train::INFO] [Train] Iter 17546 | Loss 0.316194 | Grad 0.0499 \n","[2024-04-10 13:09:34,912::train::INFO] [Train] Iter 17547 | Loss 0.336301 | Grad 0.0498 \n","[2024-04-10 13:09:35,032::train::INFO] [Train] Iter 17548 | Loss 0.308035 | Grad 0.0350 \n","[2024-04-10 13:09:35,150::train::INFO] [Train] Iter 17549 | Loss 0.328859 | Grad 0.0349 \n","[2024-04-10 13:09:35,225::train::INFO] [Train] Iter 17550 | Loss 0.299479 | Grad 0.0433 \n","[2024-04-10 13:09:35,343::train::INFO] [Train] Iter 17551 | Loss 0.299315 | Grad 0.0367 \n","[2024-04-10 13:09:35,461::train::INFO] [Train] Iter 17552 | Loss 0.341082 | Grad 0.0364 \n","[2024-04-10 13:09:35,579::train::INFO] [Train] Iter 17553 | Loss 0.314209 | Grad 0.0461 \n","[2024-04-10 13:09:35,696::train::INFO] [Train] Iter 17554 | Loss 0.334069 | Grad 0.0410 \n","[2024-04-10 13:09:35,814::train::INFO] [Train] Iter 17555 | Loss 0.296519 | Grad 0.0564 \n","[2024-04-10 13:09:35,936::train::INFO] [Train] Iter 17556 | Loss 0.304149 | Grad 0.0382 \n","[2024-04-10 13:09:36,054::train::INFO] [Train] Iter 17557 | Loss 0.342258 | Grad 0.0615 \n","[2024-04-10 13:09:36,173::train::INFO] [Train] Iter 17558 | Loss 0.321748 | Grad 0.0581 \n","[2024-04-10 13:09:36,294::train::INFO] [Train] Iter 17559 | Loss 0.309155 | Grad 0.0293 \n","[2024-04-10 13:09:36,414::train::INFO] [Train] Iter 17560 | Loss 0.272047 | Grad 0.0321 \n","[2024-04-10 13:09:36,535::train::INFO] [Train] Iter 17561 | Loss 0.284441 | Grad 0.0418 \n","[2024-04-10 13:09:36,655::train::INFO] [Train] Iter 17562 | Loss 0.315327 | Grad 0.0714 \n","[2024-04-10 13:09:36,775::train::INFO] [Train] Iter 17563 | Loss 0.315108 | Grad 0.0575 \n","[2024-04-10 13:09:36,896::train::INFO] [Train] Iter 17564 | Loss 0.333587 | Grad 0.0488 \n","[2024-04-10 13:09:37,015::train::INFO] [Train] Iter 17565 | Loss 0.311797 | Grad 0.0415 \n","[2024-04-10 13:09:37,135::train::INFO] [Train] Iter 17566 | Loss 0.275516 | Grad 0.0309 \n","[2024-04-10 13:09:37,255::train::INFO] [Train] Iter 17567 | Loss 0.320605 | Grad 0.0548 \n","[2024-04-10 13:09:37,376::train::INFO] [Train] Iter 17568 | Loss 0.313443 | Grad 0.0395 \n","[2024-04-10 13:09:37,505::train::INFO] [Train] Iter 17569 | Loss 0.314208 | Grad 0.0388 \n","[2024-04-10 13:09:37,626::train::INFO] [Train] Iter 17570 | Loss 0.313390 | Grad 0.0441 \n","[2024-04-10 13:09:37,754::train::INFO] [Train] Iter 17571 | Loss 0.324835 | Grad 0.0527 \n","[2024-04-10 13:09:37,878::train::INFO] [Train] Iter 17572 | Loss 0.305309 | Grad 0.0371 \n","[2024-04-10 13:09:38,005::train::INFO] [Train] Iter 17573 | Loss 0.303228 | Grad 0.0434 \n","[2024-04-10 13:09:38,126::train::INFO] [Train] Iter 17574 | Loss 0.292403 | Grad 0.0583 \n","[2024-04-10 13:09:38,247::train::INFO] [Train] Iter 17575 | Loss 0.329374 | Grad 0.0533 \n","[2024-04-10 13:09:38,369::train::INFO] [Train] Iter 17576 | Loss 0.332530 | Grad 0.0511 \n","[2024-04-10 13:09:38,490::train::INFO] [Train] Iter 17577 | Loss 0.309829 | Grad 0.0322 \n","[2024-04-10 13:09:38,612::train::INFO] [Train] Iter 17578 | Loss 0.306305 | Grad 0.0570 \n","[2024-04-10 13:09:38,743::train::INFO] [Train] Iter 17579 | Loss 0.272695 | Grad 0.0337 \n","[2024-04-10 13:09:38,864::train::INFO] [Train] Iter 17580 | Loss 0.304440 | Grad 0.0383 \n","[2024-04-10 13:09:38,985::train::INFO] [Train] Iter 17581 | Loss 0.303385 | Grad 0.0451 \n","[2024-04-10 13:09:39,106::train::INFO] [Train] Iter 17582 | Loss 0.300727 | Grad 0.0347 \n","[2024-04-10 13:09:39,233::train::INFO] [Train] Iter 17583 | Loss 0.278872 | Grad 0.0392 \n","[2024-04-10 13:09:39,354::train::INFO] [Train] Iter 17584 | Loss 0.311180 | Grad 0.0437 \n","[2024-04-10 13:09:39,473::train::INFO] [Train] Iter 17585 | Loss 0.311568 | Grad 0.0351 \n","[2024-04-10 13:09:39,594::train::INFO] [Train] Iter 17586 | Loss 0.313643 | Grad 0.0567 \n","[2024-04-10 13:09:39,716::train::INFO] [Train] Iter 17587 | Loss 0.305733 | Grad 0.0424 \n","[2024-04-10 13:09:39,839::train::INFO] [Train] Iter 17588 | Loss 0.298516 | Grad 0.0522 \n","[2024-04-10 13:09:39,967::train::INFO] [Train] Iter 17589 | Loss 0.301351 | Grad 0.0408 \n","[2024-04-10 13:09:40,089::train::INFO] [Train] Iter 17590 | Loss 0.307490 | Grad 0.0632 \n","[2024-04-10 13:09:40,211::train::INFO] [Train] Iter 17591 | Loss 0.278823 | Grad 0.0363 \n","[2024-04-10 13:09:40,332::train::INFO] [Train] Iter 17592 | Loss 0.256606 | Grad 0.0332 \n","[2024-04-10 13:09:40,451::train::INFO] [Train] Iter 17593 | Loss 0.308060 | Grad 0.0356 \n","[2024-04-10 13:09:40,570::train::INFO] [Train] Iter 17594 | Loss 0.350149 | Grad 0.0514 \n","[2024-04-10 13:09:40,645::train::INFO] [Train] Iter 17595 | Loss 0.332917 | Grad 0.0524 \n","[2024-04-10 13:09:40,765::train::INFO] [Train] Iter 17596 | Loss 0.353668 | Grad 0.0430 \n","[2024-04-10 13:09:40,885::train::INFO] [Train] Iter 17597 | Loss 0.309939 | Grad 0.0264 \n","[2024-04-10 13:09:41,005::train::INFO] [Train] Iter 17598 | Loss 0.334942 | Grad 0.0523 \n","[2024-04-10 13:09:41,125::train::INFO] [Train] Iter 17599 | Loss 0.283045 | Grad 0.0455 \n","[2024-04-10 13:09:41,247::train::INFO] [Train] Iter 17600 | Loss 0.279433 | Grad 0.0299 \n","[2024-04-10 13:09:41,367::train::INFO] [Train] Iter 17601 | Loss 0.324326 | Grad 0.0603 \n","[2024-04-10 13:09:41,486::train::INFO] [Train] Iter 17602 | Loss 0.337013 | Grad 0.0671 \n","[2024-04-10 13:09:41,606::train::INFO] [Train] Iter 17603 | Loss 0.348930 | Grad 0.0569 \n","[2024-04-10 13:09:41,725::train::INFO] [Train] Iter 17604 | Loss 0.281149 | Grad 0.0481 \n","[2024-04-10 13:09:41,843::train::INFO] [Train] Iter 17605 | Loss 0.307609 | Grad 0.0477 \n","[2024-04-10 13:09:41,964::train::INFO] [Train] Iter 17606 | Loss 0.339990 | Grad 0.0533 \n","[2024-04-10 13:09:42,083::train::INFO] [Train] Iter 17607 | Loss 0.330979 | Grad 0.0525 \n","[2024-04-10 13:09:42,202::train::INFO] [Train] Iter 17608 | Loss 0.323104 | Grad 0.0389 \n","[2024-04-10 13:09:42,321::train::INFO] [Train] Iter 17609 | Loss 0.300271 | Grad 0.0634 \n","[2024-04-10 13:09:42,441::train::INFO] [Train] Iter 17610 | Loss 0.320926 | Grad 0.0620 \n","[2024-04-10 13:09:42,563::train::INFO] [Train] Iter 17611 | Loss 0.328270 | Grad 0.0577 \n","[2024-04-10 13:09:42,683::train::INFO] [Train] Iter 17612 | Loss 0.327643 | Grad 0.0508 \n","[2024-04-10 13:09:42,803::train::INFO] [Train] Iter 17613 | Loss 0.259977 | Grad 0.0495 \n","[2024-04-10 13:09:42,922::train::INFO] [Train] Iter 17614 | Loss 0.331557 | Grad 0.0523 \n","[2024-04-10 13:09:43,042::train::INFO] [Train] Iter 17615 | Loss 0.313427 | Grad 0.0543 \n","[2024-04-10 13:09:43,162::train::INFO] [Train] Iter 17616 | Loss 0.324396 | Grad 0.0478 \n","[2024-04-10 13:09:43,283::train::INFO] [Train] Iter 17617 | Loss 0.279950 | Grad 0.0393 \n","[2024-04-10 13:09:43,403::train::INFO] [Train] Iter 17618 | Loss 0.317950 | Grad 0.0500 \n","[2024-04-10 13:09:43,523::train::INFO] [Train] Iter 17619 | Loss 0.298931 | Grad 0.0465 \n","[2024-04-10 13:09:43,642::train::INFO] [Train] Iter 17620 | Loss 0.309865 | Grad 0.0451 \n","[2024-04-10 13:09:43,763::train::INFO] [Train] Iter 17621 | Loss 0.304884 | Grad 0.0331 \n","[2024-04-10 13:09:43,882::train::INFO] [Train] Iter 17622 | Loss 0.349023 | Grad 0.0477 \n","[2024-04-10 13:09:44,001::train::INFO] [Train] Iter 17623 | Loss 0.345124 | Grad 0.0371 \n","[2024-04-10 13:09:44,119::train::INFO] [Train] Iter 17624 | Loss 0.318912 | Grad 0.0373 \n","[2024-04-10 13:09:44,239::train::INFO] [Train] Iter 17625 | Loss 0.278757 | Grad 0.0298 \n","[2024-04-10 13:09:44,357::train::INFO] [Train] Iter 17626 | Loss 0.316589 | Grad 0.0271 \n","[2024-04-10 13:09:44,476::train::INFO] [Train] Iter 17627 | Loss 0.318747 | Grad 0.0282 \n","[2024-04-10 13:09:44,595::train::INFO] [Train] Iter 17628 | Loss 0.346424 | Grad 0.0374 \n","[2024-04-10 13:09:44,714::train::INFO] [Train] Iter 17629 | Loss 0.305545 | Grad 0.0332 \n","[2024-04-10 13:09:44,832::train::INFO] [Train] Iter 17630 | Loss 0.327784 | Grad 0.0437 \n","[2024-04-10 13:09:44,953::train::INFO] [Train] Iter 17631 | Loss 0.331661 | Grad 0.0359 \n","[2024-04-10 13:09:45,072::train::INFO] [Train] Iter 17632 | Loss 0.300520 | Grad 0.0418 \n","[2024-04-10 13:09:45,192::train::INFO] [Train] Iter 17633 | Loss 0.323642 | Grad 0.0475 \n","[2024-04-10 13:09:45,312::train::INFO] [Train] Iter 17634 | Loss 0.324872 | Grad 0.0588 \n","[2024-04-10 13:09:45,433::train::INFO] [Train] Iter 17635 | Loss 0.355178 | Grad 0.0428 \n","[2024-04-10 13:09:45,553::train::INFO] [Train] Iter 17636 | Loss 0.306855 | Grad 0.0266 \n","[2024-04-10 13:09:45,672::train::INFO] [Train] Iter 17637 | Loss 0.341964 | Grad 0.0511 \n","[2024-04-10 13:09:45,794::train::INFO] [Train] Iter 17638 | Loss 0.276882 | Grad 0.0258 \n","[2024-04-10 13:09:45,913::train::INFO] [Train] Iter 17639 | Loss 0.337122 | Grad 0.0554 \n","[2024-04-10 13:09:45,989::train::INFO] [Train] Iter 17640 | Loss 0.349855 | Grad 0.0368 \n","[2024-04-10 13:09:46,110::train::INFO] [Train] Iter 17641 | Loss 0.312283 | Grad 0.0434 \n","[2024-04-10 13:09:46,229::train::INFO] [Train] Iter 17642 | Loss 0.330262 | Grad 0.0653 \n","[2024-04-10 13:09:46,348::train::INFO] [Train] Iter 17643 | Loss 0.315010 | Grad 0.0286 \n","[2024-04-10 13:09:46,467::train::INFO] [Train] Iter 17644 | Loss 0.320078 | Grad 0.0342 \n","[2024-04-10 13:09:46,588::train::INFO] [Train] Iter 17645 | Loss 0.317073 | Grad 0.0551 \n","[2024-04-10 13:09:46,707::train::INFO] [Train] Iter 17646 | Loss 0.322527 | Grad 0.0524 \n","[2024-04-10 13:09:46,826::train::INFO] [Train] Iter 17647 | Loss 0.343851 | Grad 0.0600 \n","[2024-04-10 13:09:46,946::train::INFO] [Train] Iter 17648 | Loss 0.323690 | Grad 0.0678 \n","[2024-04-10 13:09:47,068::train::INFO] [Train] Iter 17649 | Loss 0.325415 | Grad 0.0437 \n","[2024-04-10 13:09:47,189::train::INFO] [Train] Iter 17650 | Loss 0.313797 | Grad 0.0318 \n","[2024-04-10 13:09:47,313::train::INFO] [Train] Iter 17651 | Loss 0.308142 | Grad 0.0240 \n","[2024-04-10 13:09:47,432::train::INFO] [Train] Iter 17652 | Loss 0.299401 | Grad 0.0541 \n","[2024-04-10 13:09:47,551::train::INFO] [Train] Iter 17653 | Loss 0.299138 | Grad 0.0422 \n","[2024-04-10 13:09:47,670::train::INFO] [Train] Iter 17654 | Loss 0.267983 | Grad 0.0632 \n","[2024-04-10 13:09:47,789::train::INFO] [Train] Iter 17655 | Loss 0.320060 | Grad 0.0445 \n","[2024-04-10 13:09:47,908::train::INFO] [Train] Iter 17656 | Loss 0.359515 | Grad 0.0682 \n","[2024-04-10 13:09:48,029::train::INFO] [Train] Iter 17657 | Loss 0.338659 | Grad 0.0437 \n","[2024-04-10 13:09:48,150::train::INFO] [Train] Iter 17658 | Loss 0.339616 | Grad 0.0517 \n","[2024-04-10 13:09:48,269::train::INFO] [Train] Iter 17659 | Loss 0.346587 | Grad 0.0458 \n","[2024-04-10 13:09:48,391::train::INFO] [Train] Iter 17660 | Loss 0.294688 | Grad 0.0463 \n","[2024-04-10 13:09:48,510::train::INFO] [Train] Iter 17661 | Loss 0.287066 | Grad 0.0442 \n","[2024-04-10 13:09:48,630::train::INFO] [Train] Iter 17662 | Loss 0.287966 | Grad 0.0301 \n","[2024-04-10 13:09:48,750::train::INFO] [Train] Iter 17663 | Loss 0.295467 | Grad 0.0359 \n","[2024-04-10 13:09:48,870::train::INFO] [Train] Iter 17664 | Loss 0.316052 | Grad 0.0523 \n","[2024-04-10 13:09:48,990::train::INFO] [Train] Iter 17665 | Loss 0.342706 | Grad 0.0497 \n","[2024-04-10 13:09:49,110::train::INFO] [Train] Iter 17666 | Loss 0.283232 | Grad 0.0470 \n","[2024-04-10 13:09:49,230::train::INFO] [Train] Iter 17667 | Loss 0.283451 | Grad 0.0360 \n","[2024-04-10 13:09:49,350::train::INFO] [Train] Iter 17668 | Loss 0.274674 | Grad 0.0258 \n","[2024-04-10 13:09:49,470::train::INFO] [Train] Iter 17669 | Loss 0.316786 | Grad 0.0424 \n","[2024-04-10 13:09:49,590::train::INFO] [Train] Iter 17670 | Loss 0.316119 | Grad 0.0498 \n","[2024-04-10 13:09:49,710::train::INFO] [Train] Iter 17671 | Loss 0.324496 | Grad 0.0430 \n","[2024-04-10 13:09:49,830::train::INFO] [Train] Iter 17672 | Loss 0.311010 | Grad 0.0391 \n","[2024-04-10 13:09:49,950::train::INFO] [Train] Iter 17673 | Loss 0.327762 | Grad 0.0447 \n","[2024-04-10 13:09:50,070::train::INFO] [Train] Iter 17674 | Loss 0.314644 | Grad 0.0522 \n","[2024-04-10 13:09:50,190::train::INFO] [Train] Iter 17675 | Loss 0.307281 | Grad 0.0484 \n","[2024-04-10 13:09:50,311::train::INFO] [Train] Iter 17676 | Loss 0.288541 | Grad 0.0476 \n","[2024-04-10 13:09:50,433::train::INFO] [Train] Iter 17677 | Loss 0.355831 | Grad 0.0578 \n","[2024-04-10 13:09:50,555::train::INFO] [Train] Iter 17678 | Loss 0.308556 | Grad 0.0588 \n","[2024-04-10 13:09:50,681::train::INFO] [Train] Iter 17679 | Loss 0.343215 | Grad 0.0402 \n","[2024-04-10 13:09:50,812::train::INFO] [Train] Iter 17680 | Loss 0.275178 | Grad 0.0365 \n","[2024-04-10 13:09:50,934::train::INFO] [Train] Iter 17681 | Loss 0.309715 | Grad 0.0900 \n","[2024-04-10 13:09:51,054::train::INFO] [Train] Iter 17682 | Loss 0.316324 | Grad 0.0641 \n","[2024-04-10 13:09:51,174::train::INFO] [Train] Iter 17683 | Loss 0.331121 | Grad 0.0461 \n","[2024-04-10 13:09:51,296::train::INFO] [Train] Iter 17684 | Loss 0.273513 | Grad 0.0345 \n","[2024-04-10 13:09:51,374::train::INFO] [Train] Iter 17685 | Loss 0.320892 | Grad 0.0544 \n","[2024-04-10 13:09:51,495::train::INFO] [Train] Iter 17686 | Loss 0.268318 | Grad 0.0387 \n","[2024-04-10 13:09:51,621::train::INFO] [Train] Iter 17687 | Loss 0.347799 | Grad 0.0481 \n","[2024-04-10 13:09:51,743::train::INFO] [Train] Iter 17688 | Loss 0.318575 | Grad 0.0464 \n","[2024-04-10 13:09:51,864::train::INFO] [Train] Iter 17689 | Loss 0.350017 | Grad 0.0488 \n","[2024-04-10 13:09:51,986::train::INFO] [Train] Iter 17690 | Loss 0.333448 | Grad 0.0480 \n","[2024-04-10 13:09:52,111::train::INFO] [Train] Iter 17691 | Loss 0.324491 | Grad 0.0404 \n","[2024-04-10 13:09:52,234::train::INFO] [Train] Iter 17692 | Loss 0.306912 | Grad 0.0344 \n","[2024-04-10 13:09:52,355::train::INFO] [Train] Iter 17693 | Loss 0.287106 | Grad 0.0577 \n","[2024-04-10 13:09:52,485::train::INFO] [Train] Iter 17694 | Loss 0.305954 | Grad 0.0539 \n","[2024-04-10 13:09:52,606::train::INFO] [Train] Iter 17695 | Loss 0.331162 | Grad 0.0364 \n","[2024-04-10 13:09:52,729::train::INFO] [Train] Iter 17696 | Loss 0.332130 | Grad 0.0504 \n","[2024-04-10 13:09:52,852::train::INFO] [Train] Iter 17697 | Loss 0.356786 | Grad 0.0563 \n","[2024-04-10 13:09:52,974::train::INFO] [Train] Iter 17698 | Loss 0.333388 | Grad 0.0350 \n","[2024-04-10 13:09:53,096::train::INFO] [Train] Iter 17699 | Loss 0.319198 | Grad 0.0601 \n","[2024-04-10 13:09:53,217::train::INFO] [Train] Iter 17700 | Loss 0.319614 | Grad 0.0693 \n","[2024-04-10 13:09:53,342::train::INFO] [Train] Iter 17701 | Loss 0.326598 | Grad 0.0518 \n","[2024-04-10 13:09:53,464::train::INFO] [Train] Iter 17702 | Loss 0.276408 | Grad 0.0352 \n","[2024-04-10 13:09:53,583::train::INFO] [Train] Iter 17703 | Loss 0.331078 | Grad 0.0603 \n","[2024-04-10 13:09:53,701::train::INFO] [Train] Iter 17704 | Loss 0.312298 | Grad 0.0372 \n","[2024-04-10 13:09:53,821::train::INFO] [Train] Iter 17705 | Loss 0.348125 | Grad 0.0469 \n","[2024-04-10 13:09:53,938::train::INFO] [Train] Iter 17706 | Loss 0.301340 | Grad 0.0455 \n","[2024-04-10 13:09:54,056::train::INFO] [Train] Iter 17707 | Loss 0.307369 | Grad 0.0384 \n","[2024-04-10 13:09:54,175::train::INFO] [Train] Iter 17708 | Loss 0.310692 | Grad 0.0510 \n","[2024-04-10 13:09:54,293::train::INFO] [Train] Iter 17709 | Loss 0.295600 | Grad 0.0456 \n","[2024-04-10 13:09:54,413::train::INFO] [Train] Iter 17710 | Loss 0.300467 | Grad 0.0578 \n","[2024-04-10 13:09:54,535::train::INFO] [Train] Iter 17711 | Loss 0.297809 | Grad 0.0486 \n","[2024-04-10 13:09:54,656::train::INFO] [Train] Iter 17712 | Loss 0.285543 | Grad 0.0343 \n","[2024-04-10 13:09:54,775::train::INFO] [Train] Iter 17713 | Loss 0.318158 | Grad 0.0317 \n","[2024-04-10 13:09:54,894::train::INFO] [Train] Iter 17714 | Loss 0.336841 | Grad 0.0490 \n","[2024-04-10 13:09:55,013::train::INFO] [Train] Iter 17715 | Loss 0.342702 | Grad 0.0461 \n","[2024-04-10 13:09:55,133::train::INFO] [Train] Iter 17716 | Loss 0.331337 | Grad 0.0491 \n","[2024-04-10 13:09:55,252::train::INFO] [Train] Iter 17717 | Loss 0.316863 | Grad 0.0581 \n","[2024-04-10 13:09:55,371::train::INFO] [Train] Iter 17718 | Loss 0.323570 | Grad 0.0395 \n","[2024-04-10 13:09:55,489::train::INFO] [Train] Iter 17719 | Loss 0.322464 | Grad 0.0415 \n","[2024-04-10 13:09:55,608::train::INFO] [Train] Iter 17720 | Loss 0.299950 | Grad 0.0356 \n","[2024-04-10 13:09:55,728::train::INFO] [Train] Iter 17721 | Loss 0.309887 | Grad 0.0437 \n","[2024-04-10 13:09:55,849::train::INFO] [Train] Iter 17722 | Loss 0.288684 | Grad 0.0374 \n","[2024-04-10 13:09:55,968::train::INFO] [Train] Iter 17723 | Loss 0.300215 | Grad 0.0621 \n","[2024-04-10 13:09:56,087::train::INFO] [Train] Iter 17724 | Loss 0.319272 | Grad 0.0452 \n","[2024-04-10 13:09:56,207::train::INFO] [Train] Iter 17725 | Loss 0.317198 | Grad 0.0393 \n","[2024-04-10 13:09:56,327::train::INFO] [Train] Iter 17726 | Loss 0.318654 | Grad 0.0497 \n","[2024-04-10 13:09:56,446::train::INFO] [Train] Iter 17727 | Loss 0.322275 | Grad 0.0516 \n","[2024-04-10 13:09:56,565::train::INFO] [Train] Iter 17728 | Loss 0.313966 | Grad 0.0345 \n","[2024-04-10 13:09:56,685::train::INFO] [Train] Iter 17729 | Loss 0.316471 | Grad 0.0406 \n","[2024-04-10 13:09:56,759::train::INFO] [Train] Iter 17730 | Loss 0.310770 | Grad 0.0421 \n","[2024-04-10 13:09:56,877::train::INFO] [Train] Iter 17731 | Loss 0.316937 | Grad 0.0338 \n","[2024-04-10 13:09:57,007::train::INFO] [Train] Iter 17732 | Loss 0.324386 | Grad 0.0374 \n","[2024-04-10 13:09:57,126::train::INFO] [Train] Iter 17733 | Loss 0.279391 | Grad 0.0468 \n","[2024-04-10 13:09:57,245::train::INFO] [Train] Iter 17734 | Loss 0.328965 | Grad 0.0459 \n","[2024-04-10 13:09:57,362::train::INFO] [Train] Iter 17735 | Loss 0.327619 | Grad 0.0378 \n","[2024-04-10 13:09:57,481::train::INFO] [Train] Iter 17736 | Loss 0.326638 | Grad 0.0503 \n","[2024-04-10 13:09:57,600::train::INFO] [Train] Iter 17737 | Loss 0.320440 | Grad 0.0409 \n","[2024-04-10 13:09:57,720::train::INFO] [Train] Iter 17738 | Loss 0.296525 | Grad 0.0495 \n","[2024-04-10 13:09:57,839::train::INFO] [Train] Iter 17739 | Loss 0.271912 | Grad 0.0403 \n","[2024-04-10 13:09:57,959::train::INFO] [Train] Iter 17740 | Loss 0.321740 | Grad 0.0394 \n","[2024-04-10 13:09:58,078::train::INFO] [Train] Iter 17741 | Loss 0.298229 | Grad 0.0395 \n","[2024-04-10 13:09:58,198::train::INFO] [Train] Iter 17742 | Loss 0.296186 | Grad 0.0464 \n","[2024-04-10 13:09:58,317::train::INFO] [Train] Iter 17743 | Loss 0.282369 | Grad 0.0351 \n","[2024-04-10 13:09:58,437::train::INFO] [Train] Iter 17744 | Loss 0.342979 | Grad 0.0559 \n","[2024-04-10 13:09:58,557::train::INFO] [Train] Iter 17745 | Loss 0.276850 | Grad 0.0439 \n","[2024-04-10 13:09:58,676::train::INFO] [Train] Iter 17746 | Loss 0.319659 | Grad 0.0577 \n","[2024-04-10 13:09:58,795::train::INFO] [Train] Iter 17747 | Loss 0.289064 | Grad 0.0678 \n","[2024-04-10 13:09:58,913::train::INFO] [Train] Iter 17748 | Loss 0.311536 | Grad 0.0745 \n","[2024-04-10 13:09:59,033::train::INFO] [Train] Iter 17749 | Loss 0.270140 | Grad 0.0435 \n","[2024-04-10 13:09:59,152::train::INFO] [Train] Iter 17750 | Loss 0.310181 | Grad 0.0336 \n","[2024-04-10 13:09:59,273::train::INFO] [Train] Iter 17751 | Loss 0.305596 | Grad 0.0584 \n","[2024-04-10 13:09:59,392::train::INFO] [Train] Iter 17752 | Loss 0.327383 | Grad 0.0573 \n","[2024-04-10 13:09:59,511::train::INFO] [Train] Iter 17753 | Loss 0.319315 | Grad 0.0513 \n","[2024-04-10 13:09:59,632::train::INFO] [Train] Iter 17754 | Loss 0.295222 | Grad 0.0457 \n","[2024-04-10 13:09:59,752::train::INFO] [Train] Iter 17755 | Loss 0.316558 | Grad 0.0468 \n","[2024-04-10 13:09:59,877::train::INFO] [Train] Iter 17756 | Loss 0.311620 | Grad 0.0305 \n","[2024-04-10 13:09:59,998::train::INFO] [Train] Iter 17757 | Loss 0.347320 | Grad 0.0403 \n","[2024-04-10 13:10:00,117::train::INFO] [Train] Iter 17758 | Loss 0.300991 | Grad 0.0316 \n","[2024-04-10 13:10:00,239::train::INFO] [Train] Iter 17759 | Loss 0.308017 | Grad 0.0416 \n","[2024-04-10 13:10:00,358::train::INFO] [Train] Iter 17760 | Loss 0.305000 | Grad 0.0258 \n","[2024-04-10 13:10:00,480::train::INFO] [Train] Iter 17761 | Loss 0.307450 | Grad 0.0388 \n","[2024-04-10 13:10:00,599::train::INFO] [Train] Iter 17762 | Loss 0.368077 | Grad 0.0543 \n","[2024-04-10 13:10:00,719::train::INFO] [Train] Iter 17763 | Loss 0.337505 | Grad 0.0348 \n","[2024-04-10 13:10:00,839::train::INFO] [Train] Iter 17764 | Loss 0.325550 | Grad 0.0374 \n","[2024-04-10 13:10:00,958::train::INFO] [Train] Iter 17765 | Loss 0.313246 | Grad 0.0357 \n","[2024-04-10 13:10:01,080::train::INFO] [Train] Iter 17766 | Loss 0.316468 | Grad 0.0484 \n","[2024-04-10 13:10:01,199::train::INFO] [Train] Iter 17767 | Loss 0.319832 | Grad 0.0392 \n","[2024-04-10 13:10:01,318::train::INFO] [Train] Iter 17768 | Loss 0.316386 | Grad 0.0363 \n","[2024-04-10 13:10:01,437::train::INFO] [Train] Iter 17769 | Loss 0.337300 | Grad 0.0524 \n","[2024-04-10 13:10:01,555::train::INFO] [Train] Iter 17770 | Loss 0.306179 | Grad 0.0465 \n","[2024-04-10 13:10:01,675::train::INFO] [Train] Iter 17771 | Loss 0.301570 | Grad 0.0467 \n","[2024-04-10 13:10:01,793::train::INFO] [Train] Iter 17772 | Loss 0.329442 | Grad 0.0411 \n","[2024-04-10 13:10:01,911::train::INFO] [Train] Iter 17773 | Loss 0.334937 | Grad 0.0400 \n","[2024-04-10 13:10:02,030::train::INFO] [Train] Iter 17774 | Loss 0.255410 | Grad 0.0177 \n","[2024-04-10 13:10:02,105::train::INFO] [Train] Iter 17775 | Loss 0.279556 | Grad 0.0346 \n","[2024-04-10 13:10:02,224::train::INFO] [Train] Iter 17776 | Loss 0.341773 | Grad 0.0318 \n","[2024-04-10 13:10:02,343::train::INFO] [Train] Iter 17777 | Loss 0.334962 | Grad 0.0285 \n","[2024-04-10 13:10:02,464::train::INFO] [Train] Iter 17778 | Loss 0.321486 | Grad 0.0384 \n","[2024-04-10 13:10:02,584::train::INFO] [Train] Iter 17779 | Loss 0.294648 | Grad 0.0242 \n","[2024-04-10 13:10:02,707::train::INFO] [Train] Iter 17780 | Loss 0.303598 | Grad 0.0401 \n","[2024-04-10 13:10:02,826::train::INFO] [Train] Iter 17781 | Loss 0.305297 | Grad 0.0364 \n","[2024-04-10 13:10:02,947::train::INFO] [Train] Iter 17782 | Loss 0.321975 | Grad 0.0305 \n","[2024-04-10 13:10:03,067::train::INFO] [Train] Iter 17783 | Loss 0.283529 | Grad 0.0383 \n","[2024-04-10 13:10:03,187::train::INFO] [Train] Iter 17784 | Loss 0.300128 | Grad 0.0383 \n","[2024-04-10 13:10:03,306::train::INFO] [Train] Iter 17785 | Loss 0.305877 | Grad 0.0298 \n","[2024-04-10 13:10:03,427::train::INFO] [Train] Iter 17786 | Loss 0.301124 | Grad 0.0306 \n","[2024-04-10 13:10:03,547::train::INFO] [Train] Iter 17787 | Loss 0.312879 | Grad 0.0586 \n","[2024-04-10 13:10:03,669::train::INFO] [Train] Iter 17788 | Loss 0.315582 | Grad 0.0310 \n","[2024-04-10 13:10:03,791::train::INFO] [Train] Iter 17789 | Loss 0.300125 | Grad 0.0388 \n","[2024-04-10 13:10:03,913::train::INFO] [Train] Iter 17790 | Loss 0.329324 | Grad 0.0475 \n","[2024-04-10 13:10:04,033::train::INFO] [Train] Iter 17791 | Loss 0.288961 | Grad 0.0377 \n","[2024-04-10 13:10:04,153::train::INFO] [Train] Iter 17792 | Loss 0.344774 | Grad 0.0870 \n","[2024-04-10 13:10:04,275::train::INFO] [Train] Iter 17793 | Loss 0.327704 | Grad 0.0591 \n","[2024-04-10 13:10:04,397::train::INFO] [Train] Iter 17794 | Loss 0.327215 | Grad 0.0645 \n","[2024-04-10 13:10:04,519::train::INFO] [Train] Iter 17795 | Loss 0.259409 | Grad 0.0236 \n","[2024-04-10 13:10:04,640::train::INFO] [Train] Iter 17796 | Loss 0.272947 | Grad 0.0404 \n","[2024-04-10 13:10:04,764::train::INFO] [Train] Iter 17797 | Loss 0.325944 | Grad 0.0372 \n","[2024-04-10 13:10:04,886::train::INFO] [Train] Iter 17798 | Loss 0.334590 | Grad 0.0540 \n","[2024-04-10 13:10:05,010::train::INFO] [Train] Iter 17799 | Loss 0.362560 | Grad 0.0639 \n","[2024-04-10 13:10:05,131::train::INFO] [Train] Iter 17800 | Loss 0.318220 | Grad 0.0724 \n","[2024-04-10 13:10:05,252::train::INFO] [Train] Iter 17801 | Loss 0.326822 | Grad 0.0487 \n","[2024-04-10 13:10:05,372::train::INFO] [Train] Iter 17802 | Loss 0.297742 | Grad 0.0404 \n","[2024-04-10 13:10:05,492::train::INFO] [Train] Iter 17803 | Loss 0.324903 | Grad 0.0718 \n","[2024-04-10 13:10:05,618::train::INFO] [Train] Iter 17804 | Loss 0.302255 | Grad 0.0379 \n","[2024-04-10 13:10:05,738::train::INFO] [Train] Iter 17805 | Loss 0.316754 | Grad 0.0377 \n","[2024-04-10 13:10:05,857::train::INFO] [Train] Iter 17806 | Loss 0.320922 | Grad 0.0534 \n","[2024-04-10 13:10:05,978::train::INFO] [Train] Iter 17807 | Loss 0.279146 | Grad 0.0381 \n","[2024-04-10 13:10:06,097::train::INFO] [Train] Iter 17808 | Loss 0.309558 | Grad 0.0486 \n","[2024-04-10 13:10:06,217::train::INFO] [Train] Iter 17809 | Loss 0.326867 | Grad 0.0436 \n","[2024-04-10 13:10:06,338::train::INFO] [Train] Iter 17810 | Loss 0.338500 | Grad 0.0360 \n","[2024-04-10 13:10:06,459::train::INFO] [Train] Iter 17811 | Loss 0.290329 | Grad 0.0417 \n","[2024-04-10 13:10:06,583::train::INFO] [Train] Iter 17812 | Loss 0.326546 | Grad 0.0445 \n","[2024-04-10 13:10:06,704::train::INFO] [Train] Iter 17813 | Loss 0.308231 | Grad 0.0405 \n","[2024-04-10 13:10:06,827::train::INFO] [Train] Iter 17814 | Loss 0.327189 | Grad 0.0669 \n","[2024-04-10 13:10:06,945::train::INFO] [Train] Iter 17815 | Loss 0.330313 | Grad 0.0455 \n","[2024-04-10 13:10:07,064::train::INFO] [Train] Iter 17816 | Loss 0.299566 | Grad 0.0363 \n","[2024-04-10 13:10:07,183::train::INFO] [Train] Iter 17817 | Loss 0.385661 | Grad 0.0621 \n","[2024-04-10 13:10:07,301::train::INFO] [Train] Iter 17818 | Loss 0.296020 | Grad 0.0417 \n","[2024-04-10 13:10:07,425::train::INFO] [Train] Iter 17819 | Loss 0.269130 | Grad 0.0474 \n","[2024-04-10 13:10:07,499::train::INFO] [Train] Iter 17820 | Loss 0.332578 | Grad 0.0421 \n","[2024-04-10 13:10:07,617::train::INFO] [Train] Iter 17821 | Loss 0.302670 | Grad 0.0385 \n","[2024-04-10 13:10:07,737::train::INFO] [Train] Iter 17822 | Loss 0.317509 | Grad 0.0325 \n","[2024-04-10 13:10:07,857::train::INFO] [Train] Iter 17823 | Loss 0.319260 | Grad 0.0550 \n","[2024-04-10 13:10:07,975::train::INFO] [Train] Iter 17824 | Loss 0.283270 | Grad 0.0371 \n","[2024-04-10 13:10:08,094::train::INFO] [Train] Iter 17825 | Loss 0.307005 | Grad 0.0341 \n","[2024-04-10 13:10:08,213::train::INFO] [Train] Iter 17826 | Loss 0.280660 | Grad 0.0395 \n","[2024-04-10 13:10:08,331::train::INFO] [Train] Iter 17827 | Loss 0.310301 | Grad 0.0509 \n","[2024-04-10 13:10:08,449::train::INFO] [Train] Iter 17828 | Loss 0.297963 | Grad 0.0442 \n","[2024-04-10 13:10:08,568::train::INFO] [Train] Iter 17829 | Loss 0.312345 | Grad 0.0347 \n","[2024-04-10 13:10:08,688::train::INFO] [Train] Iter 17830 | Loss 0.331209 | Grad 0.0493 \n","[2024-04-10 13:10:08,808::train::INFO] [Train] Iter 17831 | Loss 0.318210 | Grad 0.0582 \n","[2024-04-10 13:10:08,927::train::INFO] [Train] Iter 17832 | Loss 0.293530 | Grad 0.0577 \n","[2024-04-10 13:10:09,049::train::INFO] [Train] Iter 17833 | Loss 0.304257 | Grad 0.0465 \n","[2024-04-10 13:10:09,169::train::INFO] [Train] Iter 17834 | Loss 0.327407 | Grad 0.0652 \n","[2024-04-10 13:10:09,291::train::INFO] [Train] Iter 17835 | Loss 0.334808 | Grad 0.0713 \n","[2024-04-10 13:10:09,409::train::INFO] [Train] Iter 17836 | Loss 0.305660 | Grad 0.0548 \n","[2024-04-10 13:10:09,528::train::INFO] [Train] Iter 17837 | Loss 0.298271 | Grad 0.0594 \n","[2024-04-10 13:10:09,649::train::INFO] [Train] Iter 17838 | Loss 0.355703 | Grad 0.0799 \n","[2024-04-10 13:10:09,768::train::INFO] [Train] Iter 17839 | Loss 0.329897 | Grad 0.0770 \n","[2024-04-10 13:10:09,887::train::INFO] [Train] Iter 17840 | Loss 0.305649 | Grad 0.0426 \n","[2024-04-10 13:10:10,005::train::INFO] [Train] Iter 17841 | Loss 0.268860 | Grad 0.0315 \n","[2024-04-10 13:10:10,125::train::INFO] [Train] Iter 17842 | Loss 0.318149 | Grad 0.0866 \n","[2024-04-10 13:10:10,245::train::INFO] [Train] Iter 17843 | Loss 0.336899 | Grad 0.0584 \n","[2024-04-10 13:10:10,363::train::INFO] [Train] Iter 17844 | Loss 0.316035 | Grad 0.0573 \n","[2024-04-10 13:10:10,482::train::INFO] [Train] Iter 17845 | Loss 0.292597 | Grad 0.0491 \n","[2024-04-10 13:10:10,601::train::INFO] [Train] Iter 17846 | Loss 0.338211 | Grad 0.0623 \n","[2024-04-10 13:10:10,720::train::INFO] [Train] Iter 17847 | Loss 0.306078 | Grad 0.0372 \n","[2024-04-10 13:10:10,842::train::INFO] [Train] Iter 17848 | Loss 0.314072 | Grad 0.0409 \n","[2024-04-10 13:10:10,962::train::INFO] [Train] Iter 17849 | Loss 0.284706 | Grad 0.0354 \n","[2024-04-10 13:10:11,087::train::INFO] [Train] Iter 17850 | Loss 0.293634 | Grad 0.0468 \n","[2024-04-10 13:10:11,205::train::INFO] [Train] Iter 17851 | Loss 0.310288 | Grad 0.0421 \n","[2024-04-10 13:10:11,325::train::INFO] [Train] Iter 17852 | Loss 0.354403 | Grad 0.0547 \n","[2024-04-10 13:10:11,445::train::INFO] [Train] Iter 17853 | Loss 0.317415 | Grad 0.0428 \n","[2024-04-10 13:10:11,564::train::INFO] [Train] Iter 17854 | Loss 0.321461 | Grad 0.0404 \n","[2024-04-10 13:10:11,684::train::INFO] [Train] Iter 17855 | Loss 0.346646 | Grad 0.0487 \n","[2024-04-10 13:10:11,805::train::INFO] [Train] Iter 17856 | Loss 0.348927 | Grad 0.0448 \n","[2024-04-10 13:10:11,924::train::INFO] [Train] Iter 17857 | Loss 0.309520 | Grad 0.0543 \n","[2024-04-10 13:10:12,045::train::INFO] [Train] Iter 17858 | Loss 0.308644 | Grad 0.0418 \n","[2024-04-10 13:10:12,165::train::INFO] [Train] Iter 17859 | Loss 0.301917 | Grad 0.0409 \n","[2024-04-10 13:10:12,284::train::INFO] [Train] Iter 17860 | Loss 0.310721 | Grad 0.0587 \n","[2024-04-10 13:10:12,403::train::INFO] [Train] Iter 17861 | Loss 0.319349 | Grad 0.0654 \n","[2024-04-10 13:10:12,521::train::INFO] [Train] Iter 17862 | Loss 0.303613 | Grad 0.0643 \n","[2024-04-10 13:10:12,642::train::INFO] [Train] Iter 17863 | Loss 0.305825 | Grad 0.0322 \n","[2024-04-10 13:10:12,763::train::INFO] [Train] Iter 17864 | Loss 0.345495 | Grad 0.0446 \n","[2024-04-10 13:10:12,837::train::INFO] [Train] Iter 17865 | Loss 0.325576 | Grad 0.0601 \n","[2024-04-10 13:10:12,955::train::INFO] [Train] Iter 17866 | Loss 0.373904 | Grad 0.0663 \n","[2024-04-10 13:10:13,075::train::INFO] [Train] Iter 17867 | Loss 0.325490 | Grad 0.0415 \n","[2024-04-10 13:10:13,194::train::INFO] [Train] Iter 17868 | Loss 0.327058 | Grad 0.0481 \n","[2024-04-10 13:10:13,314::train::INFO] [Train] Iter 17869 | Loss 0.317385 | Grad 0.0462 \n","[2024-04-10 13:10:13,433::train::INFO] [Train] Iter 17870 | Loss 0.305120 | Grad 0.0499 \n","[2024-04-10 13:10:13,552::train::INFO] [Train] Iter 17871 | Loss 0.313497 | Grad 0.0538 \n","[2024-04-10 13:10:13,670::train::INFO] [Train] Iter 17872 | Loss 0.322092 | Grad 0.0578 \n","[2024-04-10 13:10:13,788::train::INFO] [Train] Iter 17873 | Loss 0.303683 | Grad 0.0546 \n","[2024-04-10 13:10:13,907::train::INFO] [Train] Iter 17874 | Loss 0.317814 | Grad 0.0376 \n","[2024-04-10 13:10:14,027::train::INFO] [Train] Iter 17875 | Loss 0.282823 | Grad 0.0412 \n","[2024-04-10 13:10:14,147::train::INFO] [Train] Iter 17876 | Loss 0.289583 | Grad 0.0314 \n","[2024-04-10 13:10:14,267::train::INFO] [Train] Iter 17877 | Loss 0.268700 | Grad 0.0291 \n","[2024-04-10 13:10:14,386::train::INFO] [Train] Iter 17878 | Loss 0.305027 | Grad 0.0309 \n","[2024-04-10 13:10:14,506::train::INFO] [Train] Iter 17879 | Loss 0.323050 | Grad 0.0465 \n","[2024-04-10 13:10:14,625::train::INFO] [Train] Iter 17880 | Loss 0.326121 | Grad 0.0431 \n","[2024-04-10 13:10:14,746::train::INFO] [Train] Iter 17881 | Loss 0.318609 | Grad 0.0398 \n","[2024-04-10 13:10:14,866::train::INFO] [Train] Iter 17882 | Loss 0.285049 | Grad 0.0329 \n","[2024-04-10 13:10:14,992::train::INFO] [Train] Iter 17883 | Loss 0.310689 | Grad 0.0475 \n","[2024-04-10 13:10:15,116::train::INFO] [Train] Iter 17884 | Loss 0.322170 | Grad 0.0468 \n","[2024-04-10 13:10:15,234::train::INFO] [Train] Iter 17885 | Loss 0.325633 | Grad 0.0467 \n","[2024-04-10 13:10:15,354::train::INFO] [Train] Iter 17886 | Loss 0.363952 | Grad 0.0460 \n","[2024-04-10 13:10:15,472::train::INFO] [Train] Iter 17887 | Loss 0.262167 | Grad 0.0262 \n","[2024-04-10 13:10:15,593::train::INFO] [Train] Iter 17888 | Loss 0.314022 | Grad 0.0440 \n","[2024-04-10 13:10:15,712::train::INFO] [Train] Iter 17889 | Loss 0.330686 | Grad 0.0508 \n","[2024-04-10 13:10:15,833::train::INFO] [Train] Iter 17890 | Loss 0.316964 | Grad 0.0380 \n","[2024-04-10 13:10:15,952::train::INFO] [Train] Iter 17891 | Loss 0.332368 | Grad 0.0496 \n","[2024-04-10 13:10:16,071::train::INFO] [Train] Iter 17892 | Loss 0.308884 | Grad 0.0711 \n","[2024-04-10 13:10:16,193::train::INFO] [Train] Iter 17893 | Loss 0.262285 | Grad 0.0284 \n","[2024-04-10 13:10:16,312::train::INFO] [Train] Iter 17894 | Loss 0.315221 | Grad 0.0344 \n","[2024-04-10 13:10:16,432::train::INFO] [Train] Iter 17895 | Loss 0.303224 | Grad 0.0354 \n","[2024-04-10 13:10:16,551::train::INFO] [Train] Iter 17896 | Loss 0.233175 | Grad 0.0296 \n","[2024-04-10 13:10:16,671::train::INFO] [Train] Iter 17897 | Loss 0.335632 | Grad 0.0859 \n","[2024-04-10 13:10:16,792::train::INFO] [Train] Iter 17898 | Loss 0.321606 | Grad 0.0394 \n","[2024-04-10 13:10:16,912::train::INFO] [Train] Iter 17899 | Loss 0.337634 | Grad 0.0404 \n","[2024-04-10 13:10:17,033::train::INFO] [Train] Iter 17900 | Loss 0.325524 | Grad 0.0371 \n","[2024-04-10 13:10:17,162::train::INFO] [Train] Iter 17901 | Loss 0.324679 | Grad 0.0584 \n","[2024-04-10 13:10:17,285::train::INFO] [Train] Iter 17902 | Loss 0.326861 | Grad 0.0340 \n","[2024-04-10 13:10:17,405::train::INFO] [Train] Iter 17903 | Loss 0.279275 | Grad 0.0568 \n","[2024-04-10 13:10:17,526::train::INFO] [Train] Iter 17904 | Loss 0.317722 | Grad 0.0258 \n","[2024-04-10 13:10:17,646::train::INFO] [Train] Iter 17905 | Loss 0.360378 | Grad 0.0339 \n","[2024-04-10 13:10:17,765::train::INFO] [Train] Iter 17906 | Loss 0.301183 | Grad 0.0366 \n","[2024-04-10 13:10:17,885::train::INFO] [Train] Iter 17907 | Loss 0.313239 | Grad 0.0415 \n","[2024-04-10 13:10:18,011::train::INFO] [Train] Iter 17908 | Loss 0.314853 | Grad 0.0433 \n","[2024-04-10 13:10:18,137::train::INFO] [Train] Iter 17909 | Loss 0.282485 | Grad 0.0399 \n","[2024-04-10 13:10:18,214::train::INFO] [Train] Iter 17910 | Loss 0.323960 | Grad 0.0508 \n","[2024-04-10 13:10:18,334::train::INFO] [Train] Iter 17911 | Loss 0.286178 | Grad 0.0393 \n","[2024-04-10 13:10:18,456::train::INFO] [Train] Iter 17912 | Loss 0.332543 | Grad 0.0521 \n","[2024-04-10 13:10:18,578::train::INFO] [Train] Iter 17913 | Loss 0.334390 | Grad 0.0515 \n","[2024-04-10 13:10:18,700::train::INFO] [Train] Iter 17914 | Loss 0.322478 | Grad 0.0478 \n","[2024-04-10 13:10:18,821::train::INFO] [Train] Iter 17915 | Loss 0.321937 | Grad 0.0362 \n","[2024-04-10 13:10:18,943::train::INFO] [Train] Iter 17916 | Loss 0.270749 | Grad 0.0293 \n","[2024-04-10 13:10:19,065::train::INFO] [Train] Iter 17917 | Loss 0.359549 | Grad 0.0608 \n","[2024-04-10 13:10:19,187::train::INFO] [Train] Iter 17918 | Loss 0.283496 | Grad 0.0587 \n","[2024-04-10 13:10:19,308::train::INFO] [Train] Iter 17919 | Loss 0.343135 | Grad 0.0581 \n","[2024-04-10 13:10:19,429::train::INFO] [Train] Iter 17920 | Loss 0.312696 | Grad 0.0560 \n","[2024-04-10 13:10:19,549::train::INFO] [Train] Iter 17921 | Loss 0.306856 | Grad 0.0459 \n","[2024-04-10 13:10:19,672::train::INFO] [Train] Iter 17922 | Loss 0.295981 | Grad 0.0478 \n","[2024-04-10 13:10:19,793::train::INFO] [Train] Iter 17923 | Loss 0.321650 | Grad 0.0410 \n","[2024-04-10 13:10:19,922::train::INFO] [Train] Iter 17924 | Loss 0.297752 | Grad 0.0435 \n","[2024-04-10 13:10:20,041::train::INFO] [Train] Iter 17925 | Loss 0.330809 | Grad 0.0676 \n","[2024-04-10 13:10:20,161::train::INFO] [Train] Iter 17926 | Loss 0.326737 | Grad 0.0434 \n","[2024-04-10 13:10:20,281::train::INFO] [Train] Iter 17927 | Loss 0.272217 | Grad 0.0270 \n","[2024-04-10 13:10:20,402::train::INFO] [Train] Iter 17928 | Loss 0.292489 | Grad 0.0564 \n","[2024-04-10 13:10:20,521::train::INFO] [Train] Iter 17929 | Loss 0.297899 | Grad 0.0501 \n","[2024-04-10 13:10:20,640::train::INFO] [Train] Iter 17930 | Loss 0.356007 | Grad 0.0449 \n","[2024-04-10 13:10:20,759::train::INFO] [Train] Iter 17931 | Loss 0.304922 | Grad 0.0359 \n","[2024-04-10 13:10:20,878::train::INFO] [Train] Iter 17932 | Loss 0.317315 | Grad 0.0457 \n","[2024-04-10 13:10:20,997::train::INFO] [Train] Iter 17933 | Loss 0.328312 | Grad 0.0401 \n","[2024-04-10 13:10:21,116::train::INFO] [Train] Iter 17934 | Loss 0.334980 | Grad 0.0431 \n","[2024-04-10 13:10:21,234::train::INFO] [Train] Iter 17935 | Loss 0.289425 | Grad 0.0421 \n","[2024-04-10 13:10:21,353::train::INFO] [Train] Iter 17936 | Loss 0.280313 | Grad 0.0389 \n","[2024-04-10 13:10:21,472::train::INFO] [Train] Iter 17937 | Loss 0.318018 | Grad 0.0458 \n","[2024-04-10 13:10:21,591::train::INFO] [Train] Iter 17938 | Loss 0.308570 | Grad 0.0335 \n","[2024-04-10 13:10:21,711::train::INFO] [Train] Iter 17939 | Loss 0.348254 | Grad 0.0829 \n","[2024-04-10 13:10:21,830::train::INFO] [Train] Iter 17940 | Loss 0.336137 | Grad 0.0511 \n","[2024-04-10 13:10:21,950::train::INFO] [Train] Iter 17941 | Loss 0.321511 | Grad 0.0375 \n","[2024-04-10 13:10:22,070::train::INFO] [Train] Iter 17942 | Loss 0.334145 | Grad 0.0528 \n","[2024-04-10 13:10:22,191::train::INFO] [Train] Iter 17943 | Loss 0.287449 | Grad 0.0410 \n","[2024-04-10 13:10:22,311::train::INFO] [Train] Iter 17944 | Loss 0.314848 | Grad 0.0651 \n","[2024-04-10 13:10:22,432::train::INFO] [Train] Iter 17945 | Loss 0.325636 | Grad 0.0493 \n","[2024-04-10 13:10:22,552::train::INFO] [Train] Iter 17946 | Loss 0.308606 | Grad 0.0403 \n","[2024-04-10 13:10:22,672::train::INFO] [Train] Iter 17947 | Loss 0.335337 | Grad 0.0563 \n","[2024-04-10 13:10:22,792::train::INFO] [Train] Iter 17948 | Loss 0.349932 | Grad 0.0422 \n","[2024-04-10 13:10:22,912::train::INFO] [Train] Iter 17949 | Loss 0.304890 | Grad 0.0347 \n","[2024-04-10 13:10:23,031::train::INFO] [Train] Iter 17950 | Loss 0.323663 | Grad 0.0544 \n","[2024-04-10 13:10:23,151::train::INFO] [Train] Iter 17951 | Loss 0.334314 | Grad 0.0600 \n","[2024-04-10 13:10:23,269::train::INFO] [Train] Iter 17952 | Loss 0.274277 | Grad 0.0265 \n","[2024-04-10 13:10:23,389::train::INFO] [Train] Iter 17953 | Loss 0.331029 | Grad 0.0514 \n","[2024-04-10 13:10:23,507::train::INFO] [Train] Iter 17954 | Loss 0.277326 | Grad 0.0455 \n","[2024-04-10 13:10:23,582::train::INFO] [Train] Iter 17955 | Loss 0.314370 | Grad 0.0559 \n","[2024-04-10 13:10:23,701::train::INFO] [Train] Iter 17956 | Loss 0.305562 | Grad 0.0446 \n","[2024-04-10 13:10:23,820::train::INFO] [Train] Iter 17957 | Loss 0.336399 | Grad 0.0439 \n","[2024-04-10 13:10:23,940::train::INFO] [Train] Iter 17958 | Loss 0.331752 | Grad 0.0489 \n","[2024-04-10 13:10:24,060::train::INFO] [Train] Iter 17959 | Loss 0.330536 | Grad 0.0438 \n","[2024-04-10 13:10:24,180::train::INFO] [Train] Iter 17960 | Loss 0.273839 | Grad 0.0453 \n","[2024-04-10 13:10:24,299::train::INFO] [Train] Iter 17961 | Loss 0.366120 | Grad 0.0597 \n","[2024-04-10 13:10:24,420::train::INFO] [Train] Iter 17962 | Loss 0.311363 | Grad 0.0458 \n","[2024-04-10 13:10:24,539::train::INFO] [Train] Iter 17963 | Loss 0.311633 | Grad 0.0498 \n","[2024-04-10 13:10:24,660::train::INFO] [Train] Iter 17964 | Loss 0.310039 | Grad 0.0397 \n","[2024-04-10 13:10:24,778::train::INFO] [Train] Iter 17965 | Loss 0.337761 | Grad 0.0367 \n","[2024-04-10 13:10:24,897::train::INFO] [Train] Iter 17966 | Loss 0.286390 | Grad 0.0381 \n","[2024-04-10 13:10:25,017::train::INFO] [Train] Iter 17967 | Loss 0.305477 | Grad 0.0391 \n","[2024-04-10 13:10:25,137::train::INFO] [Train] Iter 17968 | Loss 0.333519 | Grad 0.0524 \n","[2024-04-10 13:10:25,255::train::INFO] [Train] Iter 17969 | Loss 0.290706 | Grad 0.0290 \n","[2024-04-10 13:10:25,374::train::INFO] [Train] Iter 17970 | Loss 0.306401 | Grad 0.0384 \n","[2024-04-10 13:10:25,495::train::INFO] [Train] Iter 17971 | Loss 0.316211 | Grad 0.0773 \n","[2024-04-10 13:10:25,615::train::INFO] [Train] Iter 17972 | Loss 0.280639 | Grad 0.0542 \n","[2024-04-10 13:10:25,737::train::INFO] [Train] Iter 17973 | Loss 0.286106 | Grad 0.0459 \n","[2024-04-10 13:10:25,857::train::INFO] [Train] Iter 17974 | Loss 0.296704 | Grad 0.0385 \n","[2024-04-10 13:10:25,976::train::INFO] [Train] Iter 17975 | Loss 0.316410 | Grad 0.0538 \n","[2024-04-10 13:10:26,095::train::INFO] [Train] Iter 17976 | Loss 0.281267 | Grad 0.0359 \n","[2024-04-10 13:10:26,214::train::INFO] [Train] Iter 17977 | Loss 0.304808 | Grad 0.0503 \n","[2024-04-10 13:10:26,334::train::INFO] [Train] Iter 17978 | Loss 0.297329 | Grad 0.0437 \n","[2024-04-10 13:10:26,454::train::INFO] [Train] Iter 17979 | Loss 0.316460 | Grad 0.0479 \n","[2024-04-10 13:10:26,573::train::INFO] [Train] Iter 17980 | Loss 0.281606 | Grad 0.0367 \n","[2024-04-10 13:10:26,692::train::INFO] [Train] Iter 17981 | Loss 0.303045 | Grad 0.0465 \n","[2024-04-10 13:10:26,813::train::INFO] [Train] Iter 17982 | Loss 0.347343 | Grad 0.0552 \n","[2024-04-10 13:10:26,931::train::INFO] [Train] Iter 17983 | Loss 0.326073 | Grad 0.0525 \n","[2024-04-10 13:10:27,051::train::INFO] [Train] Iter 17984 | Loss 0.319282 | Grad 0.0478 \n","[2024-04-10 13:10:27,171::train::INFO] [Train] Iter 17985 | Loss 0.303943 | Grad 0.0507 \n","[2024-04-10 13:10:27,292::train::INFO] [Train] Iter 17986 | Loss 0.305279 | Grad 0.0327 \n","[2024-04-10 13:10:27,411::train::INFO] [Train] Iter 17987 | Loss 0.261695 | Grad 0.0380 \n","[2024-04-10 13:10:27,531::train::INFO] [Train] Iter 17988 | Loss 0.285805 | Grad 0.0304 \n","[2024-04-10 13:10:27,651::train::INFO] [Train] Iter 17989 | Loss 0.302097 | Grad 0.0565 \n","[2024-04-10 13:10:27,772::train::INFO] [Train] Iter 17990 | Loss 0.370175 | Grad 0.0775 \n","[2024-04-10 13:10:27,891::train::INFO] [Train] Iter 17991 | Loss 0.325133 | Grad 0.0317 \n","[2024-04-10 13:10:28,010::train::INFO] [Train] Iter 17992 | Loss 0.317765 | Grad 0.0530 \n","[2024-04-10 13:10:28,130::train::INFO] [Train] Iter 17993 | Loss 0.282372 | Grad 0.0323 \n","[2024-04-10 13:10:28,252::train::INFO] [Train] Iter 17994 | Loss 0.318682 | Grad 0.0523 \n","[2024-04-10 13:10:28,372::train::INFO] [Train] Iter 17995 | Loss 0.269303 | Grad 0.0471 \n","[2024-04-10 13:10:28,491::train::INFO] [Train] Iter 17996 | Loss 0.288247 | Grad 0.0520 \n","[2024-04-10 13:10:28,611::train::INFO] [Train] Iter 17997 | Loss 0.316614 | Grad 0.0509 \n","[2024-04-10 13:10:28,730::train::INFO] [Train] Iter 17998 | Loss 0.350472 | Grad 0.0410 \n","[2024-04-10 13:10:28,851::train::INFO] [Train] Iter 17999 | Loss 0.325860 | Grad 0.0410 \n","[2024-04-10 13:10:28,926::train::INFO] [Train] Iter 18000 | Loss 0.297868 | Grad 0.0409 \n","Validate: 100% 31/31 [00:48<00:00,  1.57s/it]\n","EMD-CD: 100% 31/31 [00:00<00:00, 136.55it/s]\n","[2024-04-10 13:11:17,920::train::INFO] [Val] Iter 18000 | CD 0.000821 | EMD 0.000000  \n","Inspect:   3% 1/31 [00:03<01:33,  3.13s/it]\n","[2024-04-10 13:11:21,293::train::INFO] [Train] Iter 18001 | Loss 0.329995 | Grad 0.0479 \n","[2024-04-10 13:11:21,411::train::INFO] [Train] Iter 18002 | Loss 0.324187 | Grad 0.0493 \n","[2024-04-10 13:11:21,530::train::INFO] [Train] Iter 18003 | Loss 0.358692 | Grad 0.0596 \n","[2024-04-10 13:11:21,650::train::INFO] [Train] Iter 18004 | Loss 0.324981 | Grad 0.0622 \n","[2024-04-10 13:11:21,772::train::INFO] [Train] Iter 18005 | Loss 0.305413 | Grad 0.0359 \n","[2024-04-10 13:11:21,894::train::INFO] [Train] Iter 18006 | Loss 0.319510 | Grad 0.0383 \n","[2024-04-10 13:11:22,015::train::INFO] [Train] Iter 18007 | Loss 0.317378 | Grad 0.0406 \n","[2024-04-10 13:11:22,138::train::INFO] [Train] Iter 18008 | Loss 0.332432 | Grad 0.0573 \n","[2024-04-10 13:11:22,260::train::INFO] [Train] Iter 18009 | Loss 0.351194 | Grad 0.0680 \n","[2024-04-10 13:11:22,381::train::INFO] [Train] Iter 18010 | Loss 0.333863 | Grad 0.0440 \n","[2024-04-10 13:11:22,501::train::INFO] [Train] Iter 18011 | Loss 0.348701 | Grad 0.0449 \n","[2024-04-10 13:11:22,622::train::INFO] [Train] Iter 18012 | Loss 0.274845 | Grad 0.0302 \n","[2024-04-10 13:11:22,742::train::INFO] [Train] Iter 18013 | Loss 0.306837 | Grad 0.0350 \n","[2024-04-10 13:11:22,863::train::INFO] [Train] Iter 18014 | Loss 0.334623 | Grad 0.0395 \n","[2024-04-10 13:11:22,984::train::INFO] [Train] Iter 18015 | Loss 0.294128 | Grad 0.0271 \n","[2024-04-10 13:11:23,104::train::INFO] [Train] Iter 18016 | Loss 0.327231 | Grad 0.0480 \n","[2024-04-10 13:11:23,227::train::INFO] [Train] Iter 18017 | Loss 0.347183 | Grad 0.0508 \n","[2024-04-10 13:11:23,348::train::INFO] [Train] Iter 18018 | Loss 0.315521 | Grad 0.0603 \n","[2024-04-10 13:11:23,469::train::INFO] [Train] Iter 18019 | Loss 0.287851 | Grad 0.0565 \n","[2024-04-10 13:11:23,591::train::INFO] [Train] Iter 18020 | Loss 0.302596 | Grad 0.0479 \n","[2024-04-10 13:11:23,711::train::INFO] [Train] Iter 18021 | Loss 0.340307 | Grad 0.0533 \n","[2024-04-10 13:11:23,832::train::INFO] [Train] Iter 18022 | Loss 0.352399 | Grad 0.0487 \n","[2024-04-10 13:11:23,956::train::INFO] [Train] Iter 18023 | Loss 0.327403 | Grad 0.0492 \n","[2024-04-10 13:11:24,078::train::INFO] [Train] Iter 18024 | Loss 0.296702 | Grad 0.0368 \n","[2024-04-10 13:11:24,205::train::INFO] [Train] Iter 18025 | Loss 0.319883 | Grad 0.0490 \n","[2024-04-10 13:11:24,328::train::INFO] [Train] Iter 18026 | Loss 0.276624 | Grad 0.0355 \n","[2024-04-10 13:11:24,449::train::INFO] [Train] Iter 18027 | Loss 0.290134 | Grad 0.0426 \n","[2024-04-10 13:11:24,571::train::INFO] [Train] Iter 18028 | Loss 0.339654 | Grad 0.0576 \n","[2024-04-10 13:11:24,693::train::INFO] [Train] Iter 18029 | Loss 0.295658 | Grad 0.0485 \n","[2024-04-10 13:11:24,816::train::INFO] [Train] Iter 18030 | Loss 0.336326 | Grad 0.0432 \n","[2024-04-10 13:11:24,935::train::INFO] [Train] Iter 18031 | Loss 0.299847 | Grad 0.0474 \n","[2024-04-10 13:11:25,056::train::INFO] [Train] Iter 18032 | Loss 0.338444 | Grad 0.0446 \n","[2024-04-10 13:11:25,177::train::INFO] [Train] Iter 18033 | Loss 0.318063 | Grad 0.0504 \n","[2024-04-10 13:11:25,296::train::INFO] [Train] Iter 18034 | Loss 0.328169 | Grad 0.0442 \n","[2024-04-10 13:11:25,414::train::INFO] [Train] Iter 18035 | Loss 0.291660 | Grad 0.0643 \n","[2024-04-10 13:11:25,532::train::INFO] [Train] Iter 18036 | Loss 0.301653 | Grad 0.0465 \n","[2024-04-10 13:11:25,651::train::INFO] [Train] Iter 18037 | Loss 0.325069 | Grad 0.0653 \n","[2024-04-10 13:11:25,770::train::INFO] [Train] Iter 18038 | Loss 0.342475 | Grad 0.0450 \n","[2024-04-10 13:11:25,889::train::INFO] [Train] Iter 18039 | Loss 0.331579 | Grad 0.0428 \n","[2024-04-10 13:11:26,007::train::INFO] [Train] Iter 18040 | Loss 0.293000 | Grad 0.0588 \n","[2024-04-10 13:11:26,127::train::INFO] [Train] Iter 18041 | Loss 0.348744 | Grad 0.0521 \n","[2024-04-10 13:11:26,248::train::INFO] [Train] Iter 18042 | Loss 0.313313 | Grad 0.0459 \n","[2024-04-10 13:11:26,371::train::INFO] [Train] Iter 18043 | Loss 0.340563 | Grad 0.0609 \n","[2024-04-10 13:11:26,489::train::INFO] [Train] Iter 18044 | Loss 0.336305 | Grad 0.0471 \n","[2024-04-10 13:11:26,564::train::INFO] [Train] Iter 18045 | Loss 0.331118 | Grad 0.0464 \n","[2024-04-10 13:11:26,684::train::INFO] [Train] Iter 18046 | Loss 0.284740 | Grad 0.0560 \n","[2024-04-10 13:11:26,803::train::INFO] [Train] Iter 18047 | Loss 0.306952 | Grad 0.0413 \n","[2024-04-10 13:11:26,922::train::INFO] [Train] Iter 18048 | Loss 0.336979 | Grad 0.0613 \n","[2024-04-10 13:11:27,041::train::INFO] [Train] Iter 18049 | Loss 0.260853 | Grad 0.0347 \n","[2024-04-10 13:11:27,159::train::INFO] [Train] Iter 18050 | Loss 0.313657 | Grad 0.0361 \n","[2024-04-10 13:11:27,278::train::INFO] [Train] Iter 18051 | Loss 0.308687 | Grad 0.0502 \n","[2024-04-10 13:11:27,399::train::INFO] [Train] Iter 18052 | Loss 0.311285 | Grad 0.0536 \n","[2024-04-10 13:11:27,517::train::INFO] [Train] Iter 18053 | Loss 0.320710 | Grad 0.0628 \n","[2024-04-10 13:11:27,636::train::INFO] [Train] Iter 18054 | Loss 0.305512 | Grad 0.0432 \n","[2024-04-10 13:11:27,755::train::INFO] [Train] Iter 18055 | Loss 0.293348 | Grad 0.0354 \n","[2024-04-10 13:11:27,875::train::INFO] [Train] Iter 18056 | Loss 0.340261 | Grad 0.0482 \n","[2024-04-10 13:11:27,994::train::INFO] [Train] Iter 18057 | Loss 0.297580 | Grad 0.0464 \n","[2024-04-10 13:11:28,113::train::INFO] [Train] Iter 18058 | Loss 0.334297 | Grad 0.0488 \n","[2024-04-10 13:11:28,233::train::INFO] [Train] Iter 18059 | Loss 0.318329 | Grad 0.0391 \n","[2024-04-10 13:11:28,354::train::INFO] [Train] Iter 18060 | Loss 0.287104 | Grad 0.0297 \n","[2024-04-10 13:11:28,474::train::INFO] [Train] Iter 18061 | Loss 0.305686 | Grad 0.0544 \n","[2024-04-10 13:11:28,594::train::INFO] [Train] Iter 18062 | Loss 0.316655 | Grad 0.0770 \n","[2024-04-10 13:11:28,718::train::INFO] [Train] Iter 18063 | Loss 0.320017 | Grad 0.0595 \n","[2024-04-10 13:11:28,837::train::INFO] [Train] Iter 18064 | Loss 0.323218 | Grad 0.0349 \n","[2024-04-10 13:11:28,956::train::INFO] [Train] Iter 18065 | Loss 0.301043 | Grad 0.0413 \n","[2024-04-10 13:11:29,074::train::INFO] [Train] Iter 18066 | Loss 0.357638 | Grad 0.0623 \n","[2024-04-10 13:11:29,193::train::INFO] [Train] Iter 18067 | Loss 0.367091 | Grad 0.0381 \n","[2024-04-10 13:11:29,314::train::INFO] [Train] Iter 18068 | Loss 0.314583 | Grad 0.0453 \n","[2024-04-10 13:11:29,433::train::INFO] [Train] Iter 18069 | Loss 0.297842 | Grad 0.0321 \n","[2024-04-10 13:11:29,552::train::INFO] [Train] Iter 18070 | Loss 0.332933 | Grad 0.0509 \n","[2024-04-10 13:11:29,671::train::INFO] [Train] Iter 18071 | Loss 0.299717 | Grad 0.0362 \n","[2024-04-10 13:11:29,790::train::INFO] [Train] Iter 18072 | Loss 0.313251 | Grad 0.0557 \n","[2024-04-10 13:11:29,910::train::INFO] [Train] Iter 18073 | Loss 0.342543 | Grad 0.0784 \n","[2024-04-10 13:11:30,031::train::INFO] [Train] Iter 18074 | Loss 0.291287 | Grad 0.0610 \n","[2024-04-10 13:11:30,150::train::INFO] [Train] Iter 18075 | Loss 0.281033 | Grad 0.0561 \n","[2024-04-10 13:11:30,269::train::INFO] [Train] Iter 18076 | Loss 0.368271 | Grad 0.0724 \n","[2024-04-10 13:11:30,388::train::INFO] [Train] Iter 18077 | Loss 0.321531 | Grad 0.0369 \n","[2024-04-10 13:11:30,506::train::INFO] [Train] Iter 18078 | Loss 0.325307 | Grad 0.0381 \n","[2024-04-10 13:11:30,626::train::INFO] [Train] Iter 18079 | Loss 0.312989 | Grad 0.0490 \n","[2024-04-10 13:11:30,745::train::INFO] [Train] Iter 18080 | Loss 0.327045 | Grad 0.0895 \n","[2024-04-10 13:11:30,863::train::INFO] [Train] Iter 18081 | Loss 0.302610 | Grad 0.0441 \n","[2024-04-10 13:11:30,982::train::INFO] [Train] Iter 18082 | Loss 0.297711 | Grad 0.0596 \n","[2024-04-10 13:11:31,104::train::INFO] [Train] Iter 18083 | Loss 0.355209 | Grad 0.0666 \n","[2024-04-10 13:11:31,225::train::INFO] [Train] Iter 18084 | Loss 0.308051 | Grad 0.0490 \n","[2024-04-10 13:11:31,344::train::INFO] [Train] Iter 18085 | Loss 0.319296 | Grad 0.0552 \n","[2024-04-10 13:11:31,464::train::INFO] [Train] Iter 18086 | Loss 0.338332 | Grad 0.0627 \n","[2024-04-10 13:11:31,583::train::INFO] [Train] Iter 18087 | Loss 0.328501 | Grad 0.0495 \n","[2024-04-10 13:11:31,702::train::INFO] [Train] Iter 18088 | Loss 0.311796 | Grad 0.0389 \n","[2024-04-10 13:11:31,821::train::INFO] [Train] Iter 18089 | Loss 0.318960 | Grad 0.0357 \n","[2024-04-10 13:11:31,896::train::INFO] [Train] Iter 18090 | Loss 0.372547 | Grad 0.0434 \n","[2024-04-10 13:11:32,014::train::INFO] [Train] Iter 18091 | Loss 0.301716 | Grad 0.0412 \n","[2024-04-10 13:11:32,132::train::INFO] [Train] Iter 18092 | Loss 0.355826 | Grad 0.0496 \n","[2024-04-10 13:11:32,250::train::INFO] [Train] Iter 18093 | Loss 0.317954 | Grad 0.0468 \n","[2024-04-10 13:11:32,369::train::INFO] [Train] Iter 18094 | Loss 0.326517 | Grad 0.0592 \n","[2024-04-10 13:11:32,488::train::INFO] [Train] Iter 18095 | Loss 0.342066 | Grad 0.0544 \n","[2024-04-10 13:11:32,607::train::INFO] [Train] Iter 18096 | Loss 0.305679 | Grad 0.0384 \n","[2024-04-10 13:11:32,732::train::INFO] [Train] Iter 18097 | Loss 0.303036 | Grad 0.0352 \n","[2024-04-10 13:11:32,851::train::INFO] [Train] Iter 18098 | Loss 0.344318 | Grad 0.0618 \n","[2024-04-10 13:11:32,970::train::INFO] [Train] Iter 18099 | Loss 0.296433 | Grad 0.0511 \n","[2024-04-10 13:11:33,090::train::INFO] [Train] Iter 18100 | Loss 0.328756 | Grad 0.0562 \n","[2024-04-10 13:11:33,210::train::INFO] [Train] Iter 18101 | Loss 0.360478 | Grad 0.0739 \n","[2024-04-10 13:11:33,333::train::INFO] [Train] Iter 18102 | Loss 0.316767 | Grad 0.0489 \n","[2024-04-10 13:11:33,453::train::INFO] [Train] Iter 18103 | Loss 0.317959 | Grad 0.0454 \n","[2024-04-10 13:11:33,574::train::INFO] [Train] Iter 18104 | Loss 0.319839 | Grad 0.0396 \n","[2024-04-10 13:11:33,693::train::INFO] [Train] Iter 18105 | Loss 0.309684 | Grad 0.0471 \n","[2024-04-10 13:11:33,811::train::INFO] [Train] Iter 18106 | Loss 0.319858 | Grad 0.0755 \n","[2024-04-10 13:11:33,932::train::INFO] [Train] Iter 18107 | Loss 0.305485 | Grad 0.0425 \n","[2024-04-10 13:11:34,051::train::INFO] [Train] Iter 18108 | Loss 0.360695 | Grad 0.0699 \n","[2024-04-10 13:11:34,170::train::INFO] [Train] Iter 18109 | Loss 0.343874 | Grad 0.0466 \n","[2024-04-10 13:11:34,289::train::INFO] [Train] Iter 18110 | Loss 0.299432 | Grad 0.0358 \n","[2024-04-10 13:11:34,409::train::INFO] [Train] Iter 18111 | Loss 0.284426 | Grad 0.0577 \n","[2024-04-10 13:11:34,529::train::INFO] [Train] Iter 18112 | Loss 0.290717 | Grad 0.0433 \n","[2024-04-10 13:11:34,650::train::INFO] [Train] Iter 18113 | Loss 0.317935 | Grad 0.0329 \n","[2024-04-10 13:11:34,770::train::INFO] [Train] Iter 18114 | Loss 0.301034 | Grad 0.0356 \n","[2024-04-10 13:11:34,892::train::INFO] [Train] Iter 18115 | Loss 0.310443 | Grad 0.0369 \n","[2024-04-10 13:11:35,014::train::INFO] [Train] Iter 18116 | Loss 0.318700 | Grad 0.0479 \n","[2024-04-10 13:11:35,137::train::INFO] [Train] Iter 18117 | Loss 0.306748 | Grad 0.0397 \n","[2024-04-10 13:11:35,260::train::INFO] [Train] Iter 18118 | Loss 0.295023 | Grad 0.0525 \n","[2024-04-10 13:11:35,381::train::INFO] [Train] Iter 18119 | Loss 0.343156 | Grad 0.0969 \n","[2024-04-10 13:11:35,503::train::INFO] [Train] Iter 18120 | Loss 0.297707 | Grad 0.0678 \n","[2024-04-10 13:11:35,630::train::INFO] [Train] Iter 18121 | Loss 0.307666 | Grad 0.0438 \n","[2024-04-10 13:11:35,751::train::INFO] [Train] Iter 18122 | Loss 0.312620 | Grad 0.0427 \n","[2024-04-10 13:11:35,871::train::INFO] [Train] Iter 18123 | Loss 0.324042 | Grad 0.0538 \n","[2024-04-10 13:11:35,993::train::INFO] [Train] Iter 18124 | Loss 0.329230 | Grad 0.0521 \n","[2024-04-10 13:11:36,116::train::INFO] [Train] Iter 18125 | Loss 0.281915 | Grad 0.0417 \n","[2024-04-10 13:11:36,241::train::INFO] [Train] Iter 18126 | Loss 0.324568 | Grad 0.0554 \n","[2024-04-10 13:11:36,361::train::INFO] [Train] Iter 18127 | Loss 0.297622 | Grad 0.0526 \n","[2024-04-10 13:11:36,485::train::INFO] [Train] Iter 18128 | Loss 0.291856 | Grad 0.0586 \n","[2024-04-10 13:11:36,611::train::INFO] [Train] Iter 18129 | Loss 0.298765 | Grad 0.0659 \n","[2024-04-10 13:11:36,732::train::INFO] [Train] Iter 18130 | Loss 0.294572 | Grad 0.0643 \n","[2024-04-10 13:11:36,856::train::INFO] [Train] Iter 18131 | Loss 0.322608 | Grad 0.0570 \n","[2024-04-10 13:11:36,976::train::INFO] [Train] Iter 18132 | Loss 0.309782 | Grad 0.0513 \n","[2024-04-10 13:11:37,100::train::INFO] [Train] Iter 18133 | Loss 0.289145 | Grad 0.0510 \n","[2024-04-10 13:11:37,220::train::INFO] [Train] Iter 18134 | Loss 0.398450 | Grad 0.1027 \n","[2024-04-10 13:11:37,297::train::INFO] [Train] Iter 18135 | Loss 0.309789 | Grad 0.0667 \n","[2024-04-10 13:11:37,418::train::INFO] [Train] Iter 18136 | Loss 0.301673 | Grad 0.0376 \n","[2024-04-10 13:11:37,541::train::INFO] [Train] Iter 18137 | Loss 0.281416 | Grad 0.0417 \n","[2024-04-10 13:11:37,662::train::INFO] [Train] Iter 18138 | Loss 0.298236 | Grad 0.0538 \n","[2024-04-10 13:11:37,785::train::INFO] [Train] Iter 18139 | Loss 0.338838 | Grad 0.0709 \n","[2024-04-10 13:11:37,905::train::INFO] [Train] Iter 18140 | Loss 0.307779 | Grad 0.0490 \n","[2024-04-10 13:11:38,028::train::INFO] [Train] Iter 18141 | Loss 0.356883 | Grad 0.0380 \n","[2024-04-10 13:11:38,147::train::INFO] [Train] Iter 18142 | Loss 0.317404 | Grad 0.0385 \n","[2024-04-10 13:11:38,266::train::INFO] [Train] Iter 18143 | Loss 0.312025 | Grad 0.0420 \n","[2024-04-10 13:11:38,385::train::INFO] [Train] Iter 18144 | Loss 0.329644 | Grad 0.0440 \n","[2024-04-10 13:11:38,505::train::INFO] [Train] Iter 18145 | Loss 0.300455 | Grad 0.0377 \n","[2024-04-10 13:11:38,625::train::INFO] [Train] Iter 18146 | Loss 0.281803 | Grad 0.0577 \n","[2024-04-10 13:11:38,747::train::INFO] [Train] Iter 18147 | Loss 0.318992 | Grad 0.0556 \n","[2024-04-10 13:11:38,867::train::INFO] [Train] Iter 18148 | Loss 0.306122 | Grad 0.0310 \n","[2024-04-10 13:11:38,993::train::INFO] [Train] Iter 18149 | Loss 0.318178 | Grad 0.0350 \n","[2024-04-10 13:11:39,112::train::INFO] [Train] Iter 18150 | Loss 0.339122 | Grad 0.0405 \n","[2024-04-10 13:11:39,233::train::INFO] [Train] Iter 18151 | Loss 0.314707 | Grad 0.0420 \n","[2024-04-10 13:11:39,353::train::INFO] [Train] Iter 18152 | Loss 0.339044 | Grad 0.0447 \n","[2024-04-10 13:11:39,473::train::INFO] [Train] Iter 18153 | Loss 0.322895 | Grad 0.0573 \n","[2024-04-10 13:11:39,596::train::INFO] [Train] Iter 18154 | Loss 0.290660 | Grad 0.0350 \n","[2024-04-10 13:11:39,716::train::INFO] [Train] Iter 18155 | Loss 0.361705 | Grad 0.0496 \n","[2024-04-10 13:11:39,837::train::INFO] [Train] Iter 18156 | Loss 0.320555 | Grad 0.0325 \n","[2024-04-10 13:11:39,957::train::INFO] [Train] Iter 18157 | Loss 0.314923 | Grad 0.0317 \n","[2024-04-10 13:11:40,077::train::INFO] [Train] Iter 18158 | Loss 0.300891 | Grad 0.0418 \n","[2024-04-10 13:11:40,199::train::INFO] [Train] Iter 18159 | Loss 0.295084 | Grad 0.0437 \n","[2024-04-10 13:11:40,320::train::INFO] [Train] Iter 18160 | Loss 0.330108 | Grad 0.0568 \n","[2024-04-10 13:11:40,439::train::INFO] [Train] Iter 18161 | Loss 0.307546 | Grad 0.0343 \n","[2024-04-10 13:11:40,560::train::INFO] [Train] Iter 18162 | Loss 0.329740 | Grad 0.0518 \n","[2024-04-10 13:11:40,681::train::INFO] [Train] Iter 18163 | Loss 0.301826 | Grad 0.0356 \n","[2024-04-10 13:11:40,805::train::INFO] [Train] Iter 18164 | Loss 0.346493 | Grad 0.0385 \n","[2024-04-10 13:11:40,923::train::INFO] [Train] Iter 18165 | Loss 0.323379 | Grad 0.0461 \n","[2024-04-10 13:11:41,042::train::INFO] [Train] Iter 18166 | Loss 0.322688 | Grad 0.0562 \n","[2024-04-10 13:11:41,162::train::INFO] [Train] Iter 18167 | Loss 0.364385 | Grad 0.0843 \n","[2024-04-10 13:11:41,282::train::INFO] [Train] Iter 18168 | Loss 0.341103 | Grad 0.0441 \n","[2024-04-10 13:11:41,403::train::INFO] [Train] Iter 18169 | Loss 0.340490 | Grad 0.0440 \n","[2024-04-10 13:11:41,522::train::INFO] [Train] Iter 18170 | Loss 0.279789 | Grad 0.0446 \n","[2024-04-10 13:11:41,642::train::INFO] [Train] Iter 18171 | Loss 0.336040 | Grad 0.0466 \n","[2024-04-10 13:11:41,763::train::INFO] [Train] Iter 18172 | Loss 0.307785 | Grad 0.0543 \n","[2024-04-10 13:11:41,883::train::INFO] [Train] Iter 18173 | Loss 0.320887 | Grad 0.0785 \n","[2024-04-10 13:11:42,008::train::INFO] [Train] Iter 18174 | Loss 0.324093 | Grad 0.0535 \n","[2024-04-10 13:11:42,128::train::INFO] [Train] Iter 18175 | Loss 0.292915 | Grad 0.0470 \n","[2024-04-10 13:11:42,248::train::INFO] [Train] Iter 18176 | Loss 0.288717 | Grad 0.0387 \n","[2024-04-10 13:11:42,368::train::INFO] [Train] Iter 18177 | Loss 0.294657 | Grad 0.0422 \n","[2024-04-10 13:11:42,488::train::INFO] [Train] Iter 18178 | Loss 0.315515 | Grad 0.0412 \n","[2024-04-10 13:11:42,610::train::INFO] [Train] Iter 18179 | Loss 0.337617 | Grad 0.0579 \n","[2024-04-10 13:11:42,685::train::INFO] [Train] Iter 18180 | Loss 0.312881 | Grad 0.0544 \n","[2024-04-10 13:11:42,804::train::INFO] [Train] Iter 18181 | Loss 0.323406 | Grad 0.0507 \n","[2024-04-10 13:11:42,922::train::INFO] [Train] Iter 18182 | Loss 0.348966 | Grad 0.0829 \n","[2024-04-10 13:11:43,039::train::INFO] [Train] Iter 18183 | Loss 0.299768 | Grad 0.0404 \n","[2024-04-10 13:11:43,157::train::INFO] [Train] Iter 18184 | Loss 0.283580 | Grad 0.0391 \n","[2024-04-10 13:11:43,277::train::INFO] [Train] Iter 18185 | Loss 0.301586 | Grad 0.0291 \n","[2024-04-10 13:11:43,396::train::INFO] [Train] Iter 18186 | Loss 0.264167 | Grad 0.0352 \n","[2024-04-10 13:11:43,517::train::INFO] [Train] Iter 18187 | Loss 0.275543 | Grad 0.0441 \n","[2024-04-10 13:11:43,636::train::INFO] [Train] Iter 18188 | Loss 0.286524 | Grad 0.0475 \n","[2024-04-10 13:11:43,757::train::INFO] [Train] Iter 18189 | Loss 0.312900 | Grad 0.0361 \n","[2024-04-10 13:11:43,881::train::INFO] [Train] Iter 18190 | Loss 0.285442 | Grad 0.0443 \n","[2024-04-10 13:11:44,000::train::INFO] [Train] Iter 18191 | Loss 0.315501 | Grad 0.0570 \n","[2024-04-10 13:11:44,120::train::INFO] [Train] Iter 18192 | Loss 0.330973 | Grad 0.0565 \n","[2024-04-10 13:11:44,240::train::INFO] [Train] Iter 18193 | Loss 0.315049 | Grad 0.0409 \n","[2024-04-10 13:11:44,358::train::INFO] [Train] Iter 18194 | Loss 0.309107 | Grad 0.0458 \n","[2024-04-10 13:11:44,478::train::INFO] [Train] Iter 18195 | Loss 0.328289 | Grad 0.0582 \n","[2024-04-10 13:11:44,599::train::INFO] [Train] Iter 18196 | Loss 0.299927 | Grad 0.0348 \n","[2024-04-10 13:11:44,722::train::INFO] [Train] Iter 18197 | Loss 0.321801 | Grad 0.0474 \n","[2024-04-10 13:11:44,842::train::INFO] [Train] Iter 18198 | Loss 0.347211 | Grad 0.0679 \n","[2024-04-10 13:11:44,961::train::INFO] [Train] Iter 18199 | Loss 0.277304 | Grad 0.0585 \n","[2024-04-10 13:11:45,081::train::INFO] [Train] Iter 18200 | Loss 0.314515 | Grad 0.0386 \n","[2024-04-10 13:11:45,201::train::INFO] [Train] Iter 18201 | Loss 0.325138 | Grad 0.0472 \n","[2024-04-10 13:11:45,323::train::INFO] [Train] Iter 18202 | Loss 0.302381 | Grad 0.0407 \n","[2024-04-10 13:11:45,443::train::INFO] [Train] Iter 18203 | Loss 0.294085 | Grad 0.0279 \n","[2024-04-10 13:11:45,562::train::INFO] [Train] Iter 18204 | Loss 0.269873 | Grad 0.0323 \n","[2024-04-10 13:11:45,681::train::INFO] [Train] Iter 18205 | Loss 0.343024 | Grad 0.0433 \n","[2024-04-10 13:11:45,801::train::INFO] [Train] Iter 18206 | Loss 0.300270 | Grad 0.0464 \n","[2024-04-10 13:11:45,924::train::INFO] [Train] Iter 18207 | Loss 0.310233 | Grad 0.0684 \n","[2024-04-10 13:11:46,044::train::INFO] [Train] Iter 18208 | Loss 0.324955 | Grad 0.0476 \n","[2024-04-10 13:11:46,163::train::INFO] [Train] Iter 18209 | Loss 0.346995 | Grad 0.0436 \n","[2024-04-10 13:11:46,283::train::INFO] [Train] Iter 18210 | Loss 0.361661 | Grad 0.0480 \n","[2024-04-10 13:11:46,404::train::INFO] [Train] Iter 18211 | Loss 0.285742 | Grad 0.0359 \n","[2024-04-10 13:11:46,524::train::INFO] [Train] Iter 18212 | Loss 0.273676 | Grad 0.0387 \n","[2024-04-10 13:11:46,644::train::INFO] [Train] Iter 18213 | Loss 0.244350 | Grad 0.0472 \n","[2024-04-10 13:11:46,764::train::INFO] [Train] Iter 18214 | Loss 0.317585 | Grad 0.0426 \n","[2024-04-10 13:11:46,884::train::INFO] [Train] Iter 18215 | Loss 0.322780 | Grad 0.0543 \n","[2024-04-10 13:11:47,004::train::INFO] [Train] Iter 18216 | Loss 0.304940 | Grad 0.0385 \n","[2024-04-10 13:11:47,125::train::INFO] [Train] Iter 18217 | Loss 0.336067 | Grad 0.0446 \n","[2024-04-10 13:11:47,244::train::INFO] [Train] Iter 18218 | Loss 0.329448 | Grad 0.0993 \n","[2024-04-10 13:11:47,362::train::INFO] [Train] Iter 18219 | Loss 0.317659 | Grad 0.0686 \n","[2024-04-10 13:11:47,480::train::INFO] [Train] Iter 18220 | Loss 0.310806 | Grad 0.0534 \n","[2024-04-10 13:11:47,598::train::INFO] [Train] Iter 18221 | Loss 0.299447 | Grad 0.0508 \n","[2024-04-10 13:11:47,717::train::INFO] [Train] Iter 18222 | Loss 0.271124 | Grad 0.0418 \n","[2024-04-10 13:11:47,836::train::INFO] [Train] Iter 18223 | Loss 0.366133 | Grad 0.0609 \n","[2024-04-10 13:11:47,955::train::INFO] [Train] Iter 18224 | Loss 0.332435 | Grad 0.0469 \n","[2024-04-10 13:11:48,030::train::INFO] [Train] Iter 18225 | Loss 0.292892 | Grad 0.0627 \n","[2024-04-10 13:11:48,153::train::INFO] [Train] Iter 18226 | Loss 0.320808 | Grad 0.0433 \n","[2024-04-10 13:11:48,274::train::INFO] [Train] Iter 18227 | Loss 0.315228 | Grad 0.0565 \n","[2024-04-10 13:11:48,395::train::INFO] [Train] Iter 18228 | Loss 0.341361 | Grad 0.0727 \n","[2024-04-10 13:11:48,515::train::INFO] [Train] Iter 18229 | Loss 0.296586 | Grad 0.0452 \n","[2024-04-10 13:11:48,636::train::INFO] [Train] Iter 18230 | Loss 0.352142 | Grad 0.0743 \n","[2024-04-10 13:11:48,756::train::INFO] [Train] Iter 18231 | Loss 0.319283 | Grad 0.0425 \n","[2024-04-10 13:11:48,876::train::INFO] [Train] Iter 18232 | Loss 0.313244 | Grad 0.0524 \n","[2024-04-10 13:11:48,995::train::INFO] [Train] Iter 18233 | Loss 0.269051 | Grad 0.0588 \n","[2024-04-10 13:11:49,116::train::INFO] [Train] Iter 18234 | Loss 0.341726 | Grad 0.0514 \n","[2024-04-10 13:11:49,241::train::INFO] [Train] Iter 18235 | Loss 0.318055 | Grad 0.0425 \n","[2024-04-10 13:11:49,362::train::INFO] [Train] Iter 18236 | Loss 0.297039 | Grad 0.0436 \n","[2024-04-10 13:11:49,483::train::INFO] [Train] Iter 18237 | Loss 0.327498 | Grad 0.0480 \n","[2024-04-10 13:11:49,605::train::INFO] [Train] Iter 18238 | Loss 0.348267 | Grad 0.0527 \n","[2024-04-10 13:11:49,727::train::INFO] [Train] Iter 18239 | Loss 0.288778 | Grad 0.0424 \n","[2024-04-10 13:11:49,848::train::INFO] [Train] Iter 18240 | Loss 0.290352 | Grad 0.0275 \n","[2024-04-10 13:11:49,970::train::INFO] [Train] Iter 18241 | Loss 0.325024 | Grad 0.0376 \n","[2024-04-10 13:11:50,091::train::INFO] [Train] Iter 18242 | Loss 0.314456 | Grad 0.0414 \n","[2024-04-10 13:11:50,211::train::INFO] [Train] Iter 18243 | Loss 0.279863 | Grad 0.0276 \n","[2024-04-10 13:11:50,331::train::INFO] [Train] Iter 18244 | Loss 0.280748 | Grad 0.0378 \n","[2024-04-10 13:11:50,452::train::INFO] [Train] Iter 18245 | Loss 0.327115 | Grad 0.0536 \n","[2024-04-10 13:11:50,573::train::INFO] [Train] Iter 18246 | Loss 0.278730 | Grad 0.0425 \n","[2024-04-10 13:11:50,699::train::INFO] [Train] Iter 18247 | Loss 0.275589 | Grad 0.0336 \n","[2024-04-10 13:11:50,826::train::INFO] [Train] Iter 18248 | Loss 0.277013 | Grad 0.0377 \n","[2024-04-10 13:11:50,946::train::INFO] [Train] Iter 18249 | Loss 0.309016 | Grad 0.0472 \n","[2024-04-10 13:11:51,067::train::INFO] [Train] Iter 18250 | Loss 0.354794 | Grad 0.0414 \n","[2024-04-10 13:11:51,189::train::INFO] [Train] Iter 18251 | Loss 0.287388 | Grad 0.0300 \n","[2024-04-10 13:11:51,311::train::INFO] [Train] Iter 18252 | Loss 0.290907 | Grad 0.0427 \n","[2024-04-10 13:11:51,430::train::INFO] [Train] Iter 18253 | Loss 0.293405 | Grad 0.0487 \n","[2024-04-10 13:11:51,553::train::INFO] [Train] Iter 18254 | Loss 0.343246 | Grad 0.0433 \n","[2024-04-10 13:11:51,672::train::INFO] [Train] Iter 18255 | Loss 0.310966 | Grad 0.0414 \n","[2024-04-10 13:11:51,791::train::INFO] [Train] Iter 18256 | Loss 0.334834 | Grad 0.0439 \n","[2024-04-10 13:11:51,911::train::INFO] [Train] Iter 18257 | Loss 0.303840 | Grad 0.0360 \n","[2024-04-10 13:11:52,029::train::INFO] [Train] Iter 18258 | Loss 0.308544 | Grad 0.0406 \n","[2024-04-10 13:11:52,153::train::INFO] [Train] Iter 18259 | Loss 0.294113 | Grad 0.0425 \n","[2024-04-10 13:11:52,271::train::INFO] [Train] Iter 18260 | Loss 0.293190 | Grad 0.0324 \n","[2024-04-10 13:11:52,390::train::INFO] [Train] Iter 18261 | Loss 0.297943 | Grad 0.0303 \n","[2024-04-10 13:11:52,509::train::INFO] [Train] Iter 18262 | Loss 0.360353 | Grad 0.0525 \n","[2024-04-10 13:11:52,629::train::INFO] [Train] Iter 18263 | Loss 0.323532 | Grad 0.0366 \n","[2024-04-10 13:11:52,749::train::INFO] [Train] Iter 18264 | Loss 0.331891 | Grad 0.0577 \n","[2024-04-10 13:11:52,870::train::INFO] [Train] Iter 18265 | Loss 0.307275 | Grad 0.0507 \n","[2024-04-10 13:11:52,990::train::INFO] [Train] Iter 18266 | Loss 0.300847 | Grad 0.0607 \n","[2024-04-10 13:11:53,110::train::INFO] [Train] Iter 18267 | Loss 0.339985 | Grad 0.0670 \n","[2024-04-10 13:11:53,230::train::INFO] [Train] Iter 18268 | Loss 0.308016 | Grad 0.0418 \n","[2024-04-10 13:11:53,352::train::INFO] [Train] Iter 18269 | Loss 0.344235 | Grad 0.0561 \n","[2024-04-10 13:11:53,427::train::INFO] [Train] Iter 18270 | Loss 0.294497 | Grad 0.0537 \n","[2024-04-10 13:11:53,545::train::INFO] [Train] Iter 18271 | Loss 0.297304 | Grad 0.0729 \n","[2024-04-10 13:11:53,664::train::INFO] [Train] Iter 18272 | Loss 0.336166 | Grad 0.0712 \n","[2024-04-10 13:11:53,781::train::INFO] [Train] Iter 18273 | Loss 0.287683 | Grad 0.0443 \n","[2024-04-10 13:11:53,899::train::INFO] [Train] Iter 18274 | Loss 0.320867 | Grad 0.0665 \n","[2024-04-10 13:11:54,018::train::INFO] [Train] Iter 18275 | Loss 0.292322 | Grad 0.0322 \n","[2024-04-10 13:11:54,136::train::INFO] [Train] Iter 18276 | Loss 0.307513 | Grad 0.0497 \n","[2024-04-10 13:11:54,255::train::INFO] [Train] Iter 18277 | Loss 0.305836 | Grad 0.0681 \n","[2024-04-10 13:11:54,373::train::INFO] [Train] Iter 18278 | Loss 0.311393 | Grad 0.0517 \n","[2024-04-10 13:11:54,494::train::INFO] [Train] Iter 18279 | Loss 0.330737 | Grad 0.0496 \n","[2024-04-10 13:11:54,614::train::INFO] [Train] Iter 18280 | Loss 0.295367 | Grad 0.0344 \n","[2024-04-10 13:11:54,735::train::INFO] [Train] Iter 18281 | Loss 0.331337 | Grad 0.0437 \n","[2024-04-10 13:11:54,854::train::INFO] [Train] Iter 18282 | Loss 0.300989 | Grad 0.0404 \n","[2024-04-10 13:11:54,974::train::INFO] [Train] Iter 18283 | Loss 0.310046 | Grad 0.0411 \n","[2024-04-10 13:11:55,094::train::INFO] [Train] Iter 18284 | Loss 0.303228 | Grad 0.0447 \n","[2024-04-10 13:11:55,214::train::INFO] [Train] Iter 18285 | Loss 0.290078 | Grad 0.0358 \n","[2024-04-10 13:11:55,332::train::INFO] [Train] Iter 18286 | Loss 0.307171 | Grad 0.0540 \n","[2024-04-10 13:11:55,451::train::INFO] [Train] Iter 18287 | Loss 0.334496 | Grad 0.0396 \n","[2024-04-10 13:11:55,570::train::INFO] [Train] Iter 18288 | Loss 0.350641 | Grad 0.0480 \n","[2024-04-10 13:11:55,688::train::INFO] [Train] Iter 18289 | Loss 0.288284 | Grad 0.0427 \n","[2024-04-10 13:11:55,808::train::INFO] [Train] Iter 18290 | Loss 0.319011 | Grad 0.0495 \n","[2024-04-10 13:11:55,927::train::INFO] [Train] Iter 18291 | Loss 0.338977 | Grad 0.0560 \n","[2024-04-10 13:11:56,045::train::INFO] [Train] Iter 18292 | Loss 0.314177 | Grad 0.0451 \n","[2024-04-10 13:11:56,165::train::INFO] [Train] Iter 18293 | Loss 0.317205 | Grad 0.0451 \n","[2024-04-10 13:11:56,287::train::INFO] [Train] Iter 18294 | Loss 0.254331 | Grad 0.0307 \n","[2024-04-10 13:11:56,406::train::INFO] [Train] Iter 18295 | Loss 0.317836 | Grad 0.0534 \n","[2024-04-10 13:11:56,525::train::INFO] [Train] Iter 18296 | Loss 0.309925 | Grad 0.0536 \n","[2024-04-10 13:11:56,644::train::INFO] [Train] Iter 18297 | Loss 0.309019 | Grad 0.0563 \n","[2024-04-10 13:11:56,763::train::INFO] [Train] Iter 18298 | Loss 0.324259 | Grad 0.0418 \n","[2024-04-10 13:11:56,882::train::INFO] [Train] Iter 18299 | Loss 0.301837 | Grad 0.0397 \n","[2024-04-10 13:11:57,001::train::INFO] [Train] Iter 18300 | Loss 0.307116 | Grad 0.0337 \n","[2024-04-10 13:11:57,120::train::INFO] [Train] Iter 18301 | Loss 0.288193 | Grad 0.0374 \n","[2024-04-10 13:11:57,239::train::INFO] [Train] Iter 18302 | Loss 0.317636 | Grad 0.0430 \n","[2024-04-10 13:11:57,357::train::INFO] [Train] Iter 18303 | Loss 0.348849 | Grad 0.0427 \n","[2024-04-10 13:11:57,477::train::INFO] [Train] Iter 18304 | Loss 0.345691 | Grad 0.0283 \n","[2024-04-10 13:11:57,600::train::INFO] [Train] Iter 18305 | Loss 0.324664 | Grad 0.0360 \n","[2024-04-10 13:11:57,719::train::INFO] [Train] Iter 18306 | Loss 0.306237 | Grad 0.0407 \n","[2024-04-10 13:11:57,839::train::INFO] [Train] Iter 18307 | Loss 0.312279 | Grad 0.0475 \n","[2024-04-10 13:11:57,959::train::INFO] [Train] Iter 18308 | Loss 0.266843 | Grad 0.0291 \n","[2024-04-10 13:11:58,077::train::INFO] [Train] Iter 18309 | Loss 0.273320 | Grad 0.0349 \n","[2024-04-10 13:11:58,198::train::INFO] [Train] Iter 18310 | Loss 0.277274 | Grad 0.0532 \n","[2024-04-10 13:11:58,318::train::INFO] [Train] Iter 18311 | Loss 0.313915 | Grad 0.0606 \n","[2024-04-10 13:11:58,438::train::INFO] [Train] Iter 18312 | Loss 0.325627 | Grad 0.0615 \n","[2024-04-10 13:11:58,557::train::INFO] [Train] Iter 18313 | Loss 0.321534 | Grad 0.0346 \n","[2024-04-10 13:11:58,677::train::INFO] [Train] Iter 18314 | Loss 0.338094 | Grad 0.0408 \n","[2024-04-10 13:11:58,753::train::INFO] [Train] Iter 18315 | Loss 0.315558 | Grad 0.0417 \n","[2024-04-10 13:11:58,873::train::INFO] [Train] Iter 18316 | Loss 0.333876 | Grad 0.0455 \n","[2024-04-10 13:11:58,990::train::INFO] [Train] Iter 18317 | Loss 0.301209 | Grad 0.0617 \n","[2024-04-10 13:11:59,110::train::INFO] [Train] Iter 18318 | Loss 0.288379 | Grad 0.0614 \n","[2024-04-10 13:11:59,228::train::INFO] [Train] Iter 18319 | Loss 0.332698 | Grad 0.0964 \n","[2024-04-10 13:11:59,349::train::INFO] [Train] Iter 18320 | Loss 0.345508 | Grad 0.0548 \n","[2024-04-10 13:11:59,467::train::INFO] [Train] Iter 18321 | Loss 0.350693 | Grad 0.0476 \n","[2024-04-10 13:11:59,586::train::INFO] [Train] Iter 18322 | Loss 0.310751 | Grad 0.0454 \n","[2024-04-10 13:11:59,705::train::INFO] [Train] Iter 18323 | Loss 0.279851 | Grad 0.0465 \n","[2024-04-10 13:11:59,825::train::INFO] [Train] Iter 18324 | Loss 0.331458 | Grad 0.0505 \n","[2024-04-10 13:11:59,944::train::INFO] [Train] Iter 18325 | Loss 0.307091 | Grad 0.0558 \n","[2024-04-10 13:12:00,063::train::INFO] [Train] Iter 18326 | Loss 0.310354 | Grad 0.0371 \n","[2024-04-10 13:12:00,185::train::INFO] [Train] Iter 18327 | Loss 0.292633 | Grad 0.0474 \n","[2024-04-10 13:12:00,306::train::INFO] [Train] Iter 18328 | Loss 0.330412 | Grad 0.0450 \n","[2024-04-10 13:12:00,425::train::INFO] [Train] Iter 18329 | Loss 0.259523 | Grad 0.0269 \n","[2024-04-10 13:12:00,545::train::INFO] [Train] Iter 18330 | Loss 0.332479 | Grad 0.0363 \n","[2024-04-10 13:12:00,663::train::INFO] [Train] Iter 18331 | Loss 0.374955 | Grad 0.0526 \n","[2024-04-10 13:12:00,782::train::INFO] [Train] Iter 18332 | Loss 0.348677 | Grad 0.0443 \n","[2024-04-10 13:12:00,901::train::INFO] [Train] Iter 18333 | Loss 0.319550 | Grad 0.0467 \n","[2024-04-10 13:12:01,027::train::INFO] [Train] Iter 18334 | Loss 0.284524 | Grad 0.0322 \n","[2024-04-10 13:12:01,146::train::INFO] [Train] Iter 18335 | Loss 0.332839 | Grad 0.0513 \n","[2024-04-10 13:12:01,265::train::INFO] [Train] Iter 18336 | Loss 0.261564 | Grad 0.0319 \n","[2024-04-10 13:12:01,386::train::INFO] [Train] Iter 18337 | Loss 0.312906 | Grad 0.0473 \n","[2024-04-10 13:12:01,508::train::INFO] [Train] Iter 18338 | Loss 0.353335 | Grad 0.0401 \n","[2024-04-10 13:12:01,630::train::INFO] [Train] Iter 18339 | Loss 0.312808 | Grad 0.0368 \n","[2024-04-10 13:12:01,750::train::INFO] [Train] Iter 18340 | Loss 0.312853 | Grad 0.0321 \n","[2024-04-10 13:12:01,872::train::INFO] [Train] Iter 18341 | Loss 0.278487 | Grad 0.0538 \n","[2024-04-10 13:12:01,994::train::INFO] [Train] Iter 18342 | Loss 0.293840 | Grad 0.0474 \n","[2024-04-10 13:12:02,115::train::INFO] [Train] Iter 18343 | Loss 0.337713 | Grad 0.0383 \n","[2024-04-10 13:12:02,239::train::INFO] [Train] Iter 18344 | Loss 0.298550 | Grad 0.0444 \n","[2024-04-10 13:12:02,363::train::INFO] [Train] Iter 18345 | Loss 0.288017 | Grad 0.0420 \n","[2024-04-10 13:12:02,486::train::INFO] [Train] Iter 18346 | Loss 0.328035 | Grad 0.0514 \n","[2024-04-10 13:12:02,608::train::INFO] [Train] Iter 18347 | Loss 0.299331 | Grad 0.0356 \n","[2024-04-10 13:12:02,729::train::INFO] [Train] Iter 18348 | Loss 0.296489 | Grad 0.0364 \n","[2024-04-10 13:12:02,850::train::INFO] [Train] Iter 18349 | Loss 0.328686 | Grad 0.0357 \n","[2024-04-10 13:12:02,971::train::INFO] [Train] Iter 18350 | Loss 0.298509 | Grad 0.0290 \n","[2024-04-10 13:12:03,091::train::INFO] [Train] Iter 18351 | Loss 0.299149 | Grad 0.0378 \n","[2024-04-10 13:12:03,212::train::INFO] [Train] Iter 18352 | Loss 0.323436 | Grad 0.0380 \n","[2024-04-10 13:12:03,333::train::INFO] [Train] Iter 18353 | Loss 0.290327 | Grad 0.0357 \n","[2024-04-10 13:12:03,454::train::INFO] [Train] Iter 18354 | Loss 0.314812 | Grad 0.0417 \n","[2024-04-10 13:12:03,577::train::INFO] [Train] Iter 18355 | Loss 0.339573 | Grad 0.0463 \n","[2024-04-10 13:12:03,696::train::INFO] [Train] Iter 18356 | Loss 0.318961 | Grad 0.0450 \n","[2024-04-10 13:12:03,818::train::INFO] [Train] Iter 18357 | Loss 0.302851 | Grad 0.0647 \n","[2024-04-10 13:12:03,940::train::INFO] [Train] Iter 18358 | Loss 0.321480 | Grad 0.0549 \n","[2024-04-10 13:12:04,063::train::INFO] [Train] Iter 18359 | Loss 0.358379 | Grad 0.0482 \n","[2024-04-10 13:12:04,138::train::INFO] [Train] Iter 18360 | Loss 0.321936 | Grad 0.0534 \n","[2024-04-10 13:12:04,258::train::INFO] [Train] Iter 18361 | Loss 0.344164 | Grad 0.0591 \n","[2024-04-10 13:12:04,379::train::INFO] [Train] Iter 18362 | Loss 0.315080 | Grad 0.0397 \n","[2024-04-10 13:12:04,499::train::INFO] [Train] Iter 18363 | Loss 0.294135 | Grad 0.0551 \n","[2024-04-10 13:12:04,617::train::INFO] [Train] Iter 18364 | Loss 0.328937 | Grad 0.0565 \n","[2024-04-10 13:12:04,736::train::INFO] [Train] Iter 18365 | Loss 0.303223 | Grad 0.0560 \n","[2024-04-10 13:12:04,855::train::INFO] [Train] Iter 18366 | Loss 0.317980 | Grad 0.0346 \n","[2024-04-10 13:12:04,975::train::INFO] [Train] Iter 18367 | Loss 0.343233 | Grad 0.0444 \n","[2024-04-10 13:12:05,095::train::INFO] [Train] Iter 18368 | Loss 0.324406 | Grad 0.0518 \n","[2024-04-10 13:12:05,216::train::INFO] [Train] Iter 18369 | Loss 0.288298 | Grad 0.0405 \n","[2024-04-10 13:12:05,341::train::INFO] [Train] Iter 18370 | Loss 0.289624 | Grad 0.0397 \n","[2024-04-10 13:12:05,460::train::INFO] [Train] Iter 18371 | Loss 0.305780 | Grad 0.0531 \n","[2024-04-10 13:12:05,579::train::INFO] [Train] Iter 18372 | Loss 0.287008 | Grad 0.0480 \n","[2024-04-10 13:12:05,700::train::INFO] [Train] Iter 18373 | Loss 0.302993 | Grad 0.0403 \n","[2024-04-10 13:12:05,819::train::INFO] [Train] Iter 18374 | Loss 0.287387 | Grad 0.0301 \n","[2024-04-10 13:12:05,940::train::INFO] [Train] Iter 18375 | Loss 0.355689 | Grad 0.0483 \n","[2024-04-10 13:12:06,060::train::INFO] [Train] Iter 18376 | Loss 0.323455 | Grad 0.0428 \n","[2024-04-10 13:12:06,180::train::INFO] [Train] Iter 18377 | Loss 0.323854 | Grad 0.0511 \n","[2024-04-10 13:12:06,301::train::INFO] [Train] Iter 18378 | Loss 0.338004 | Grad 0.0492 \n","[2024-04-10 13:12:06,421::train::INFO] [Train] Iter 18379 | Loss 0.305925 | Grad 0.0374 \n","[2024-04-10 13:12:06,546::train::INFO] [Train] Iter 18380 | Loss 0.276262 | Grad 0.0400 \n","[2024-04-10 13:12:06,664::train::INFO] [Train] Iter 18381 | Loss 0.281094 | Grad 0.0451 \n","[2024-04-10 13:12:06,783::train::INFO] [Train] Iter 18382 | Loss 0.325934 | Grad 0.0497 \n","[2024-04-10 13:12:06,902::train::INFO] [Train] Iter 18383 | Loss 0.296550 | Grad 0.0413 \n","[2024-04-10 13:12:07,023::train::INFO] [Train] Iter 18384 | Loss 0.291389 | Grad 0.0446 \n","[2024-04-10 13:12:07,142::train::INFO] [Train] Iter 18385 | Loss 0.335065 | Grad 0.0365 \n","[2024-04-10 13:12:07,267::train::INFO] [Train] Iter 18386 | Loss 0.238711 | Grad 0.0227 \n","[2024-04-10 13:12:07,385::train::INFO] [Train] Iter 18387 | Loss 0.323096 | Grad 0.0703 \n","[2024-04-10 13:12:07,504::train::INFO] [Train] Iter 18388 | Loss 0.309435 | Grad 0.0504 \n","[2024-04-10 13:12:07,624::train::INFO] [Train] Iter 18389 | Loss 0.330716 | Grad 0.0591 \n","[2024-04-10 13:12:07,744::train::INFO] [Train] Iter 18390 | Loss 0.329329 | Grad 0.0390 \n","[2024-04-10 13:12:07,865::train::INFO] [Train] Iter 18391 | Loss 0.245857 | Grad 0.0296 \n","[2024-04-10 13:12:07,984::train::INFO] [Train] Iter 18392 | Loss 0.343751 | Grad 0.0473 \n","[2024-04-10 13:12:08,103::train::INFO] [Train] Iter 18393 | Loss 0.339839 | Grad 0.0502 \n","[2024-04-10 13:12:08,224::train::INFO] [Train] Iter 18394 | Loss 0.320807 | Grad 0.0343 \n","[2024-04-10 13:12:08,345::train::INFO] [Train] Iter 18395 | Loss 0.333372 | Grad 0.0378 \n","[2024-04-10 13:12:08,464::train::INFO] [Train] Iter 18396 | Loss 0.339164 | Grad 0.0426 \n","[2024-04-10 13:12:08,583::train::INFO] [Train] Iter 18397 | Loss 0.358757 | Grad 0.0420 \n","[2024-04-10 13:12:08,703::train::INFO] [Train] Iter 18398 | Loss 0.313165 | Grad 0.0442 \n","[2024-04-10 13:12:08,822::train::INFO] [Train] Iter 18399 | Loss 0.295396 | Grad 0.0402 \n","[2024-04-10 13:12:08,941::train::INFO] [Train] Iter 18400 | Loss 0.277901 | Grad 0.0315 \n","[2024-04-10 13:12:09,059::train::INFO] [Train] Iter 18401 | Loss 0.366087 | Grad 0.0435 \n","[2024-04-10 13:12:09,177::train::INFO] [Train] Iter 18402 | Loss 0.315639 | Grad 0.0526 \n","[2024-04-10 13:12:09,296::train::INFO] [Train] Iter 18403 | Loss 0.318256 | Grad 0.0478 \n","[2024-04-10 13:12:09,415::train::INFO] [Train] Iter 18404 | Loss 0.336130 | Grad 0.0613 \n","[2024-04-10 13:12:09,490::train::INFO] [Train] Iter 18405 | Loss 0.321649 | Grad 0.0487 \n","[2024-04-10 13:12:09,610::train::INFO] [Train] Iter 18406 | Loss 0.304701 | Grad 0.0471 \n","[2024-04-10 13:12:09,728::train::INFO] [Train] Iter 18407 | Loss 0.303223 | Grad 0.0481 \n","[2024-04-10 13:12:09,847::train::INFO] [Train] Iter 18408 | Loss 0.280788 | Grad 0.0310 \n","[2024-04-10 13:12:09,966::train::INFO] [Train] Iter 18409 | Loss 0.259678 | Grad 0.0416 \n","[2024-04-10 13:12:10,085::train::INFO] [Train] Iter 18410 | Loss 0.310141 | Grad 0.0342 \n","[2024-04-10 13:12:10,202::train::INFO] [Train] Iter 18411 | Loss 0.343990 | Grad 0.0581 \n","[2024-04-10 13:12:10,319::train::INFO] [Train] Iter 18412 | Loss 0.351263 | Grad 0.0585 \n","[2024-04-10 13:12:10,439::train::INFO] [Train] Iter 18413 | Loss 0.337901 | Grad 0.0437 \n","[2024-04-10 13:12:10,559::train::INFO] [Train] Iter 18414 | Loss 0.311452 | Grad 0.0439 \n","[2024-04-10 13:12:10,681::train::INFO] [Train] Iter 18415 | Loss 0.349619 | Grad 0.0606 \n","[2024-04-10 13:12:10,800::train::INFO] [Train] Iter 18416 | Loss 0.327414 | Grad 0.0673 \n","[2024-04-10 13:12:10,919::train::INFO] [Train] Iter 18417 | Loss 0.310406 | Grad 0.0585 \n","[2024-04-10 13:12:11,039::train::INFO] [Train] Iter 18418 | Loss 0.312521 | Grad 0.0457 \n","[2024-04-10 13:12:11,157::train::INFO] [Train] Iter 18419 | Loss 0.303940 | Grad 0.0324 \n","[2024-04-10 13:12:11,277::train::INFO] [Train] Iter 18420 | Loss 0.313925 | Grad 0.0293 \n","[2024-04-10 13:12:11,397::train::INFO] [Train] Iter 18421 | Loss 0.316040 | Grad 0.0362 \n","[2024-04-10 13:12:11,516::train::INFO] [Train] Iter 18422 | Loss 0.344523 | Grad 0.0444 \n","[2024-04-10 13:12:11,635::train::INFO] [Train] Iter 18423 | Loss 0.292141 | Grad 0.0540 \n","[2024-04-10 13:12:11,756::train::INFO] [Train] Iter 18424 | Loss 0.318826 | Grad 0.0476 \n","[2024-04-10 13:12:11,875::train::INFO] [Train] Iter 18425 | Loss 0.316094 | Grad 0.0549 \n","[2024-04-10 13:12:11,994::train::INFO] [Train] Iter 18426 | Loss 0.347084 | Grad 0.0551 \n","[2024-04-10 13:12:12,112::train::INFO] [Train] Iter 18427 | Loss 0.299193 | Grad 0.0640 \n","[2024-04-10 13:12:12,230::train::INFO] [Train] Iter 18428 | Loss 0.326183 | Grad 0.0531 \n","[2024-04-10 13:12:12,349::train::INFO] [Train] Iter 18429 | Loss 0.276219 | Grad 0.0361 \n","[2024-04-10 13:12:12,469::train::INFO] [Train] Iter 18430 | Loss 0.348610 | Grad 0.0487 \n","[2024-04-10 13:12:12,589::train::INFO] [Train] Iter 18431 | Loss 0.290100 | Grad 0.0305 \n","[2024-04-10 13:12:12,713::train::INFO] [Train] Iter 18432 | Loss 0.297596 | Grad 0.0330 \n","[2024-04-10 13:12:12,833::train::INFO] [Train] Iter 18433 | Loss 0.377739 | Grad 0.0502 \n","[2024-04-10 13:12:12,953::train::INFO] [Train] Iter 18434 | Loss 0.340743 | Grad 0.0616 \n","[2024-04-10 13:12:13,073::train::INFO] [Train] Iter 18435 | Loss 0.294962 | Grad 0.0580 \n","[2024-04-10 13:12:13,193::train::INFO] [Train] Iter 18436 | Loss 0.319753 | Grad 0.0383 \n","[2024-04-10 13:12:13,314::train::INFO] [Train] Iter 18437 | Loss 0.311086 | Grad 0.0605 \n","[2024-04-10 13:12:13,434::train::INFO] [Train] Iter 18438 | Loss 0.312728 | Grad 0.0346 \n","[2024-04-10 13:12:13,554::train::INFO] [Train] Iter 18439 | Loss 0.334041 | Grad 0.0396 \n","[2024-04-10 13:12:13,674::train::INFO] [Train] Iter 18440 | Loss 0.315716 | Grad 0.0456 \n","[2024-04-10 13:12:13,793::train::INFO] [Train] Iter 18441 | Loss 0.290568 | Grad 0.0429 \n","[2024-04-10 13:12:13,914::train::INFO] [Train] Iter 18442 | Loss 0.316262 | Grad 0.0500 \n","[2024-04-10 13:12:14,032::train::INFO] [Train] Iter 18443 | Loss 0.330840 | Grad 0.0442 \n","[2024-04-10 13:12:14,151::train::INFO] [Train] Iter 18444 | Loss 0.304274 | Grad 0.0395 \n","[2024-04-10 13:12:14,270::train::INFO] [Train] Iter 18445 | Loss 0.291921 | Grad 0.0283 \n","[2024-04-10 13:12:14,389::train::INFO] [Train] Iter 18446 | Loss 0.322551 | Grad 0.0365 \n","[2024-04-10 13:12:14,509::train::INFO] [Train] Iter 18447 | Loss 0.350253 | Grad 0.0789 \n","[2024-04-10 13:12:14,631::train::INFO] [Train] Iter 18448 | Loss 0.308426 | Grad 0.0434 \n","[2024-04-10 13:12:14,753::train::INFO] [Train] Iter 18449 | Loss 0.296713 | Grad 0.0560 \n","[2024-04-10 13:12:14,831::train::INFO] [Train] Iter 18450 | Loss 0.266363 | Grad 0.0471 \n","[2024-04-10 13:12:14,952::train::INFO] [Train] Iter 18451 | Loss 0.294225 | Grad 0.0425 \n","[2024-04-10 13:12:15,074::train::INFO] [Train] Iter 18452 | Loss 0.333412 | Grad 0.0491 \n","[2024-04-10 13:12:15,197::train::INFO] [Train] Iter 18453 | Loss 0.287659 | Grad 0.0512 \n","[2024-04-10 13:12:15,317::train::INFO] [Train] Iter 18454 | Loss 0.300892 | Grad 0.0937 \n","[2024-04-10 13:12:15,437::train::INFO] [Train] Iter 18455 | Loss 0.299700 | Grad 0.0559 \n","[2024-04-10 13:12:15,559::train::INFO] [Train] Iter 18456 | Loss 0.329880 | Grad 0.0456 \n","[2024-04-10 13:12:15,680::train::INFO] [Train] Iter 18457 | Loss 0.325942 | Grad 0.0447 \n","[2024-04-10 13:12:15,802::train::INFO] [Train] Iter 18458 | Loss 0.285267 | Grad 0.0273 \n","[2024-04-10 13:12:15,925::train::INFO] [Train] Iter 18459 | Loss 0.318392 | Grad 0.0578 \n","[2024-04-10 13:12:16,048::train::INFO] [Train] Iter 18460 | Loss 0.309473 | Grad 0.0492 \n","[2024-04-10 13:12:16,170::train::INFO] [Train] Iter 18461 | Loss 0.320323 | Grad 0.0319 \n","[2024-04-10 13:12:16,291::train::INFO] [Train] Iter 18462 | Loss 0.342888 | Grad 0.0530 \n","[2024-04-10 13:12:16,413::train::INFO] [Train] Iter 18463 | Loss 0.323276 | Grad 0.0499 \n","[2024-04-10 13:12:16,536::train::INFO] [Train] Iter 18464 | Loss 0.326478 | Grad 0.0669 \n","[2024-04-10 13:12:16,657::train::INFO] [Train] Iter 18465 | Loss 0.318960 | Grad 0.0372 \n","[2024-04-10 13:12:16,779::train::INFO] [Train] Iter 18466 | Loss 0.353585 | Grad 0.0479 \n","[2024-04-10 13:12:16,902::train::INFO] [Train] Iter 18467 | Loss 0.306318 | Grad 0.0374 \n","[2024-04-10 13:12:17,025::train::INFO] [Train] Iter 18468 | Loss 0.324497 | Grad 0.0622 \n","[2024-04-10 13:12:17,147::train::INFO] [Train] Iter 18469 | Loss 0.305267 | Grad 0.0276 \n","[2024-04-10 13:12:17,271::train::INFO] [Train] Iter 18470 | Loss 0.317122 | Grad 0.0400 \n","[2024-04-10 13:12:17,391::train::INFO] [Train] Iter 18471 | Loss 0.319213 | Grad 0.0416 \n","[2024-04-10 13:12:17,512::train::INFO] [Train] Iter 18472 | Loss 0.332610 | Grad 0.0258 \n","[2024-04-10 13:12:17,631::train::INFO] [Train] Iter 18473 | Loss 0.307535 | Grad 0.0355 \n","[2024-04-10 13:12:17,751::train::INFO] [Train] Iter 18474 | Loss 0.298908 | Grad 0.0300 \n","[2024-04-10 13:12:17,869::train::INFO] [Train] Iter 18475 | Loss 0.296521 | Grad 0.0312 \n","[2024-04-10 13:12:17,987::train::INFO] [Train] Iter 18476 | Loss 0.285418 | Grad 0.0317 \n","[2024-04-10 13:12:18,105::train::INFO] [Train] Iter 18477 | Loss 0.342058 | Grad 0.0478 \n","[2024-04-10 13:12:18,225::train::INFO] [Train] Iter 18478 | Loss 0.355719 | Grad 0.0511 \n","[2024-04-10 13:12:18,344::train::INFO] [Train] Iter 18479 | Loss 0.318861 | Grad 0.0328 \n","[2024-04-10 13:12:18,465::train::INFO] [Train] Iter 18480 | Loss 0.349156 | Grad 0.0520 \n","[2024-04-10 13:12:18,585::train::INFO] [Train] Iter 18481 | Loss 0.296738 | Grad 0.0319 \n","[2024-04-10 13:12:18,706::train::INFO] [Train] Iter 18482 | Loss 0.296159 | Grad 0.0445 \n","[2024-04-10 13:12:18,826::train::INFO] [Train] Iter 18483 | Loss 0.303416 | Grad 0.0375 \n","[2024-04-10 13:12:18,951::train::INFO] [Train] Iter 18484 | Loss 0.322791 | Grad 0.0532 \n","[2024-04-10 13:12:19,070::train::INFO] [Train] Iter 18485 | Loss 0.291959 | Grad 0.0251 \n","[2024-04-10 13:12:19,190::train::INFO] [Train] Iter 18486 | Loss 0.286041 | Grad 0.0269 \n","[2024-04-10 13:12:19,309::train::INFO] [Train] Iter 18487 | Loss 0.306898 | Grad 0.0380 \n","[2024-04-10 13:12:19,429::train::INFO] [Train] Iter 18488 | Loss 0.334830 | Grad 0.0617 \n","[2024-04-10 13:12:19,552::train::INFO] [Train] Iter 18489 | Loss 0.315491 | Grad 0.0356 \n","[2024-04-10 13:12:19,671::train::INFO] [Train] Iter 18490 | Loss 0.293375 | Grad 0.0502 \n","[2024-04-10 13:12:19,790::train::INFO] [Train] Iter 18491 | Loss 0.301991 | Grad 0.0266 \n","[2024-04-10 13:12:19,910::train::INFO] [Train] Iter 18492 | Loss 0.313463 | Grad 0.0447 \n","[2024-04-10 13:12:20,031::train::INFO] [Train] Iter 18493 | Loss 0.281428 | Grad 0.0273 \n","[2024-04-10 13:12:20,150::train::INFO] [Train] Iter 18494 | Loss 0.368823 | Grad 0.0621 \n","[2024-04-10 13:12:20,227::train::INFO] [Train] Iter 18495 | Loss 0.424043 | Grad 0.0531 \n","[2024-04-10 13:12:20,346::train::INFO] [Train] Iter 18496 | Loss 0.288920 | Grad 0.0275 \n","[2024-04-10 13:12:20,466::train::INFO] [Train] Iter 18497 | Loss 0.289593 | Grad 0.0320 \n","[2024-04-10 13:12:20,584::train::INFO] [Train] Iter 18498 | Loss 0.324046 | Grad 0.0643 \n","[2024-04-10 13:12:20,703::train::INFO] [Train] Iter 18499 | Loss 0.306210 | Grad 0.0427 \n","[2024-04-10 13:12:20,822::train::INFO] [Train] Iter 18500 | Loss 0.316460 | Grad 0.0578 \n","[2024-04-10 13:12:20,942::train::INFO] [Train] Iter 18501 | Loss 0.330557 | Grad 0.0507 \n","[2024-04-10 13:12:21,061::train::INFO] [Train] Iter 18502 | Loss 0.304267 | Grad 0.0383 \n","[2024-04-10 13:12:21,180::train::INFO] [Train] Iter 18503 | Loss 0.305072 | Grad 0.0624 \n","[2024-04-10 13:12:21,301::train::INFO] [Train] Iter 18504 | Loss 0.353445 | Grad 0.0413 \n","[2024-04-10 13:12:21,420::train::INFO] [Train] Iter 18505 | Loss 0.325479 | Grad 0.0332 \n","[2024-04-10 13:12:21,541::train::INFO] [Train] Iter 18506 | Loss 0.304125 | Grad 0.0457 \n","[2024-04-10 13:12:21,662::train::INFO] [Train] Iter 18507 | Loss 0.278896 | Grad 0.0448 \n","[2024-04-10 13:12:21,783::train::INFO] [Train] Iter 18508 | Loss 0.298344 | Grad 0.0501 \n","[2024-04-10 13:12:21,902::train::INFO] [Train] Iter 18509 | Loss 0.341003 | Grad 0.0653 \n","[2024-04-10 13:12:22,022::train::INFO] [Train] Iter 18510 | Loss 0.337677 | Grad 0.0839 \n","[2024-04-10 13:12:22,141::train::INFO] [Train] Iter 18511 | Loss 0.329820 | Grad 0.0391 \n","[2024-04-10 13:12:22,262::train::INFO] [Train] Iter 18512 | Loss 0.332478 | Grad 0.0482 \n","[2024-04-10 13:12:22,380::train::INFO] [Train] Iter 18513 | Loss 0.271937 | Grad 0.0449 \n","[2024-04-10 13:12:22,498::train::INFO] [Train] Iter 18514 | Loss 0.312811 | Grad 0.0468 \n","[2024-04-10 13:12:22,617::train::INFO] [Train] Iter 18515 | Loss 0.274214 | Grad 0.0368 \n","[2024-04-10 13:12:22,736::train::INFO] [Train] Iter 18516 | Loss 0.321594 | Grad 0.0456 \n","[2024-04-10 13:12:22,856::train::INFO] [Train] Iter 18517 | Loss 0.309273 | Grad 0.0354 \n","[2024-04-10 13:12:22,975::train::INFO] [Train] Iter 18518 | Loss 0.295123 | Grad 0.0459 \n","[2024-04-10 13:12:23,093::train::INFO] [Train] Iter 18519 | Loss 0.320481 | Grad 0.0457 \n","[2024-04-10 13:12:23,213::train::INFO] [Train] Iter 18520 | Loss 0.339779 | Grad 0.0629 \n","[2024-04-10 13:12:23,333::train::INFO] [Train] Iter 18521 | Loss 0.339747 | Grad 0.0742 \n","[2024-04-10 13:12:23,452::train::INFO] [Train] Iter 18522 | Loss 0.307594 | Grad 0.0308 \n","[2024-04-10 13:12:23,573::train::INFO] [Train] Iter 18523 | Loss 0.338500 | Grad 0.0440 \n","[2024-04-10 13:12:23,693::train::INFO] [Train] Iter 18524 | Loss 0.342884 | Grad 0.0510 \n","[2024-04-10 13:12:23,812::train::INFO] [Train] Iter 18525 | Loss 0.329056 | Grad 0.0526 \n","[2024-04-10 13:12:23,930::train::INFO] [Train] Iter 18526 | Loss 0.306733 | Grad 0.0467 \n","[2024-04-10 13:12:24,052::train::INFO] [Train] Iter 18527 | Loss 0.289895 | Grad 0.0445 \n","[2024-04-10 13:12:24,172::train::INFO] [Train] Iter 18528 | Loss 0.318559 | Grad 0.0408 \n","[2024-04-10 13:12:24,293::train::INFO] [Train] Iter 18529 | Loss 0.307994 | Grad 0.0334 \n","[2024-04-10 13:12:24,413::train::INFO] [Train] Iter 18530 | Loss 0.332337 | Grad 0.0284 \n","[2024-04-10 13:12:24,533::train::INFO] [Train] Iter 18531 | Loss 0.300621 | Grad 0.0339 \n","[2024-04-10 13:12:24,653::train::INFO] [Train] Iter 18532 | Loss 0.324718 | Grad 0.0389 \n","[2024-04-10 13:12:24,772::train::INFO] [Train] Iter 18533 | Loss 0.296427 | Grad 0.0637 \n","[2024-04-10 13:12:24,893::train::INFO] [Train] Iter 18534 | Loss 0.327266 | Grad 0.0286 \n","[2024-04-10 13:12:25,013::train::INFO] [Train] Iter 18535 | Loss 0.319414 | Grad 0.0551 \n","[2024-04-10 13:12:25,132::train::INFO] [Train] Iter 18536 | Loss 0.319185 | Grad 0.0496 \n","[2024-04-10 13:12:25,252::train::INFO] [Train] Iter 18537 | Loss 0.324330 | Grad 0.0453 \n","[2024-04-10 13:12:25,370::train::INFO] [Train] Iter 18538 | Loss 0.315609 | Grad 0.0457 \n","[2024-04-10 13:12:25,488::train::INFO] [Train] Iter 18539 | Loss 0.336507 | Grad 0.0566 \n","[2024-04-10 13:12:25,563::train::INFO] [Train] Iter 18540 | Loss 0.279652 | Grad 0.0459 \n","[2024-04-10 13:12:25,682::train::INFO] [Train] Iter 18541 | Loss 0.312142 | Grad 0.0267 \n","[2024-04-10 13:12:25,802::train::INFO] [Train] Iter 18542 | Loss 0.336624 | Grad 0.0430 \n","[2024-04-10 13:12:25,922::train::INFO] [Train] Iter 18543 | Loss 0.335522 | Grad 0.0481 \n","[2024-04-10 13:12:26,042::train::INFO] [Train] Iter 18544 | Loss 0.283431 | Grad 0.0430 \n","[2024-04-10 13:12:26,162::train::INFO] [Train] Iter 18545 | Loss 0.307845 | Grad 0.0731 \n","[2024-04-10 13:12:26,282::train::INFO] [Train] Iter 18546 | Loss 0.281060 | Grad 0.0326 \n","[2024-04-10 13:12:26,401::train::INFO] [Train] Iter 18547 | Loss 0.323942 | Grad 0.0468 \n","[2024-04-10 13:12:26,520::train::INFO] [Train] Iter 18548 | Loss 0.344081 | Grad 0.0452 \n","[2024-04-10 13:12:26,638::train::INFO] [Train] Iter 18549 | Loss 0.331353 | Grad 0.0366 \n","[2024-04-10 13:12:26,757::train::INFO] [Train] Iter 18550 | Loss 0.332267 | Grad 0.0447 \n","[2024-04-10 13:12:26,876::train::INFO] [Train] Iter 18551 | Loss 0.311600 | Grad 0.0426 \n","[2024-04-10 13:12:26,995::train::INFO] [Train] Iter 18552 | Loss 0.328215 | Grad 0.0540 \n","[2024-04-10 13:12:27,115::train::INFO] [Train] Iter 18553 | Loss 0.322967 | Grad 0.0360 \n","[2024-04-10 13:12:27,234::train::INFO] [Train] Iter 18554 | Loss 0.317868 | Grad 0.0511 \n","[2024-04-10 13:12:27,353::train::INFO] [Train] Iter 18555 | Loss 0.303020 | Grad 0.0367 \n","[2024-04-10 13:12:27,472::train::INFO] [Train] Iter 18556 | Loss 0.312711 | Grad 0.0466 \n","[2024-04-10 13:12:27,592::train::INFO] [Train] Iter 18557 | Loss 0.328549 | Grad 0.0450 \n","[2024-04-10 13:12:27,716::train::INFO] [Train] Iter 18558 | Loss 0.310057 | Grad 0.0476 \n","[2024-04-10 13:12:27,838::train::INFO] [Train] Iter 18559 | Loss 0.307192 | Grad 0.0351 \n","[2024-04-10 13:12:27,958::train::INFO] [Train] Iter 18560 | Loss 0.342039 | Grad 0.0395 \n","[2024-04-10 13:12:28,077::train::INFO] [Train] Iter 18561 | Loss 0.301437 | Grad 0.0360 \n","[2024-04-10 13:12:28,198::train::INFO] [Train] Iter 18562 | Loss 0.326321 | Grad 0.0520 \n","[2024-04-10 13:12:28,320::train::INFO] [Train] Iter 18563 | Loss 0.348154 | Grad 0.0387 \n","[2024-04-10 13:12:28,444::train::INFO] [Train] Iter 18564 | Loss 0.321210 | Grad 0.0434 \n","[2024-04-10 13:12:28,564::train::INFO] [Train] Iter 18565 | Loss 0.334026 | Grad 0.0493 \n","[2024-04-10 13:12:28,685::train::INFO] [Train] Iter 18566 | Loss 0.325938 | Grad 0.0525 \n","[2024-04-10 13:12:28,811::train::INFO] [Train] Iter 18567 | Loss 0.288456 | Grad 0.0311 \n","[2024-04-10 13:12:28,932::train::INFO] [Train] Iter 18568 | Loss 0.362440 | Grad 0.0354 \n","[2024-04-10 13:12:29,054::train::INFO] [Train] Iter 18569 | Loss 0.329683 | Grad 0.0438 \n","[2024-04-10 13:12:29,175::train::INFO] [Train] Iter 18570 | Loss 0.316382 | Grad 0.0329 \n","[2024-04-10 13:12:29,297::train::INFO] [Train] Iter 18571 | Loss 0.288568 | Grad 0.0337 \n","[2024-04-10 13:12:29,420::train::INFO] [Train] Iter 18572 | Loss 0.337019 | Grad 0.0453 \n","[2024-04-10 13:12:29,552::train::INFO] [Train] Iter 18573 | Loss 0.285220 | Grad 0.0404 \n","[2024-04-10 13:12:29,673::train::INFO] [Train] Iter 18574 | Loss 0.304960 | Grad 0.0635 \n","[2024-04-10 13:12:29,794::train::INFO] [Train] Iter 18575 | Loss 0.304463 | Grad 0.0402 \n","[2024-04-10 13:12:29,916::train::INFO] [Train] Iter 18576 | Loss 0.286629 | Grad 0.0297 \n","[2024-04-10 13:12:30,039::train::INFO] [Train] Iter 18577 | Loss 0.324442 | Grad 0.0426 \n","[2024-04-10 13:12:30,160::train::INFO] [Train] Iter 18578 | Loss 0.348916 | Grad 0.0724 \n","[2024-04-10 13:12:30,282::train::INFO] [Train] Iter 18579 | Loss 0.300730 | Grad 0.0498 \n","[2024-04-10 13:12:30,406::train::INFO] [Train] Iter 18580 | Loss 0.292548 | Grad 0.0476 \n","[2024-04-10 13:12:30,535::train::INFO] [Train] Iter 18581 | Loss 0.320249 | Grad 0.0510 \n","[2024-04-10 13:12:30,659::train::INFO] [Train] Iter 18582 | Loss 0.323206 | Grad 0.0455 \n","[2024-04-10 13:12:30,778::train::INFO] [Train] Iter 18583 | Loss 0.339820 | Grad 0.0340 \n","[2024-04-10 13:12:30,897::train::INFO] [Train] Iter 18584 | Loss 0.345957 | Grad 0.0506 \n","[2024-04-10 13:12:30,970::train::INFO] [Train] Iter 18585 | Loss 0.289618 | Grad 0.0453 \n","[2024-04-10 13:12:31,089::train::INFO] [Train] Iter 18586 | Loss 0.310379 | Grad 0.0360 \n","[2024-04-10 13:12:31,208::train::INFO] [Train] Iter 18587 | Loss 0.295226 | Grad 0.0278 \n","[2024-04-10 13:12:31,327::train::INFO] [Train] Iter 18588 | Loss 0.341537 | Grad 0.0665 \n","[2024-04-10 13:12:31,447::train::INFO] [Train] Iter 18589 | Loss 0.327435 | Grad 0.0358 \n","[2024-04-10 13:12:31,567::train::INFO] [Train] Iter 18590 | Loss 0.336122 | Grad 0.0453 \n","[2024-04-10 13:12:31,688::train::INFO] [Train] Iter 18591 | Loss 0.327651 | Grad 0.0591 \n","[2024-04-10 13:12:31,807::train::INFO] [Train] Iter 18592 | Loss 0.328759 | Grad 0.0537 \n","[2024-04-10 13:12:31,926::train::INFO] [Train] Iter 18593 | Loss 0.332632 | Grad 0.0490 \n","[2024-04-10 13:12:32,044::train::INFO] [Train] Iter 18594 | Loss 0.292106 | Grad 0.0322 \n","[2024-04-10 13:12:32,163::train::INFO] [Train] Iter 18595 | Loss 0.323880 | Grad 0.0337 \n","[2024-04-10 13:12:32,282::train::INFO] [Train] Iter 18596 | Loss 0.351192 | Grad 0.0449 \n","[2024-04-10 13:12:32,401::train::INFO] [Train] Iter 18597 | Loss 0.304506 | Grad 0.0554 \n","[2024-04-10 13:12:32,521::train::INFO] [Train] Iter 18598 | Loss 0.312259 | Grad 0.0357 \n","[2024-04-10 13:12:32,641::train::INFO] [Train] Iter 18599 | Loss 0.292947 | Grad 0.0459 \n","[2024-04-10 13:12:32,761::train::INFO] [Train] Iter 18600 | Loss 0.327824 | Grad 0.0665 \n","[2024-04-10 13:12:32,881::train::INFO] [Train] Iter 18601 | Loss 0.274110 | Grad 0.0275 \n","[2024-04-10 13:12:33,001::train::INFO] [Train] Iter 18602 | Loss 0.317170 | Grad 0.0475 \n","[2024-04-10 13:12:33,120::train::INFO] [Train] Iter 18603 | Loss 0.298913 | Grad 0.0387 \n","[2024-04-10 13:12:33,242::train::INFO] [Train] Iter 18604 | Loss 0.324819 | Grad 0.0358 \n","[2024-04-10 13:12:33,363::train::INFO] [Train] Iter 18605 | Loss 0.292116 | Grad 0.0399 \n","[2024-04-10 13:12:33,483::train::INFO] [Train] Iter 18606 | Loss 0.297753 | Grad 0.0360 \n","[2024-04-10 13:12:33,602::train::INFO] [Train] Iter 18607 | Loss 0.267549 | Grad 0.0266 \n","[2024-04-10 13:12:33,722::train::INFO] [Train] Iter 18608 | Loss 0.295758 | Grad 0.0356 \n","[2024-04-10 13:12:33,843::train::INFO] [Train] Iter 18609 | Loss 0.325790 | Grad 0.0427 \n","[2024-04-10 13:12:33,963::train::INFO] [Train] Iter 18610 | Loss 0.307794 | Grad 0.0308 \n","[2024-04-10 13:12:34,082::train::INFO] [Train] Iter 18611 | Loss 0.303170 | Grad 0.0290 \n","[2024-04-10 13:12:34,202::train::INFO] [Train] Iter 18612 | Loss 0.323125 | Grad 0.0329 \n","[2024-04-10 13:12:34,320::train::INFO] [Train] Iter 18613 | Loss 0.315014 | Grad 0.0361 \n","[2024-04-10 13:12:34,438::train::INFO] [Train] Iter 18614 | Loss 0.319130 | Grad 0.0345 \n","[2024-04-10 13:12:34,557::train::INFO] [Train] Iter 18615 | Loss 0.347202 | Grad 0.0412 \n","[2024-04-10 13:12:34,675::train::INFO] [Train] Iter 18616 | Loss 0.282704 | Grad 0.0341 \n","[2024-04-10 13:12:34,794::train::INFO] [Train] Iter 18617 | Loss 0.296638 | Grad 0.0459 \n","[2024-04-10 13:12:34,914::train::INFO] [Train] Iter 18618 | Loss 0.298603 | Grad 0.0449 \n","[2024-04-10 13:12:35,035::train::INFO] [Train] Iter 18619 | Loss 0.323012 | Grad 0.0548 \n","[2024-04-10 13:12:35,155::train::INFO] [Train] Iter 18620 | Loss 0.286921 | Grad 0.0279 \n","[2024-04-10 13:12:35,275::train::INFO] [Train] Iter 18621 | Loss 0.321801 | Grad 0.0417 \n","[2024-04-10 13:12:35,398::train::INFO] [Train] Iter 18622 | Loss 0.322239 | Grad 0.0310 \n","[2024-04-10 13:12:35,518::train::INFO] [Train] Iter 18623 | Loss 0.337874 | Grad 0.0369 \n","[2024-04-10 13:12:35,637::train::INFO] [Train] Iter 18624 | Loss 0.274646 | Grad 0.0381 \n","[2024-04-10 13:12:35,757::train::INFO] [Train] Iter 18625 | Loss 0.313628 | Grad 0.0399 \n","[2024-04-10 13:12:35,877::train::INFO] [Train] Iter 18626 | Loss 0.314583 | Grad 0.0650 \n","[2024-04-10 13:12:35,997::train::INFO] [Train] Iter 18627 | Loss 0.324187 | Grad 0.0409 \n","[2024-04-10 13:12:36,121::train::INFO] [Train] Iter 18628 | Loss 0.337343 | Grad 0.0488 \n","[2024-04-10 13:12:36,242::train::INFO] [Train] Iter 18629 | Loss 0.321588 | Grad 0.0536 \n","[2024-04-10 13:12:36,317::train::INFO] [Train] Iter 18630 | Loss 0.316029 | Grad 0.0445 \n","[2024-04-10 13:12:36,437::train::INFO] [Train] Iter 18631 | Loss 0.313695 | Grad 0.0376 \n","[2024-04-10 13:12:36,555::train::INFO] [Train] Iter 18632 | Loss 0.339348 | Grad 0.0414 \n","[2024-04-10 13:12:36,674::train::INFO] [Train] Iter 18633 | Loss 0.338634 | Grad 0.0442 \n","[2024-04-10 13:12:36,793::train::INFO] [Train] Iter 18634 | Loss 0.356621 | Grad 0.0599 \n","[2024-04-10 13:12:36,912::train::INFO] [Train] Iter 18635 | Loss 0.336667 | Grad 0.0246 \n","[2024-04-10 13:12:37,031::train::INFO] [Train] Iter 18636 | Loss 0.334705 | Grad 0.0379 \n","[2024-04-10 13:12:37,151::train::INFO] [Train] Iter 18637 | Loss 0.321253 | Grad 0.0298 \n","[2024-04-10 13:12:37,270::train::INFO] [Train] Iter 18638 | Loss 0.321198 | Grad 0.0530 \n","[2024-04-10 13:12:37,389::train::INFO] [Train] Iter 18639 | Loss 0.320967 | Grad 0.0387 \n","[2024-04-10 13:12:37,508::train::INFO] [Train] Iter 18640 | Loss 0.297635 | Grad 0.0511 \n","[2024-04-10 13:12:37,628::train::INFO] [Train] Iter 18641 | Loss 0.333958 | Grad 0.0422 \n","[2024-04-10 13:12:37,748::train::INFO] [Train] Iter 18642 | Loss 0.333119 | Grad 0.0411 \n","[2024-04-10 13:12:37,869::train::INFO] [Train] Iter 18643 | Loss 0.304828 | Grad 0.0338 \n","[2024-04-10 13:12:37,989::train::INFO] [Train] Iter 18644 | Loss 0.335822 | Grad 0.0528 \n","[2024-04-10 13:12:38,109::train::INFO] [Train] Iter 18645 | Loss 0.347291 | Grad 0.1185 \n","[2024-04-10 13:12:38,230::train::INFO] [Train] Iter 18646 | Loss 0.275620 | Grad 0.0464 \n","[2024-04-10 13:12:38,352::train::INFO] [Train] Iter 18647 | Loss 0.297807 | Grad 0.0562 \n","[2024-04-10 13:12:38,474::train::INFO] [Train] Iter 18648 | Loss 0.303813 | Grad 0.0334 \n","[2024-04-10 13:12:38,593::train::INFO] [Train] Iter 18649 | Loss 0.293959 | Grad 0.0386 \n","[2024-04-10 13:12:38,713::train::INFO] [Train] Iter 18650 | Loss 0.340262 | Grad 0.0655 \n","[2024-04-10 13:12:38,831::train::INFO] [Train] Iter 18651 | Loss 0.337573 | Grad 0.0405 \n","[2024-04-10 13:12:38,950::train::INFO] [Train] Iter 18652 | Loss 0.311737 | Grad 0.0366 \n","[2024-04-10 13:12:39,074::train::INFO] [Train] Iter 18653 | Loss 0.287694 | Grad 0.0277 \n","[2024-04-10 13:12:39,193::train::INFO] [Train] Iter 18654 | Loss 0.324468 | Grad 0.0424 \n","[2024-04-10 13:12:39,312::train::INFO] [Train] Iter 18655 | Loss 0.305961 | Grad 0.0441 \n","[2024-04-10 13:12:39,431::train::INFO] [Train] Iter 18656 | Loss 0.319497 | Grad 0.0331 \n","[2024-04-10 13:12:39,550::train::INFO] [Train] Iter 18657 | Loss 0.321913 | Grad 0.0454 \n","[2024-04-10 13:12:39,672::train::INFO] [Train] Iter 18658 | Loss 0.288780 | Grad 0.0361 \n","[2024-04-10 13:12:39,791::train::INFO] [Train] Iter 18659 | Loss 0.249336 | Grad 0.0234 \n","[2024-04-10 13:12:39,911::train::INFO] [Train] Iter 18660 | Loss 0.305000 | Grad 0.0548 \n","[2024-04-10 13:12:40,031::train::INFO] [Train] Iter 18661 | Loss 0.332113 | Grad 0.0587 \n","[2024-04-10 13:12:40,150::train::INFO] [Train] Iter 18662 | Loss 0.331652 | Grad 0.0433 \n","[2024-04-10 13:12:40,273::train::INFO] [Train] Iter 18663 | Loss 0.307434 | Grad 0.0433 \n","[2024-04-10 13:12:40,392::train::INFO] [Train] Iter 18664 | Loss 0.311514 | Grad 0.0484 \n","[2024-04-10 13:12:40,512::train::INFO] [Train] Iter 18665 | Loss 0.304386 | Grad 0.0268 \n","[2024-04-10 13:12:40,632::train::INFO] [Train] Iter 18666 | Loss 0.324629 | Grad 0.0591 \n","[2024-04-10 13:12:40,754::train::INFO] [Train] Iter 18667 | Loss 0.274998 | Grad 0.0319 \n","[2024-04-10 13:12:40,883::train::INFO] [Train] Iter 18668 | Loss 0.307544 | Grad 0.0416 \n","[2024-04-10 13:12:41,006::train::INFO] [Train] Iter 18669 | Loss 0.339338 | Grad 0.0473 \n","[2024-04-10 13:12:41,129::train::INFO] [Train] Iter 18670 | Loss 0.319453 | Grad 0.0447 \n","[2024-04-10 13:12:41,250::train::INFO] [Train] Iter 18671 | Loss 0.297489 | Grad 0.0482 \n","[2024-04-10 13:12:41,371::train::INFO] [Train] Iter 18672 | Loss 0.352957 | Grad 0.0558 \n","[2024-04-10 13:12:41,491::train::INFO] [Train] Iter 18673 | Loss 0.308461 | Grad 0.0382 \n","[2024-04-10 13:12:41,610::train::INFO] [Train] Iter 18674 | Loss 0.319870 | Grad 0.0477 \n","[2024-04-10 13:12:41,686::train::INFO] [Train] Iter 18675 | Loss 0.331255 | Grad 0.0405 \n","[2024-04-10 13:12:41,807::train::INFO] [Train] Iter 18676 | Loss 0.333805 | Grad 0.0543 \n","[2024-04-10 13:12:41,933::train::INFO] [Train] Iter 18677 | Loss 0.308869 | Grad 0.0308 \n","[2024-04-10 13:12:42,056::train::INFO] [Train] Iter 18678 | Loss 0.296245 | Grad 0.0414 \n","[2024-04-10 13:12:42,180::train::INFO] [Train] Iter 18679 | Loss 0.310586 | Grad 0.0518 \n","[2024-04-10 13:12:42,299::train::INFO] [Train] Iter 18680 | Loss 0.324862 | Grad 0.0700 \n","[2024-04-10 13:12:42,422::train::INFO] [Train] Iter 18681 | Loss 0.327632 | Grad 0.0416 \n","[2024-04-10 13:12:42,544::train::INFO] [Train] Iter 18682 | Loss 0.330282 | Grad 0.0515 \n","[2024-04-10 13:12:42,665::train::INFO] [Train] Iter 18683 | Loss 0.327400 | Grad 0.0466 \n","[2024-04-10 13:12:42,798::train::INFO] [Train] Iter 18684 | Loss 0.332484 | Grad 0.0369 \n","[2024-04-10 13:12:42,918::train::INFO] [Train] Iter 18685 | Loss 0.305915 | Grad 0.0424 \n","[2024-04-10 13:12:43,041::train::INFO] [Train] Iter 18686 | Loss 0.286416 | Grad 0.0344 \n","[2024-04-10 13:12:43,163::train::INFO] [Train] Iter 18687 | Loss 0.307552 | Grad 0.0626 \n","[2024-04-10 13:12:43,286::train::INFO] [Train] Iter 18688 | Loss 0.261582 | Grad 0.0308 \n","[2024-04-10 13:12:43,408::train::INFO] [Train] Iter 18689 | Loss 0.307133 | Grad 0.0656 \n","[2024-04-10 13:12:43,528::train::INFO] [Train] Iter 18690 | Loss 0.340076 | Grad 0.0531 \n","[2024-04-10 13:12:43,654::train::INFO] [Train] Iter 18691 | Loss 0.329285 | Grad 0.0422 \n","[2024-04-10 13:12:43,774::train::INFO] [Train] Iter 18692 | Loss 0.312680 | Grad 0.0608 \n","[2024-04-10 13:12:43,893::train::INFO] [Train] Iter 18693 | Loss 0.269425 | Grad 0.0445 \n","[2024-04-10 13:12:44,010::train::INFO] [Train] Iter 18694 | Loss 0.350465 | Grad 0.0612 \n","[2024-04-10 13:12:44,129::train::INFO] [Train] Iter 18695 | Loss 0.300387 | Grad 0.0529 \n","[2024-04-10 13:12:44,249::train::INFO] [Train] Iter 18696 | Loss 0.327387 | Grad 0.0431 \n","[2024-04-10 13:12:44,368::train::INFO] [Train] Iter 18697 | Loss 0.306828 | Grad 0.0602 \n","[2024-04-10 13:12:44,489::train::INFO] [Train] Iter 18698 | Loss 0.275532 | Grad 0.0285 \n","[2024-04-10 13:12:44,606::train::INFO] [Train] Iter 18699 | Loss 0.317335 | Grad 0.0462 \n","[2024-04-10 13:12:44,726::train::INFO] [Train] Iter 18700 | Loss 0.279818 | Grad 0.0539 \n","[2024-04-10 13:12:44,844::train::INFO] [Train] Iter 18701 | Loss 0.344802 | Grad 0.0702 \n","[2024-04-10 13:12:44,963::train::INFO] [Train] Iter 18702 | Loss 0.333306 | Grad 0.0398 \n","[2024-04-10 13:12:45,083::train::INFO] [Train] Iter 18703 | Loss 0.298357 | Grad 0.0318 \n","[2024-04-10 13:12:45,203::train::INFO] [Train] Iter 18704 | Loss 0.345534 | Grad 0.0592 \n","[2024-04-10 13:12:45,323::train::INFO] [Train] Iter 18705 | Loss 0.325696 | Grad 0.0492 \n","[2024-04-10 13:12:45,446::train::INFO] [Train] Iter 18706 | Loss 0.310962 | Grad 0.0449 \n","[2024-04-10 13:12:45,565::train::INFO] [Train] Iter 18707 | Loss 0.294595 | Grad 0.0444 \n","[2024-04-10 13:12:45,685::train::INFO] [Train] Iter 18708 | Loss 0.340870 | Grad 0.0399 \n","[2024-04-10 13:12:45,805::train::INFO] [Train] Iter 18709 | Loss 0.259523 | Grad 0.0328 \n","[2024-04-10 13:12:45,926::train::INFO] [Train] Iter 18710 | Loss 0.367000 | Grad 0.0401 \n","[2024-04-10 13:12:46,048::train::INFO] [Train] Iter 18711 | Loss 0.315725 | Grad 0.0498 \n","[2024-04-10 13:12:46,169::train::INFO] [Train] Iter 18712 | Loss 0.325850 | Grad 0.0378 \n","[2024-04-10 13:12:46,288::train::INFO] [Train] Iter 18713 | Loss 0.278616 | Grad 0.0303 \n","[2024-04-10 13:12:46,408::train::INFO] [Train] Iter 18714 | Loss 0.355984 | Grad 0.0592 \n","[2024-04-10 13:12:46,528::train::INFO] [Train] Iter 18715 | Loss 0.304567 | Grad 0.0744 \n","[2024-04-10 13:12:46,652::train::INFO] [Train] Iter 18716 | Loss 0.352159 | Grad 0.0571 \n","[2024-04-10 13:12:46,772::train::INFO] [Train] Iter 18717 | Loss 0.327755 | Grad 0.0420 \n","[2024-04-10 13:12:46,892::train::INFO] [Train] Iter 18718 | Loss 0.265625 | Grad 0.0364 \n","[2024-04-10 13:12:47,012::train::INFO] [Train] Iter 18719 | Loss 0.387810 | Grad 0.0646 \n","[2024-04-10 13:12:47,088::train::INFO] [Train] Iter 18720 | Loss 0.301656 | Grad 0.0397 \n","[2024-04-10 13:12:47,206::train::INFO] [Train] Iter 18721 | Loss 0.285324 | Grad 0.0439 \n","[2024-04-10 13:12:47,325::train::INFO] [Train] Iter 18722 | Loss 0.286187 | Grad 0.0388 \n","[2024-04-10 13:12:47,443::train::INFO] [Train] Iter 18723 | Loss 0.329230 | Grad 0.0537 \n","[2024-04-10 13:12:47,562::train::INFO] [Train] Iter 18724 | Loss 0.295671 | Grad 0.0352 \n","[2024-04-10 13:12:47,682::train::INFO] [Train] Iter 18725 | Loss 0.298376 | Grad 0.0400 \n","[2024-04-10 13:12:47,801::train::INFO] [Train] Iter 18726 | Loss 0.274362 | Grad 0.0376 \n","[2024-04-10 13:12:47,921::train::INFO] [Train] Iter 18727 | Loss 0.338503 | Grad 0.0637 \n","[2024-04-10 13:12:48,041::train::INFO] [Train] Iter 18728 | Loss 0.285087 | Grad 0.0403 \n","[2024-04-10 13:12:48,160::train::INFO] [Train] Iter 18729 | Loss 0.373718 | Grad 0.0880 \n","[2024-04-10 13:12:48,279::train::INFO] [Train] Iter 18730 | Loss 0.269455 | Grad 0.0277 \n","[2024-04-10 13:12:48,398::train::INFO] [Train] Iter 18731 | Loss 0.347359 | Grad 0.0476 \n","[2024-04-10 13:12:48,516::train::INFO] [Train] Iter 18732 | Loss 0.350270 | Grad 0.0522 \n","[2024-04-10 13:12:48,634::train::INFO] [Train] Iter 18733 | Loss 0.321498 | Grad 0.0416 \n","[2024-04-10 13:12:48,752::train::INFO] [Train] Iter 18734 | Loss 0.290797 | Grad 0.0324 \n","[2024-04-10 13:12:48,871::train::INFO] [Train] Iter 18735 | Loss 0.300925 | Grad 0.0483 \n","[2024-04-10 13:12:48,990::train::INFO] [Train] Iter 18736 | Loss 0.340415 | Grad 0.0489 \n","[2024-04-10 13:12:49,110::train::INFO] [Train] Iter 18737 | Loss 0.326110 | Grad 0.0711 \n","[2024-04-10 13:12:49,230::train::INFO] [Train] Iter 18738 | Loss 0.331740 | Grad 0.0729 \n","[2024-04-10 13:12:49,350::train::INFO] [Train] Iter 18739 | Loss 0.337527 | Grad 0.0393 \n","[2024-04-10 13:12:49,470::train::INFO] [Train] Iter 18740 | Loss 0.306189 | Grad 0.0430 \n","[2024-04-10 13:12:49,590::train::INFO] [Train] Iter 18741 | Loss 0.300304 | Grad 0.0432 \n","[2024-04-10 13:12:49,711::train::INFO] [Train] Iter 18742 | Loss 0.295645 | Grad 0.0367 \n","[2024-04-10 13:12:49,830::train::INFO] [Train] Iter 18743 | Loss 0.266221 | Grad 0.0528 \n","[2024-04-10 13:12:49,950::train::INFO] [Train] Iter 18744 | Loss 0.296860 | Grad 0.0578 \n","[2024-04-10 13:12:50,073::train::INFO] [Train] Iter 18745 | Loss 0.320610 | Grad 0.0324 \n","[2024-04-10 13:12:50,192::train::INFO] [Train] Iter 18746 | Loss 0.300987 | Grad 0.0364 \n","[2024-04-10 13:12:50,312::train::INFO] [Train] Iter 18747 | Loss 0.344017 | Grad 0.0502 \n","[2024-04-10 13:12:50,432::train::INFO] [Train] Iter 18748 | Loss 0.342213 | Grad 0.0658 \n","[2024-04-10 13:12:50,552::train::INFO] [Train] Iter 18749 | Loss 0.311449 | Grad 0.0297 \n","[2024-04-10 13:12:50,672::train::INFO] [Train] Iter 18750 | Loss 0.291520 | Grad 0.0319 \n","[2024-04-10 13:12:50,793::train::INFO] [Train] Iter 18751 | Loss 0.306896 | Grad 0.0586 \n","[2024-04-10 13:12:50,914::train::INFO] [Train] Iter 18752 | Loss 0.301644 | Grad 0.0333 \n","[2024-04-10 13:12:51,036::train::INFO] [Train] Iter 18753 | Loss 0.249067 | Grad 0.0282 \n","[2024-04-10 13:12:51,156::train::INFO] [Train] Iter 18754 | Loss 0.326136 | Grad 0.0389 \n","[2024-04-10 13:12:51,276::train::INFO] [Train] Iter 18755 | Loss 0.295095 | Grad 0.0288 \n","[2024-04-10 13:12:51,396::train::INFO] [Train] Iter 18756 | Loss 0.321117 | Grad 0.0539 \n","[2024-04-10 13:12:51,516::train::INFO] [Train] Iter 18757 | Loss 0.296319 | Grad 0.0237 \n","[2024-04-10 13:12:51,634::train::INFO] [Train] Iter 18758 | Loss 0.330421 | Grad 0.0370 \n","[2024-04-10 13:12:51,752::train::INFO] [Train] Iter 18759 | Loss 0.269802 | Grad 0.0267 \n","[2024-04-10 13:12:51,872::train::INFO] [Train] Iter 18760 | Loss 0.325217 | Grad 0.0350 \n","[2024-04-10 13:12:51,991::train::INFO] [Train] Iter 18761 | Loss 0.315819 | Grad 0.0557 \n","[2024-04-10 13:12:52,109::train::INFO] [Train] Iter 18762 | Loss 0.357234 | Grad 0.0570 \n","[2024-04-10 13:12:52,228::train::INFO] [Train] Iter 18763 | Loss 0.334410 | Grad 0.0380 \n","[2024-04-10 13:12:52,347::train::INFO] [Train] Iter 18764 | Loss 0.319587 | Grad 0.0394 \n","[2024-04-10 13:12:52,422::train::INFO] [Train] Iter 18765 | Loss 0.305779 | Grad 0.0474 \n","[2024-04-10 13:12:52,540::train::INFO] [Train] Iter 18766 | Loss 0.322204 | Grad 0.0376 \n","[2024-04-10 13:12:52,659::train::INFO] [Train] Iter 18767 | Loss 0.302864 | Grad 0.0554 \n","[2024-04-10 13:12:52,777::train::INFO] [Train] Iter 18768 | Loss 0.320985 | Grad 0.0569 \n","[2024-04-10 13:12:52,897::train::INFO] [Train] Iter 18769 | Loss 0.293823 | Grad 0.0395 \n","[2024-04-10 13:12:53,015::train::INFO] [Train] Iter 18770 | Loss 0.292944 | Grad 0.0618 \n","[2024-04-10 13:12:53,134::train::INFO] [Train] Iter 18771 | Loss 0.316981 | Grad 0.0384 \n","[2024-04-10 13:12:53,253::train::INFO] [Train] Iter 18772 | Loss 0.301656 | Grad 0.0455 \n","[2024-04-10 13:12:53,372::train::INFO] [Train] Iter 18773 | Loss 0.297140 | Grad 0.0666 \n","[2024-04-10 13:12:53,491::train::INFO] [Train] Iter 18774 | Loss 0.337461 | Grad 0.0356 \n","[2024-04-10 13:12:53,612::train::INFO] [Train] Iter 18775 | Loss 0.368275 | Grad 0.0685 \n","[2024-04-10 13:12:53,733::train::INFO] [Train] Iter 18776 | Loss 0.298932 | Grad 0.0394 \n","[2024-04-10 13:12:53,852::train::INFO] [Train] Iter 18777 | Loss 0.307938 | Grad 0.0491 \n","[2024-04-10 13:12:53,975::train::INFO] [Train] Iter 18778 | Loss 0.262964 | Grad 0.0250 \n","[2024-04-10 13:12:54,097::train::INFO] [Train] Iter 18779 | Loss 0.324723 | Grad 0.0374 \n","[2024-04-10 13:12:54,225::train::INFO] [Train] Iter 18780 | Loss 0.328928 | Grad 0.0453 \n","[2024-04-10 13:12:54,345::train::INFO] [Train] Iter 18781 | Loss 0.322160 | Grad 0.0364 \n","[2024-04-10 13:12:54,465::train::INFO] [Train] Iter 18782 | Loss 0.353073 | Grad 0.0722 \n","[2024-04-10 13:12:54,586::train::INFO] [Train] Iter 18783 | Loss 0.293245 | Grad 0.0556 \n","[2024-04-10 13:12:54,709::train::INFO] [Train] Iter 18784 | Loss 0.338610 | Grad 0.0695 \n","[2024-04-10 13:12:54,835::train::INFO] [Train] Iter 18785 | Loss 0.331599 | Grad 0.0509 \n","[2024-04-10 13:12:54,958::train::INFO] [Train] Iter 18786 | Loss 0.363092 | Grad 0.0858 \n","[2024-04-10 13:12:55,081::train::INFO] [Train] Iter 18787 | Loss 0.306421 | Grad 0.0521 \n","[2024-04-10 13:12:55,206::train::INFO] [Train] Iter 18788 | Loss 0.290120 | Grad 0.0447 \n","[2024-04-10 13:12:55,326::train::INFO] [Train] Iter 18789 | Loss 0.314897 | Grad 0.0730 \n","[2024-04-10 13:12:55,446::train::INFO] [Train] Iter 18790 | Loss 0.363167 | Grad 0.0893 \n","[2024-04-10 13:12:55,565::train::INFO] [Train] Iter 18791 | Loss 0.287673 | Grad 0.0325 \n","[2024-04-10 13:12:55,685::train::INFO] [Train] Iter 18792 | Loss 0.306580 | Grad 0.0434 \n","[2024-04-10 13:12:55,805::train::INFO] [Train] Iter 18793 | Loss 0.348481 | Grad 0.0526 \n","[2024-04-10 13:12:55,925::train::INFO] [Train] Iter 18794 | Loss 0.277717 | Grad 0.0292 \n","[2024-04-10 13:12:56,053::train::INFO] [Train] Iter 18795 | Loss 0.312295 | Grad 0.0547 \n","[2024-04-10 13:12:56,174::train::INFO] [Train] Iter 18796 | Loss 0.302480 | Grad 0.0556 \n","[2024-04-10 13:12:56,295::train::INFO] [Train] Iter 18797 | Loss 0.329869 | Grad 0.0400 \n","[2024-04-10 13:12:56,416::train::INFO] [Train] Iter 18798 | Loss 0.315233 | Grad 0.0361 \n","[2024-04-10 13:12:56,536::train::INFO] [Train] Iter 18799 | Loss 0.332883 | Grad 0.0320 \n","[2024-04-10 13:12:56,659::train::INFO] [Train] Iter 18800 | Loss 0.345170 | Grad 0.0398 \n","[2024-04-10 13:12:56,780::train::INFO] [Train] Iter 18801 | Loss 0.322486 | Grad 0.0421 \n","[2024-04-10 13:12:56,902::train::INFO] [Train] Iter 18802 | Loss 0.326797 | Grad 0.0392 \n","[2024-04-10 13:12:57,023::train::INFO] [Train] Iter 18803 | Loss 0.360768 | Grad 0.0399 \n","[2024-04-10 13:12:57,141::train::INFO] [Train] Iter 18804 | Loss 0.335264 | Grad 0.0673 \n","[2024-04-10 13:12:57,260::train::INFO] [Train] Iter 18805 | Loss 0.290903 | Grad 0.0346 \n","[2024-04-10 13:12:57,378::train::INFO] [Train] Iter 18806 | Loss 0.338758 | Grad 0.0494 \n","[2024-04-10 13:12:57,497::train::INFO] [Train] Iter 18807 | Loss 0.285281 | Grad 0.0520 \n","[2024-04-10 13:12:57,615::train::INFO] [Train] Iter 18808 | Loss 0.332897 | Grad 0.0765 \n","[2024-04-10 13:12:57,734::train::INFO] [Train] Iter 18809 | Loss 0.350531 | Grad 0.0615 \n","[2024-04-10 13:12:57,810::train::INFO] [Train] Iter 18810 | Loss 0.265290 | Grad 0.0336 \n","[2024-04-10 13:12:57,928::train::INFO] [Train] Iter 18811 | Loss 0.344973 | Grad 0.0454 \n","[2024-04-10 13:12:58,046::train::INFO] [Train] Iter 18812 | Loss 0.353098 | Grad 0.0400 \n","[2024-04-10 13:12:58,164::train::INFO] [Train] Iter 18813 | Loss 0.318464 | Grad 0.0452 \n","[2024-04-10 13:12:58,282::train::INFO] [Train] Iter 18814 | Loss 0.331249 | Grad 0.0419 \n","[2024-04-10 13:12:58,401::train::INFO] [Train] Iter 18815 | Loss 0.314708 | Grad 0.0363 \n","[2024-04-10 13:12:58,519::train::INFO] [Train] Iter 18816 | Loss 0.343448 | Grad 0.0483 \n","[2024-04-10 13:12:58,638::train::INFO] [Train] Iter 18817 | Loss 0.311992 | Grad 0.0331 \n","[2024-04-10 13:12:58,758::train::INFO] [Train] Iter 18818 | Loss 0.322728 | Grad 0.0635 \n","[2024-04-10 13:12:58,876::train::INFO] [Train] Iter 18819 | Loss 0.289423 | Grad 0.0402 \n","[2024-04-10 13:12:58,995::train::INFO] [Train] Iter 18820 | Loss 0.353511 | Grad 0.0405 \n","[2024-04-10 13:12:59,117::train::INFO] [Train] Iter 18821 | Loss 0.299941 | Grad 0.0503 \n","[2024-04-10 13:12:59,237::train::INFO] [Train] Iter 18822 | Loss 0.369011 | Grad 0.0434 \n","[2024-04-10 13:12:59,358::train::INFO] [Train] Iter 18823 | Loss 0.270564 | Grad 0.0367 \n","[2024-04-10 13:12:59,477::train::INFO] [Train] Iter 18824 | Loss 0.276437 | Grad 0.0328 \n","[2024-04-10 13:12:59,596::train::INFO] [Train] Iter 18825 | Loss 0.318564 | Grad 0.0620 \n","[2024-04-10 13:12:59,718::train::INFO] [Train] Iter 18826 | Loss 0.312483 | Grad 0.0465 \n","[2024-04-10 13:12:59,838::train::INFO] [Train] Iter 18827 | Loss 0.328192 | Grad 0.0520 \n","[2024-04-10 13:12:59,960::train::INFO] [Train] Iter 18828 | Loss 0.294682 | Grad 0.0633 \n","[2024-04-10 13:13:00,079::train::INFO] [Train] Iter 18829 | Loss 0.320783 | Grad 0.0473 \n","[2024-04-10 13:13:00,199::train::INFO] [Train] Iter 18830 | Loss 0.297848 | Grad 0.0332 \n","[2024-04-10 13:13:00,319::train::INFO] [Train] Iter 18831 | Loss 0.295694 | Grad 0.0394 \n","[2024-04-10 13:13:00,438::train::INFO] [Train] Iter 18832 | Loss 0.306465 | Grad 0.0357 \n","[2024-04-10 13:13:00,566::train::INFO] [Train] Iter 18833 | Loss 0.354892 | Grad 0.0413 \n","[2024-04-10 13:13:00,686::train::INFO] [Train] Iter 18834 | Loss 0.320484 | Grad 0.0452 \n","[2024-04-10 13:13:00,807::train::INFO] [Train] Iter 18835 | Loss 0.315240 | Grad 0.0383 \n","[2024-04-10 13:13:00,926::train::INFO] [Train] Iter 18836 | Loss 0.305684 | Grad 0.0531 \n","[2024-04-10 13:13:01,044::train::INFO] [Train] Iter 18837 | Loss 0.287512 | Grad 0.0453 \n","[2024-04-10 13:13:01,164::train::INFO] [Train] Iter 18838 | Loss 0.286394 | Grad 0.0598 \n","[2024-04-10 13:13:01,284::train::INFO] [Train] Iter 18839 | Loss 0.287991 | Grad 0.0411 \n","[2024-04-10 13:13:01,403::train::INFO] [Train] Iter 18840 | Loss 0.322834 | Grad 0.0534 \n","[2024-04-10 13:13:01,522::train::INFO] [Train] Iter 18841 | Loss 0.322754 | Grad 0.0478 \n","[2024-04-10 13:13:01,640::train::INFO] [Train] Iter 18842 | Loss 0.298189 | Grad 0.0660 \n","[2024-04-10 13:13:01,760::train::INFO] [Train] Iter 18843 | Loss 0.280891 | Grad 0.0514 \n","[2024-04-10 13:13:01,879::train::INFO] [Train] Iter 18844 | Loss 0.282692 | Grad 0.0287 \n","[2024-04-10 13:13:01,998::train::INFO] [Train] Iter 18845 | Loss 0.339746 | Grad 0.0450 \n","[2024-04-10 13:13:02,118::train::INFO] [Train] Iter 18846 | Loss 0.305621 | Grad 0.0535 \n","[2024-04-10 13:13:02,238::train::INFO] [Train] Iter 18847 | Loss 0.299241 | Grad 0.0340 \n","[2024-04-10 13:13:02,360::train::INFO] [Train] Iter 18848 | Loss 0.358899 | Grad 0.0397 \n","[2024-04-10 13:13:02,481::train::INFO] [Train] Iter 18849 | Loss 0.279942 | Grad 0.0360 \n","[2024-04-10 13:13:02,601::train::INFO] [Train] Iter 18850 | Loss 0.329189 | Grad 0.0392 \n","[2024-04-10 13:13:02,721::train::INFO] [Train] Iter 18851 | Loss 0.303926 | Grad 0.0527 \n","[2024-04-10 13:13:02,841::train::INFO] [Train] Iter 18852 | Loss 0.276611 | Grad 0.0347 \n","[2024-04-10 13:13:02,962::train::INFO] [Train] Iter 18853 | Loss 0.338184 | Grad 0.0507 \n","[2024-04-10 13:13:03,084::train::INFO] [Train] Iter 18854 | Loss 0.342159 | Grad 0.0799 \n","[2024-04-10 13:13:03,158::train::INFO] [Train] Iter 18855 | Loss 0.366233 | Grad 0.0423 \n","[2024-04-10 13:13:03,278::train::INFO] [Train] Iter 18856 | Loss 0.324555 | Grad 0.0393 \n","[2024-04-10 13:13:03,397::train::INFO] [Train] Iter 18857 | Loss 0.305354 | Grad 0.0368 \n","[2024-04-10 13:13:03,517::train::INFO] [Train] Iter 18858 | Loss 0.327144 | Grad 0.0425 \n","[2024-04-10 13:13:03,635::train::INFO] [Train] Iter 18859 | Loss 0.325347 | Grad 0.0371 \n","[2024-04-10 13:13:03,753::train::INFO] [Train] Iter 18860 | Loss 0.291645 | Grad 0.0352 \n","[2024-04-10 13:13:03,872::train::INFO] [Train] Iter 18861 | Loss 0.294646 | Grad 0.0413 \n","[2024-04-10 13:13:03,991::train::INFO] [Train] Iter 18862 | Loss 0.326889 | Grad 0.0527 \n","[2024-04-10 13:13:04,109::train::INFO] [Train] Iter 18863 | Loss 0.321899 | Grad 0.0502 \n","[2024-04-10 13:13:04,228::train::INFO] [Train] Iter 18864 | Loss 0.314962 | Grad 0.0404 \n","[2024-04-10 13:13:04,347::train::INFO] [Train] Iter 18865 | Loss 0.316241 | Grad 0.0461 \n","[2024-04-10 13:13:04,467::train::INFO] [Train] Iter 18866 | Loss 0.328959 | Grad 0.0399 \n","[2024-04-10 13:13:04,591::train::INFO] [Train] Iter 18867 | Loss 0.318938 | Grad 0.0499 \n","[2024-04-10 13:13:04,710::train::INFO] [Train] Iter 18868 | Loss 0.288583 | Grad 0.0385 \n","[2024-04-10 13:13:04,830::train::INFO] [Train] Iter 18869 | Loss 0.342735 | Grad 0.0498 \n","[2024-04-10 13:13:04,949::train::INFO] [Train] Iter 18870 | Loss 0.328211 | Grad 0.0413 \n","[2024-04-10 13:13:05,069::train::INFO] [Train] Iter 18871 | Loss 0.322548 | Grad 0.0653 \n","[2024-04-10 13:13:05,193::train::INFO] [Train] Iter 18872 | Loss 0.288037 | Grad 0.0389 \n","[2024-04-10 13:13:05,314::train::INFO] [Train] Iter 18873 | Loss 0.308748 | Grad 0.0637 \n","[2024-04-10 13:13:05,433::train::INFO] [Train] Iter 18874 | Loss 0.338251 | Grad 0.0655 \n","[2024-04-10 13:13:05,554::train::INFO] [Train] Iter 18875 | Loss 0.307624 | Grad 0.0573 \n","[2024-04-10 13:13:05,673::train::INFO] [Train] Iter 18876 | Loss 0.335648 | Grad 0.0515 \n","[2024-04-10 13:13:05,796::train::INFO] [Train] Iter 18877 | Loss 0.314568 | Grad 0.0348 \n","[2024-04-10 13:13:05,915::train::INFO] [Train] Iter 18878 | Loss 0.332697 | Grad 0.0490 \n","[2024-04-10 13:13:06,033::train::INFO] [Train] Iter 18879 | Loss 0.378221 | Grad 0.0601 \n","[2024-04-10 13:13:06,152::train::INFO] [Train] Iter 18880 | Loss 0.295056 | Grad 0.0408 \n","[2024-04-10 13:13:06,271::train::INFO] [Train] Iter 18881 | Loss 0.325117 | Grad 0.0382 \n","[2024-04-10 13:13:06,391::train::INFO] [Train] Iter 18882 | Loss 0.281657 | Grad 0.0366 \n","[2024-04-10 13:13:06,509::train::INFO] [Train] Iter 18883 | Loss 0.322807 | Grad 0.0414 \n","[2024-04-10 13:13:06,627::train::INFO] [Train] Iter 18884 | Loss 0.310426 | Grad 0.0539 \n","[2024-04-10 13:13:06,746::train::INFO] [Train] Iter 18885 | Loss 0.307995 | Grad 0.0445 \n","[2024-04-10 13:13:06,867::train::INFO] [Train] Iter 18886 | Loss 0.307343 | Grad 0.0497 \n","[2024-04-10 13:13:06,987::train::INFO] [Train] Iter 18887 | Loss 0.310964 | Grad 0.0266 \n","[2024-04-10 13:13:07,110::train::INFO] [Train] Iter 18888 | Loss 0.320040 | Grad 0.0418 \n","[2024-04-10 13:13:07,232::train::INFO] [Train] Iter 18889 | Loss 0.342802 | Grad 0.0656 \n","[2024-04-10 13:13:07,357::train::INFO] [Train] Iter 18890 | Loss 0.296900 | Grad 0.0496 \n","[2024-04-10 13:13:07,477::train::INFO] [Train] Iter 18891 | Loss 0.287252 | Grad 0.0468 \n","[2024-04-10 13:13:07,597::train::INFO] [Train] Iter 18892 | Loss 0.342918 | Grad 0.0659 \n","[2024-04-10 13:13:07,717::train::INFO] [Train] Iter 18893 | Loss 0.324626 | Grad 0.0468 \n","[2024-04-10 13:13:07,838::train::INFO] [Train] Iter 18894 | Loss 0.314546 | Grad 0.0520 \n","[2024-04-10 13:13:07,957::train::INFO] [Train] Iter 18895 | Loss 0.325088 | Grad 0.0747 \n","[2024-04-10 13:13:08,077::train::INFO] [Train] Iter 18896 | Loss 0.309495 | Grad 0.0358 \n","[2024-04-10 13:13:08,196::train::INFO] [Train] Iter 18897 | Loss 0.294681 | Grad 0.0415 \n","[2024-04-10 13:13:08,316::train::INFO] [Train] Iter 18898 | Loss 0.295004 | Grad 0.0481 \n","[2024-04-10 13:13:08,438::train::INFO] [Train] Iter 18899 | Loss 0.270039 | Grad 0.0492 \n","[2024-04-10 13:13:08,519::train::INFO] [Train] Iter 18900 | Loss 0.284445 | Grad 0.0538 \n","[2024-04-10 13:13:08,638::train::INFO] [Train] Iter 18901 | Loss 0.294837 | Grad 0.0478 \n","[2024-04-10 13:13:08,759::train::INFO] [Train] Iter 18902 | Loss 0.302203 | Grad 0.0413 \n","[2024-04-10 13:13:08,879::train::INFO] [Train] Iter 18903 | Loss 0.324787 | Grad 0.0808 \n","[2024-04-10 13:13:09,000::train::INFO] [Train] Iter 18904 | Loss 0.342729 | Grad 0.0527 \n","[2024-04-10 13:13:09,120::train::INFO] [Train] Iter 18905 | Loss 0.292137 | Grad 0.0459 \n","[2024-04-10 13:13:09,242::train::INFO] [Train] Iter 18906 | Loss 0.303528 | Grad 0.0915 \n","[2024-04-10 13:13:09,363::train::INFO] [Train] Iter 18907 | Loss 0.299278 | Grad 0.0791 \n","[2024-04-10 13:13:09,488::train::INFO] [Train] Iter 18908 | Loss 0.307153 | Grad 0.0541 \n","[2024-04-10 13:13:09,608::train::INFO] [Train] Iter 18909 | Loss 0.287224 | Grad 0.0453 \n","[2024-04-10 13:13:09,731::train::INFO] [Train] Iter 18910 | Loss 0.315000 | Grad 0.0408 \n","[2024-04-10 13:13:09,851::train::INFO] [Train] Iter 18911 | Loss 0.320090 | Grad 0.0380 \n","[2024-04-10 13:13:09,971::train::INFO] [Train] Iter 18912 | Loss 0.291793 | Grad 0.0378 \n","[2024-04-10 13:13:10,091::train::INFO] [Train] Iter 18913 | Loss 0.302595 | Grad 0.0428 \n","[2024-04-10 13:13:10,212::train::INFO] [Train] Iter 18914 | Loss 0.296873 | Grad 0.0403 \n","[2024-04-10 13:13:10,337::train::INFO] [Train] Iter 18915 | Loss 0.307227 | Grad 0.0373 \n","[2024-04-10 13:13:10,457::train::INFO] [Train] Iter 18916 | Loss 0.302778 | Grad 0.0516 \n","[2024-04-10 13:13:10,576::train::INFO] [Train] Iter 18917 | Loss 0.337272 | Grad 0.0527 \n","[2024-04-10 13:13:10,694::train::INFO] [Train] Iter 18918 | Loss 0.317902 | Grad 0.0711 \n","[2024-04-10 13:13:10,813::train::INFO] [Train] Iter 18919 | Loss 0.307487 | Grad 0.0381 \n","[2024-04-10 13:13:10,932::train::INFO] [Train] Iter 18920 | Loss 0.291583 | Grad 0.0352 \n","[2024-04-10 13:13:11,051::train::INFO] [Train] Iter 18921 | Loss 0.273786 | Grad 0.0333 \n","[2024-04-10 13:13:11,172::train::INFO] [Train] Iter 18922 | Loss 0.330765 | Grad 0.0474 \n","[2024-04-10 13:13:11,292::train::INFO] [Train] Iter 18923 | Loss 0.310241 | Grad 0.0444 \n","[2024-04-10 13:13:11,412::train::INFO] [Train] Iter 18924 | Loss 0.350359 | Grad 0.0441 \n","[2024-04-10 13:13:11,536::train::INFO] [Train] Iter 18925 | Loss 0.305630 | Grad 0.0225 \n","[2024-04-10 13:13:11,654::train::INFO] [Train] Iter 18926 | Loss 0.349533 | Grad 0.0533 \n","[2024-04-10 13:13:11,773::train::INFO] [Train] Iter 18927 | Loss 0.329225 | Grad 0.0565 \n","[2024-04-10 13:13:11,892::train::INFO] [Train] Iter 18928 | Loss 0.318769 | Grad 0.0467 \n","[2024-04-10 13:13:12,011::train::INFO] [Train] Iter 18929 | Loss 0.357447 | Grad 0.0632 \n","[2024-04-10 13:13:12,132::train::INFO] [Train] Iter 18930 | Loss 0.380589 | Grad 0.0618 \n","[2024-04-10 13:13:12,252::train::INFO] [Train] Iter 18931 | Loss 0.305372 | Grad 0.0416 \n","[2024-04-10 13:13:12,373::train::INFO] [Train] Iter 18932 | Loss 0.295053 | Grad 0.0441 \n","[2024-04-10 13:13:12,492::train::INFO] [Train] Iter 18933 | Loss 0.344898 | Grad 0.0461 \n","[2024-04-10 13:13:12,612::train::INFO] [Train] Iter 18934 | Loss 0.324697 | Grad 0.0323 \n","[2024-04-10 13:13:12,733::train::INFO] [Train] Iter 18935 | Loss 0.302879 | Grad 0.0390 \n","[2024-04-10 13:13:12,853::train::INFO] [Train] Iter 18936 | Loss 0.298126 | Grad 0.0351 \n","[2024-04-10 13:13:12,971::train::INFO] [Train] Iter 18937 | Loss 0.305978 | Grad 0.0442 \n","[2024-04-10 13:13:13,090::train::INFO] [Train] Iter 18938 | Loss 0.311476 | Grad 0.0508 \n","[2024-04-10 13:13:13,210::train::INFO] [Train] Iter 18939 | Loss 0.313746 | Grad 0.0371 \n","[2024-04-10 13:13:13,330::train::INFO] [Train] Iter 18940 | Loss 0.349660 | Grad 0.0430 \n","[2024-04-10 13:13:13,449::train::INFO] [Train] Iter 18941 | Loss 0.281329 | Grad 0.0285 \n","[2024-04-10 13:13:13,568::train::INFO] [Train] Iter 18942 | Loss 0.290776 | Grad 0.0481 \n","[2024-04-10 13:13:13,686::train::INFO] [Train] Iter 18943 | Loss 0.307885 | Grad 0.0472 \n","[2024-04-10 13:13:13,806::train::INFO] [Train] Iter 18944 | Loss 0.319415 | Grad 0.0486 \n","[2024-04-10 13:13:13,881::train::INFO] [Train] Iter 18945 | Loss 0.270882 | Grad 0.0400 \n","[2024-04-10 13:13:14,001::train::INFO] [Train] Iter 18946 | Loss 0.309011 | Grad 0.0578 \n","[2024-04-10 13:13:14,119::train::INFO] [Train] Iter 18947 | Loss 0.350621 | Grad 0.0443 \n","[2024-04-10 13:13:14,239::train::INFO] [Train] Iter 18948 | Loss 0.360052 | Grad 0.0634 \n","[2024-04-10 13:13:14,358::train::INFO] [Train] Iter 18949 | Loss 0.350133 | Grad 0.0641 \n","[2024-04-10 13:13:14,477::train::INFO] [Train] Iter 18950 | Loss 0.280704 | Grad 0.0249 \n","[2024-04-10 13:13:14,597::train::INFO] [Train] Iter 18951 | Loss 0.328456 | Grad 0.0467 \n","[2024-04-10 13:13:14,716::train::INFO] [Train] Iter 18952 | Loss 0.317269 | Grad 0.0404 \n","[2024-04-10 13:13:14,836::train::INFO] [Train] Iter 18953 | Loss 0.294602 | Grad 0.0604 \n","[2024-04-10 13:13:14,955::train::INFO] [Train] Iter 18954 | Loss 0.332130 | Grad 0.0551 \n","[2024-04-10 13:13:15,074::train::INFO] [Train] Iter 18955 | Loss 0.333456 | Grad 0.1128 \n","[2024-04-10 13:13:15,192::train::INFO] [Train] Iter 18956 | Loss 0.320786 | Grad 0.0495 \n","[2024-04-10 13:13:15,313::train::INFO] [Train] Iter 18957 | Loss 0.336093 | Grad 0.0486 \n","[2024-04-10 13:13:15,433::train::INFO] [Train] Iter 18958 | Loss 0.296218 | Grad 0.0442 \n","[2024-04-10 13:13:15,557::train::INFO] [Train] Iter 18959 | Loss 0.292690 | Grad 0.0713 \n","[2024-04-10 13:13:15,675::train::INFO] [Train] Iter 18960 | Loss 0.348727 | Grad 0.0588 \n","[2024-04-10 13:13:15,795::train::INFO] [Train] Iter 18961 | Loss 0.290048 | Grad 0.0377 \n","[2024-04-10 13:13:15,914::train::INFO] [Train] Iter 18962 | Loss 0.338018 | Grad 0.0578 \n","[2024-04-10 13:13:16,033::train::INFO] [Train] Iter 18963 | Loss 0.286853 | Grad 0.0388 \n","[2024-04-10 13:13:16,155::train::INFO] [Train] Iter 18964 | Loss 0.302561 | Grad 0.0281 \n","[2024-04-10 13:13:16,275::train::INFO] [Train] Iter 18965 | Loss 0.290369 | Grad 0.0286 \n","[2024-04-10 13:13:16,396::train::INFO] [Train] Iter 18966 | Loss 0.309915 | Grad 0.0621 \n","[2024-04-10 13:13:16,515::train::INFO] [Train] Iter 18967 | Loss 0.300443 | Grad 0.0539 \n","[2024-04-10 13:13:16,636::train::INFO] [Train] Iter 18968 | Loss 0.279898 | Grad 0.0322 \n","[2024-04-10 13:13:16,757::train::INFO] [Train] Iter 18969 | Loss 0.316780 | Grad 0.0382 \n","[2024-04-10 13:13:16,877::train::INFO] [Train] Iter 18970 | Loss 0.320760 | Grad 0.0412 \n","[2024-04-10 13:13:16,996::train::INFO] [Train] Iter 18971 | Loss 0.326357 | Grad 0.0422 \n","[2024-04-10 13:13:17,115::train::INFO] [Train] Iter 18972 | Loss 0.293872 | Grad 0.0418 \n","[2024-04-10 13:13:17,239::train::INFO] [Train] Iter 18973 | Loss 0.290883 | Grad 0.0879 \n","[2024-04-10 13:13:17,358::train::INFO] [Train] Iter 18974 | Loss 0.354874 | Grad 0.0851 \n","[2024-04-10 13:13:17,476::train::INFO] [Train] Iter 18975 | Loss 0.290475 | Grad 0.0524 \n","[2024-04-10 13:13:17,594::train::INFO] [Train] Iter 18976 | Loss 0.341969 | Grad 0.0501 \n","[2024-04-10 13:13:17,713::train::INFO] [Train] Iter 18977 | Loss 0.325985 | Grad 0.0485 \n","[2024-04-10 13:13:17,831::train::INFO] [Train] Iter 18978 | Loss 0.316151 | Grad 0.0460 \n","[2024-04-10 13:13:17,950::train::INFO] [Train] Iter 18979 | Loss 0.332137 | Grad 0.0493 \n","[2024-04-10 13:13:18,070::train::INFO] [Train] Iter 18980 | Loss 0.303762 | Grad 0.0518 \n","[2024-04-10 13:13:18,190::train::INFO] [Train] Iter 18981 | Loss 0.301219 | Grad 0.0473 \n","[2024-04-10 13:13:18,309::train::INFO] [Train] Iter 18982 | Loss 0.312849 | Grad 0.0461 \n","[2024-04-10 13:13:18,430::train::INFO] [Train] Iter 18983 | Loss 0.336010 | Grad 0.0576 \n","[2024-04-10 13:13:18,549::train::INFO] [Train] Iter 18984 | Loss 0.337059 | Grad 0.0399 \n","[2024-04-10 13:13:18,672::train::INFO] [Train] Iter 18985 | Loss 0.337434 | Grad 0.0437 \n","[2024-04-10 13:13:18,790::train::INFO] [Train] Iter 18986 | Loss 0.312178 | Grad 0.0388 \n","[2024-04-10 13:13:18,910::train::INFO] [Train] Iter 18987 | Loss 0.309899 | Grad 0.0428 \n","[2024-04-10 13:13:19,030::train::INFO] [Train] Iter 18988 | Loss 0.346911 | Grad 0.0391 \n","[2024-04-10 13:13:19,150::train::INFO] [Train] Iter 18989 | Loss 0.291890 | Grad 0.0322 \n","[2024-04-10 13:13:19,227::train::INFO] [Train] Iter 18990 | Loss 0.354684 | Grad 0.0457 \n","[2024-04-10 13:13:19,347::train::INFO] [Train] Iter 18991 | Loss 0.299138 | Grad 0.0410 \n","[2024-04-10 13:13:19,466::train::INFO] [Train] Iter 18992 | Loss 0.315168 | Grad 0.0499 \n","[2024-04-10 13:13:19,584::train::INFO] [Train] Iter 18993 | Loss 0.316027 | Grad 0.0762 \n","[2024-04-10 13:13:19,701::train::INFO] [Train] Iter 18994 | Loss 0.327987 | Grad 0.0588 \n","[2024-04-10 13:13:19,820::train::INFO] [Train] Iter 18995 | Loss 0.315308 | Grad 0.0428 \n","[2024-04-10 13:13:19,939::train::INFO] [Train] Iter 18996 | Loss 0.290737 | Grad 0.0549 \n","[2024-04-10 13:13:20,058::train::INFO] [Train] Iter 18997 | Loss 0.303511 | Grad 0.0409 \n","[2024-04-10 13:13:20,180::train::INFO] [Train] Iter 18998 | Loss 0.280319 | Grad 0.0502 \n","[2024-04-10 13:13:20,299::train::INFO] [Train] Iter 18999 | Loss 0.339057 | Grad 0.0827 \n","[2024-04-10 13:13:20,418::train::INFO] [Train] Iter 19000 | Loss 0.317854 | Grad 0.0588 \n","Validate: 100% 31/31 [00:48<00:00,  1.57s/it]\n","EMD-CD: 100% 31/31 [00:00<00:00, 136.49it/s]\n","[2024-04-10 13:14:09,291::train::INFO] [Val] Iter 19000 | CD 0.000817 | EMD 0.000000  \n","Inspect:   3% 1/31 [00:03<01:33,  3.13s/it]\n","[2024-04-10 13:14:12,705::train::INFO] [Train] Iter 19001 | Loss 0.297324 | Grad 0.0439 \n","[2024-04-10 13:14:12,835::train::INFO] [Train] Iter 19002 | Loss 0.322655 | Grad 0.0389 \n","[2024-04-10 13:14:12,959::train::INFO] [Train] Iter 19003 | Loss 0.321847 | Grad 0.0387 \n","[2024-04-10 13:14:13,080::train::INFO] [Train] Iter 19004 | Loss 0.312174 | Grad 0.0552 \n","[2024-04-10 13:14:13,201::train::INFO] [Train] Iter 19005 | Loss 0.309970 | Grad 0.0360 \n","[2024-04-10 13:14:13,322::train::INFO] [Train] Iter 19006 | Loss 0.323886 | Grad 0.0350 \n","[2024-04-10 13:14:13,443::train::INFO] [Train] Iter 19007 | Loss 0.305166 | Grad 0.0436 \n","[2024-04-10 13:14:13,564::train::INFO] [Train] Iter 19008 | Loss 0.297635 | Grad 0.0455 \n","[2024-04-10 13:14:13,686::train::INFO] [Train] Iter 19009 | Loss 0.329623 | Grad 0.0843 \n","[2024-04-10 13:14:13,809::train::INFO] [Train] Iter 19010 | Loss 0.293156 | Grad 0.0439 \n","[2024-04-10 13:14:13,932::train::INFO] [Train] Iter 19011 | Loss 0.327177 | Grad 0.0405 \n","[2024-04-10 13:14:14,053::train::INFO] [Train] Iter 19012 | Loss 0.283680 | Grad 0.0285 \n","[2024-04-10 13:14:14,174::train::INFO] [Train] Iter 19013 | Loss 0.319959 | Grad 0.0567 \n","[2024-04-10 13:14:14,294::train::INFO] [Train] Iter 19014 | Loss 0.279646 | Grad 0.0475 \n","[2024-04-10 13:14:14,420::train::INFO] [Train] Iter 19015 | Loss 0.300824 | Grad 0.0415 \n","[2024-04-10 13:14:14,540::train::INFO] [Train] Iter 19016 | Loss 0.314502 | Grad 0.0365 \n","[2024-04-10 13:14:14,661::train::INFO] [Train] Iter 19017 | Loss 0.308647 | Grad 0.0389 \n","[2024-04-10 13:14:14,781::train::INFO] [Train] Iter 19018 | Loss 0.314631 | Grad 0.0330 \n","[2024-04-10 13:14:14,903::train::INFO] [Train] Iter 19019 | Loss 0.290126 | Grad 0.0329 \n","[2024-04-10 13:14:15,028::train::INFO] [Train] Iter 19020 | Loss 0.328413 | Grad 0.0483 \n","[2024-04-10 13:14:15,151::train::INFO] [Train] Iter 19021 | Loss 0.328787 | Grad 0.0569 \n","[2024-04-10 13:14:15,273::train::INFO] [Train] Iter 19022 | Loss 0.339948 | Grad 0.0558 \n","[2024-04-10 13:14:15,401::train::INFO] [Train] Iter 19023 | Loss 0.301277 | Grad 0.0534 \n","[2024-04-10 13:14:15,521::train::INFO] [Train] Iter 19024 | Loss 0.327736 | Grad 0.0635 \n","[2024-04-10 13:14:15,642::train::INFO] [Train] Iter 19025 | Loss 0.376216 | Grad 0.0635 \n","[2024-04-10 13:14:15,761::train::INFO] [Train] Iter 19026 | Loss 0.326101 | Grad 0.0606 \n","[2024-04-10 13:14:15,879::train::INFO] [Train] Iter 19027 | Loss 0.323447 | Grad 0.0673 \n","[2024-04-10 13:14:15,997::train::INFO] [Train] Iter 19028 | Loss 0.319950 | Grad 0.0749 \n","[2024-04-10 13:14:16,115::train::INFO] [Train] Iter 19029 | Loss 0.304825 | Grad 0.0636 \n","[2024-04-10 13:14:16,236::train::INFO] [Train] Iter 19030 | Loss 0.332799 | Grad 0.0448 \n","[2024-04-10 13:14:16,355::train::INFO] [Train] Iter 19031 | Loss 0.305005 | Grad 0.0483 \n","[2024-04-10 13:14:16,476::train::INFO] [Train] Iter 19032 | Loss 0.356446 | Grad 0.0662 \n","[2024-04-10 13:14:16,595::train::INFO] [Train] Iter 19033 | Loss 0.303313 | Grad 0.0346 \n","[2024-04-10 13:14:16,715::train::INFO] [Train] Iter 19034 | Loss 0.317760 | Grad 0.0621 \n","[2024-04-10 13:14:16,792::train::INFO] [Train] Iter 19035 | Loss 0.291519 | Grad 0.0414 \n","[2024-04-10 13:14:16,911::train::INFO] [Train] Iter 19036 | Loss 0.322211 | Grad 0.0490 \n","[2024-04-10 13:14:17,030::train::INFO] [Train] Iter 19037 | Loss 0.311596 | Grad 0.0586 \n","[2024-04-10 13:14:17,150::train::INFO] [Train] Iter 19038 | Loss 0.300806 | Grad 0.0367 \n","[2024-04-10 13:14:17,268::train::INFO] [Train] Iter 19039 | Loss 0.345763 | Grad 0.0729 \n","[2024-04-10 13:14:17,387::train::INFO] [Train] Iter 19040 | Loss 0.313754 | Grad 0.0468 \n","[2024-04-10 13:14:17,506::train::INFO] [Train] Iter 19041 | Loss 0.360377 | Grad 0.0389 \n","[2024-04-10 13:14:17,626::train::INFO] [Train] Iter 19042 | Loss 0.346506 | Grad 0.0575 \n","[2024-04-10 13:14:17,746::train::INFO] [Train] Iter 19043 | Loss 0.351497 | Grad 0.1083 \n","[2024-04-10 13:14:17,866::train::INFO] [Train] Iter 19044 | Loss 0.338822 | Grad 0.0521 \n","[2024-04-10 13:14:17,987::train::INFO] [Train] Iter 19045 | Loss 0.354664 | Grad 0.0519 \n","[2024-04-10 13:14:18,106::train::INFO] [Train] Iter 19046 | Loss 0.290797 | Grad 0.0315 \n","[2024-04-10 13:14:18,226::train::INFO] [Train] Iter 19047 | Loss 0.299683 | Grad 0.0482 \n","[2024-04-10 13:14:18,344::train::INFO] [Train] Iter 19048 | Loss 0.343849 | Grad 0.0400 \n","[2024-04-10 13:14:18,464::train::INFO] [Train] Iter 19049 | Loss 0.300362 | Grad 0.0420 \n","[2024-04-10 13:14:18,582::train::INFO] [Train] Iter 19050 | Loss 0.349610 | Grad 0.0727 \n","[2024-04-10 13:14:18,701::train::INFO] [Train] Iter 19051 | Loss 0.367247 | Grad 0.0549 \n","[2024-04-10 13:14:18,819::train::INFO] [Train] Iter 19052 | Loss 0.296717 | Grad 0.0407 \n","[2024-04-10 13:14:18,937::train::INFO] [Train] Iter 19053 | Loss 0.294621 | Grad 0.0345 \n","[2024-04-10 13:14:19,056::train::INFO] [Train] Iter 19054 | Loss 0.305859 | Grad 0.0350 \n","[2024-04-10 13:14:19,176::train::INFO] [Train] Iter 19055 | Loss 0.317214 | Grad 0.0480 \n","[2024-04-10 13:14:19,295::train::INFO] [Train] Iter 19056 | Loss 0.307112 | Grad 0.0362 \n","[2024-04-10 13:14:19,414::train::INFO] [Train] Iter 19057 | Loss 0.339785 | Grad 0.0506 \n","[2024-04-10 13:14:19,537::train::INFO] [Train] Iter 19058 | Loss 0.323317 | Grad 0.0395 \n","[2024-04-10 13:14:19,656::train::INFO] [Train] Iter 19059 | Loss 0.329059 | Grad 0.0401 \n","[2024-04-10 13:14:19,775::train::INFO] [Train] Iter 19060 | Loss 0.335435 | Grad 0.0553 \n","[2024-04-10 13:14:19,894::train::INFO] [Train] Iter 19061 | Loss 0.314589 | Grad 0.0331 \n","[2024-04-10 13:14:20,014::train::INFO] [Train] Iter 19062 | Loss 0.290048 | Grad 0.0310 \n","[2024-04-10 13:14:20,135::train::INFO] [Train] Iter 19063 | Loss 0.312179 | Grad 0.0404 \n","[2024-04-10 13:14:20,255::train::INFO] [Train] Iter 19064 | Loss 0.281147 | Grad 0.0457 \n","[2024-04-10 13:14:20,378::train::INFO] [Train] Iter 19065 | Loss 0.293828 | Grad 0.0699 \n","[2024-04-10 13:14:20,499::train::INFO] [Train] Iter 19066 | Loss 0.366069 | Grad 0.0588 \n","[2024-04-10 13:14:20,619::train::INFO] [Train] Iter 19067 | Loss 0.316516 | Grad 0.0532 \n","[2024-04-10 13:14:20,739::train::INFO] [Train] Iter 19068 | Loss 0.312194 | Grad 0.0252 \n","[2024-04-10 13:14:20,860::train::INFO] [Train] Iter 19069 | Loss 0.299233 | Grad 0.0320 \n","[2024-04-10 13:14:20,979::train::INFO] [Train] Iter 19070 | Loss 0.332112 | Grad 0.0391 \n","[2024-04-10 13:14:21,099::train::INFO] [Train] Iter 19071 | Loss 0.323315 | Grad 0.0393 \n","[2024-04-10 13:14:21,221::train::INFO] [Train] Iter 19072 | Loss 0.314414 | Grad 0.0636 \n","[2024-04-10 13:14:21,344::train::INFO] [Train] Iter 19073 | Loss 0.309594 | Grad 0.0484 \n","[2024-04-10 13:14:21,463::train::INFO] [Train] Iter 19074 | Loss 0.290861 | Grad 0.0452 \n","[2024-04-10 13:14:21,582::train::INFO] [Train] Iter 19075 | Loss 0.284773 | Grad 0.0383 \n","[2024-04-10 13:14:21,701::train::INFO] [Train] Iter 19076 | Loss 0.323345 | Grad 0.0477 \n","[2024-04-10 13:14:21,819::train::INFO] [Train] Iter 19077 | Loss 0.295727 | Grad 0.0408 \n","[2024-04-10 13:14:21,937::train::INFO] [Train] Iter 19078 | Loss 0.352380 | Grad 0.0729 \n","[2024-04-10 13:14:22,056::train::INFO] [Train] Iter 19079 | Loss 0.312177 | Grad 0.0643 \n","[2024-04-10 13:14:22,130::train::INFO] [Train] Iter 19080 | Loss 0.285368 | Grad 0.0562 \n","[2024-04-10 13:14:22,250::train::INFO] [Train] Iter 19081 | Loss 0.297467 | Grad 0.0565 \n","[2024-04-10 13:14:22,369::train::INFO] [Train] Iter 19082 | Loss 0.334158 | Grad 0.0590 \n","[2024-04-10 13:14:22,487::train::INFO] [Train] Iter 19083 | Loss 0.294102 | Grad 0.0557 \n","[2024-04-10 13:14:22,605::train::INFO] [Train] Iter 19084 | Loss 0.290224 | Grad 0.0586 \n","[2024-04-10 13:14:22,724::train::INFO] [Train] Iter 19085 | Loss 0.311338 | Grad 0.0604 \n","[2024-04-10 13:14:22,842::train::INFO] [Train] Iter 19086 | Loss 0.302784 | Grad 0.0734 \n","[2024-04-10 13:14:22,961::train::INFO] [Train] Iter 19087 | Loss 0.344773 | Grad 0.0537 \n","[2024-04-10 13:14:23,080::train::INFO] [Train] Iter 19088 | Loss 0.287456 | Grad 0.0430 \n","[2024-04-10 13:14:23,200::train::INFO] [Train] Iter 19089 | Loss 0.346094 | Grad 0.0495 \n","[2024-04-10 13:14:23,319::train::INFO] [Train] Iter 19090 | Loss 0.288226 | Grad 0.0508 \n","[2024-04-10 13:14:23,440::train::INFO] [Train] Iter 19091 | Loss 0.306014 | Grad 0.0462 \n","[2024-04-10 13:14:23,564::train::INFO] [Train] Iter 19092 | Loss 0.283133 | Grad 0.0561 \n","[2024-04-10 13:14:23,683::train::INFO] [Train] Iter 19093 | Loss 0.257241 | Grad 0.0340 \n","[2024-04-10 13:14:23,802::train::INFO] [Train] Iter 19094 | Loss 0.297844 | Grad 0.0395 \n","[2024-04-10 13:14:23,921::train::INFO] [Train] Iter 19095 | Loss 0.297677 | Grad 0.0481 \n","[2024-04-10 13:14:24,041::train::INFO] [Train] Iter 19096 | Loss 0.300568 | Grad 0.0621 \n","[2024-04-10 13:14:24,163::train::INFO] [Train] Iter 19097 | Loss 0.309780 | Grad 0.0727 \n","[2024-04-10 13:14:24,283::train::INFO] [Train] Iter 19098 | Loss 0.310023 | Grad 0.0544 \n","[2024-04-10 13:14:24,404::train::INFO] [Train] Iter 19099 | Loss 0.347141 | Grad 0.0504 \n","[2024-04-10 13:14:24,523::train::INFO] [Train] Iter 19100 | Loss 0.321562 | Grad 0.0442 \n","[2024-04-10 13:14:24,644::train::INFO] [Train] Iter 19101 | Loss 0.276282 | Grad 0.0344 \n","[2024-04-10 13:14:24,768::train::INFO] [Train] Iter 19102 | Loss 0.333640 | Grad 0.0435 \n","[2024-04-10 13:14:24,889::train::INFO] [Train] Iter 19103 | Loss 0.337277 | Grad 0.0472 \n","[2024-04-10 13:14:25,009::train::INFO] [Train] Iter 19104 | Loss 0.373053 | Grad 0.0593 \n","[2024-04-10 13:14:25,129::train::INFO] [Train] Iter 19105 | Loss 0.372401 | Grad 0.0685 \n","[2024-04-10 13:14:25,249::train::INFO] [Train] Iter 19106 | Loss 0.313032 | Grad 0.0392 \n","[2024-04-10 13:14:25,370::train::INFO] [Train] Iter 19107 | Loss 0.330226 | Grad 0.0307 \n","[2024-04-10 13:14:25,490::train::INFO] [Train] Iter 19108 | Loss 0.331663 | Grad 0.0398 \n","[2024-04-10 13:14:25,611::train::INFO] [Train] Iter 19109 | Loss 0.330312 | Grad 0.0492 \n","[2024-04-10 13:14:25,735::train::INFO] [Train] Iter 19110 | Loss 0.313707 | Grad 0.0710 \n","[2024-04-10 13:14:25,857::train::INFO] [Train] Iter 19111 | Loss 0.309671 | Grad 0.0515 \n","[2024-04-10 13:14:25,982::train::INFO] [Train] Iter 19112 | Loss 0.371758 | Grad 0.0798 \n","[2024-04-10 13:14:26,102::train::INFO] [Train] Iter 19113 | Loss 0.307305 | Grad 0.0496 \n","[2024-04-10 13:14:26,223::train::INFO] [Train] Iter 19114 | Loss 0.346513 | Grad 0.0503 \n","[2024-04-10 13:14:26,344::train::INFO] [Train] Iter 19115 | Loss 0.317195 | Grad 0.0518 \n","[2024-04-10 13:14:26,463::train::INFO] [Train] Iter 19116 | Loss 0.322260 | Grad 0.0429 \n","[2024-04-10 13:14:26,584::train::INFO] [Train] Iter 19117 | Loss 0.318186 | Grad 0.0413 \n","[2024-04-10 13:14:26,716::train::INFO] [Train] Iter 19118 | Loss 0.353670 | Grad 0.0749 \n","[2024-04-10 13:14:26,839::train::INFO] [Train] Iter 19119 | Loss 0.312747 | Grad 0.0524 \n","[2024-04-10 13:14:26,962::train::INFO] [Train] Iter 19120 | Loss 0.318517 | Grad 0.0592 \n","[2024-04-10 13:14:27,084::train::INFO] [Train] Iter 19121 | Loss 0.379838 | Grad 0.0525 \n","[2024-04-10 13:14:27,205::train::INFO] [Train] Iter 19122 | Loss 0.299907 | Grad 0.0366 \n","[2024-04-10 13:14:27,325::train::INFO] [Train] Iter 19123 | Loss 0.333403 | Grad 0.0557 \n","[2024-04-10 13:14:27,445::train::INFO] [Train] Iter 19124 | Loss 0.282054 | Grad 0.0403 \n","[2024-04-10 13:14:27,522::train::INFO] [Train] Iter 19125 | Loss 0.296288 | Grad 0.0397 \n","[2024-04-10 13:14:27,644::train::INFO] [Train] Iter 19126 | Loss 0.302695 | Grad 0.0354 \n","[2024-04-10 13:14:27,770::train::INFO] [Train] Iter 19127 | Loss 0.291640 | Grad 0.0398 \n","[2024-04-10 13:14:27,897::train::INFO] [Train] Iter 19128 | Loss 0.309898 | Grad 0.0368 \n","[2024-04-10 13:14:28,016::train::INFO] [Train] Iter 19129 | Loss 0.334627 | Grad 0.0429 \n","[2024-04-10 13:14:28,136::train::INFO] [Train] Iter 19130 | Loss 0.326286 | Grad 0.0665 \n","[2024-04-10 13:14:28,257::train::INFO] [Train] Iter 19131 | Loss 0.268927 | Grad 0.0304 \n","[2024-04-10 13:14:28,378::train::INFO] [Train] Iter 19132 | Loss 0.301538 | Grad 0.0298 \n","[2024-04-10 13:14:28,498::train::INFO] [Train] Iter 19133 | Loss 0.285351 | Grad 0.0438 \n","[2024-04-10 13:14:28,618::train::INFO] [Train] Iter 19134 | Loss 0.289232 | Grad 0.0441 \n","[2024-04-10 13:14:28,739::train::INFO] [Train] Iter 19135 | Loss 0.349199 | Grad 0.0609 \n","[2024-04-10 13:14:28,859::train::INFO] [Train] Iter 19136 | Loss 0.351636 | Grad 0.0667 \n","[2024-04-10 13:14:28,977::train::INFO] [Train] Iter 19137 | Loss 0.332495 | Grad 0.0764 \n","[2024-04-10 13:14:29,098::train::INFO] [Train] Iter 19138 | Loss 0.324234 | Grad 0.0432 \n","[2024-04-10 13:14:29,217::train::INFO] [Train] Iter 19139 | Loss 0.335240 | Grad 0.0542 \n","[2024-04-10 13:14:29,336::train::INFO] [Train] Iter 19140 | Loss 0.328468 | Grad 0.0558 \n","[2024-04-10 13:14:29,456::train::INFO] [Train] Iter 19141 | Loss 0.292988 | Grad 0.0810 \n","[2024-04-10 13:14:29,576::train::INFO] [Train] Iter 19142 | Loss 0.304439 | Grad 0.0510 \n","[2024-04-10 13:14:29,696::train::INFO] [Train] Iter 19143 | Loss 0.332576 | Grad 0.0704 \n","[2024-04-10 13:14:29,816::train::INFO] [Train] Iter 19144 | Loss 0.295054 | Grad 0.0517 \n","[2024-04-10 13:14:29,937::train::INFO] [Train] Iter 19145 | Loss 0.308091 | Grad 0.0405 \n","[2024-04-10 13:14:30,056::train::INFO] [Train] Iter 19146 | Loss 0.339770 | Grad 0.0441 \n","[2024-04-10 13:14:30,175::train::INFO] [Train] Iter 19147 | Loss 0.385595 | Grad 0.0550 \n","[2024-04-10 13:14:30,297::train::INFO] [Train] Iter 19148 | Loss 0.303571 | Grad 0.0320 \n","[2024-04-10 13:14:30,418::train::INFO] [Train] Iter 19149 | Loss 0.347425 | Grad 0.0463 \n","[2024-04-10 13:14:30,537::train::INFO] [Train] Iter 19150 | Loss 0.320699 | Grad 0.0436 \n","[2024-04-10 13:14:30,657::train::INFO] [Train] Iter 19151 | Loss 0.296014 | Grad 0.0293 \n","[2024-04-10 13:14:30,776::train::INFO] [Train] Iter 19152 | Loss 0.305185 | Grad 0.0377 \n","[2024-04-10 13:14:30,895::train::INFO] [Train] Iter 19153 | Loss 0.318930 | Grad 0.0406 \n","[2024-04-10 13:14:31,013::train::INFO] [Train] Iter 19154 | Loss 0.354402 | Grad 0.0581 \n","[2024-04-10 13:14:31,131::train::INFO] [Train] Iter 19155 | Loss 0.336599 | Grad 0.0449 \n","[2024-04-10 13:14:31,250::train::INFO] [Train] Iter 19156 | Loss 0.306188 | Grad 0.0461 \n","[2024-04-10 13:14:31,369::train::INFO] [Train] Iter 19157 | Loss 0.345032 | Grad 0.0457 \n","[2024-04-10 13:14:31,489::train::INFO] [Train] Iter 19158 | Loss 0.340889 | Grad 0.0613 \n","[2024-04-10 13:14:31,611::train::INFO] [Train] Iter 19159 | Loss 0.318627 | Grad 0.0368 \n","[2024-04-10 13:14:31,730::train::INFO] [Train] Iter 19160 | Loss 0.321375 | Grad 0.0527 \n","[2024-04-10 13:14:31,849::train::INFO] [Train] Iter 19161 | Loss 0.313879 | Grad 0.0387 \n","[2024-04-10 13:14:31,969::train::INFO] [Train] Iter 19162 | Loss 0.327313 | Grad 0.0584 \n","[2024-04-10 13:14:32,090::train::INFO] [Train] Iter 19163 | Loss 0.300582 | Grad 0.0698 \n","[2024-04-10 13:14:32,210::train::INFO] [Train] Iter 19164 | Loss 0.306670 | Grad 0.0899 \n","[2024-04-10 13:14:32,331::train::INFO] [Train] Iter 19165 | Loss 0.322479 | Grad 0.0690 \n","[2024-04-10 13:14:32,451::train::INFO] [Train] Iter 19166 | Loss 0.315814 | Grad 0.0463 \n","[2024-04-10 13:14:32,571::train::INFO] [Train] Iter 19167 | Loss 0.341161 | Grad 0.0576 \n","[2024-04-10 13:14:32,692::train::INFO] [Train] Iter 19168 | Loss 0.312721 | Grad 0.0596 \n","[2024-04-10 13:14:32,811::train::INFO] [Train] Iter 19169 | Loss 0.338668 | Grad 0.0602 \n","[2024-04-10 13:14:32,887::train::INFO] [Train] Iter 19170 | Loss 0.328047 | Grad 0.1043 \n","[2024-04-10 13:14:33,007::train::INFO] [Train] Iter 19171 | Loss 0.348968 | Grad 0.0439 \n","[2024-04-10 13:14:33,125::train::INFO] [Train] Iter 19172 | Loss 0.271964 | Grad 0.0312 \n","[2024-04-10 13:14:33,245::train::INFO] [Train] Iter 19173 | Loss 0.284106 | Grad 0.0427 \n","[2024-04-10 13:14:33,363::train::INFO] [Train] Iter 19174 | Loss 0.348562 | Grad 0.0617 \n","[2024-04-10 13:14:33,482::train::INFO] [Train] Iter 19175 | Loss 0.301143 | Grad 0.0488 \n","[2024-04-10 13:14:33,600::train::INFO] [Train] Iter 19176 | Loss 0.319607 | Grad 0.0497 \n","[2024-04-10 13:14:33,719::train::INFO] [Train] Iter 19177 | Loss 0.290427 | Grad 0.0292 \n","[2024-04-10 13:14:33,837::train::INFO] [Train] Iter 19178 | Loss 0.357130 | Grad 0.0602 \n","[2024-04-10 13:14:33,956::train::INFO] [Train] Iter 19179 | Loss 0.251565 | Grad 0.0305 \n","[2024-04-10 13:14:34,076::train::INFO] [Train] Iter 19180 | Loss 0.326457 | Grad 0.0583 \n","[2024-04-10 13:14:34,196::train::INFO] [Train] Iter 19181 | Loss 0.316995 | Grad 0.0678 \n","[2024-04-10 13:14:34,319::train::INFO] [Train] Iter 19182 | Loss 0.328997 | Grad 0.0657 \n","[2024-04-10 13:14:34,439::train::INFO] [Train] Iter 19183 | Loss 0.311155 | Grad 0.0305 \n","[2024-04-10 13:14:34,558::train::INFO] [Train] Iter 19184 | Loss 0.325628 | Grad 0.0537 \n","[2024-04-10 13:14:34,677::train::INFO] [Train] Iter 19185 | Loss 0.278655 | Grad 0.0426 \n","[2024-04-10 13:14:34,795::train::INFO] [Train] Iter 19186 | Loss 0.328785 | Grad 0.0376 \n","[2024-04-10 13:14:34,914::train::INFO] [Train] Iter 19187 | Loss 0.326784 | Grad 0.0550 \n","[2024-04-10 13:14:35,033::train::INFO] [Train] Iter 19188 | Loss 0.283467 | Grad 0.0436 \n","[2024-04-10 13:14:35,152::train::INFO] [Train] Iter 19189 | Loss 0.298341 | Grad 0.0723 \n","[2024-04-10 13:14:35,272::train::INFO] [Train] Iter 19190 | Loss 0.302175 | Grad 0.0461 \n","[2024-04-10 13:14:35,391::train::INFO] [Train] Iter 19191 | Loss 0.314536 | Grad 0.0473 \n","[2024-04-10 13:14:35,511::train::INFO] [Train] Iter 19192 | Loss 0.282776 | Grad 0.0309 \n","[2024-04-10 13:14:35,631::train::INFO] [Train] Iter 19193 | Loss 0.298904 | Grad 0.0388 \n","[2024-04-10 13:14:35,752::train::INFO] [Train] Iter 19194 | Loss 0.352029 | Grad 0.0470 \n","[2024-04-10 13:14:35,871::train::INFO] [Train] Iter 19195 | Loss 0.292713 | Grad 0.0426 \n","[2024-04-10 13:14:35,993::train::INFO] [Train] Iter 19196 | Loss 0.312301 | Grad 0.0352 \n","[2024-04-10 13:14:36,112::train::INFO] [Train] Iter 19197 | Loss 0.309832 | Grad 0.0428 \n","[2024-04-10 13:14:36,232::train::INFO] [Train] Iter 19198 | Loss 0.354496 | Grad 0.0429 \n","[2024-04-10 13:14:36,353::train::INFO] [Train] Iter 19199 | Loss 0.319211 | Grad 0.0462 \n","[2024-04-10 13:14:36,473::train::INFO] [Train] Iter 19200 | Loss 0.265918 | Grad 0.0398 \n","[2024-04-10 13:14:36,591::train::INFO] [Train] Iter 19201 | Loss 0.312877 | Grad 0.0542 \n","[2024-04-10 13:14:36,712::train::INFO] [Train] Iter 19202 | Loss 0.326229 | Grad 0.0478 \n","[2024-04-10 13:14:36,832::train::INFO] [Train] Iter 19203 | Loss 0.322741 | Grad 0.0421 \n","[2024-04-10 13:14:36,952::train::INFO] [Train] Iter 19204 | Loss 0.344962 | Grad 0.0409 \n","[2024-04-10 13:14:37,072::train::INFO] [Train] Iter 19205 | Loss 0.293286 | Grad 0.0321 \n","[2024-04-10 13:14:37,191::train::INFO] [Train] Iter 19206 | Loss 0.296557 | Grad 0.0346 \n","[2024-04-10 13:14:37,310::train::INFO] [Train] Iter 19207 | Loss 0.338371 | Grad 0.0419 \n","[2024-04-10 13:14:37,429::train::INFO] [Train] Iter 19208 | Loss 0.333823 | Grad 0.0501 \n","[2024-04-10 13:14:37,548::train::INFO] [Train] Iter 19209 | Loss 0.296571 | Grad 0.0488 \n","[2024-04-10 13:14:37,667::train::INFO] [Train] Iter 19210 | Loss 0.281540 | Grad 0.0772 \n","[2024-04-10 13:14:37,787::train::INFO] [Train] Iter 19211 | Loss 0.316505 | Grad 0.0726 \n","[2024-04-10 13:14:37,905::train::INFO] [Train] Iter 19212 | Loss 0.318303 | Grad 0.0778 \n","[2024-04-10 13:14:38,024::train::INFO] [Train] Iter 19213 | Loss 0.319300 | Grad 0.0401 \n","[2024-04-10 13:14:38,143::train::INFO] [Train] Iter 19214 | Loss 0.349090 | Grad 0.0646 \n","[2024-04-10 13:14:38,217::train::INFO] [Train] Iter 19215 | Loss 0.271256 | Grad 0.0340 \n","[2024-04-10 13:14:38,336::train::INFO] [Train] Iter 19216 | Loss 0.333665 | Grad 0.0893 \n","[2024-04-10 13:14:38,457::train::INFO] [Train] Iter 19217 | Loss 0.308373 | Grad 0.0481 \n","[2024-04-10 13:14:38,576::train::INFO] [Train] Iter 19218 | Loss 0.283734 | Grad 0.0648 \n","[2024-04-10 13:14:38,696::train::INFO] [Train] Iter 19219 | Loss 0.348800 | Grad 0.0764 \n","[2024-04-10 13:14:38,817::train::INFO] [Train] Iter 19220 | Loss 0.310521 | Grad 0.0425 \n","[2024-04-10 13:14:38,938::train::INFO] [Train] Iter 19221 | Loss 0.330915 | Grad 0.0625 \n","[2024-04-10 13:14:39,063::train::INFO] [Train] Iter 19222 | Loss 0.283948 | Grad 0.0394 \n","[2024-04-10 13:14:39,187::train::INFO] [Train] Iter 19223 | Loss 0.279013 | Grad 0.0344 \n","[2024-04-10 13:14:39,309::train::INFO] [Train] Iter 19224 | Loss 0.301297 | Grad 0.0360 \n","[2024-04-10 13:14:39,430::train::INFO] [Train] Iter 19225 | Loss 0.303616 | Grad 0.0353 \n","[2024-04-10 13:14:39,551::train::INFO] [Train] Iter 19226 | Loss 0.341332 | Grad 0.0731 \n","[2024-04-10 13:14:39,675::train::INFO] [Train] Iter 19227 | Loss 0.327026 | Grad 0.0444 \n","[2024-04-10 13:14:39,795::train::INFO] [Train] Iter 19228 | Loss 0.286060 | Grad 0.0431 \n","[2024-04-10 13:14:39,915::train::INFO] [Train] Iter 19229 | Loss 0.358429 | Grad 0.0447 \n","[2024-04-10 13:14:40,036::train::INFO] [Train] Iter 19230 | Loss 0.333289 | Grad 0.0491 \n","[2024-04-10 13:14:40,157::train::INFO] [Train] Iter 19231 | Loss 0.305439 | Grad 0.0511 \n","[2024-04-10 13:14:40,278::train::INFO] [Train] Iter 19232 | Loss 0.299445 | Grad 0.0421 \n","[2024-04-10 13:14:40,398::train::INFO] [Train] Iter 19233 | Loss 0.306253 | Grad 0.0756 \n","[2024-04-10 13:14:40,518::train::INFO] [Train] Iter 19234 | Loss 0.275828 | Grad 0.0476 \n","[2024-04-10 13:14:40,639::train::INFO] [Train] Iter 19235 | Loss 0.308554 | Grad 0.0635 \n","[2024-04-10 13:14:40,758::train::INFO] [Train] Iter 19236 | Loss 0.305739 | Grad 0.0412 \n","[2024-04-10 13:14:40,879::train::INFO] [Train] Iter 19237 | Loss 0.361629 | Grad 0.0507 \n","[2024-04-10 13:14:40,998::train::INFO] [Train] Iter 19238 | Loss 0.328324 | Grad 0.0565 \n","[2024-04-10 13:14:41,118::train::INFO] [Train] Iter 19239 | Loss 0.325137 | Grad 0.0375 \n","[2024-04-10 13:14:41,239::train::INFO] [Train] Iter 19240 | Loss 0.287710 | Grad 0.0411 \n","[2024-04-10 13:14:41,365::train::INFO] [Train] Iter 19241 | Loss 0.330049 | Grad 0.0408 \n","[2024-04-10 13:14:41,486::train::INFO] [Train] Iter 19242 | Loss 0.332482 | Grad 0.0627 \n","[2024-04-10 13:14:41,608::train::INFO] [Train] Iter 19243 | Loss 0.296858 | Grad 0.0381 \n","[2024-04-10 13:14:41,734::train::INFO] [Train] Iter 19244 | Loss 0.281502 | Grad 0.0330 \n","[2024-04-10 13:14:41,856::train::INFO] [Train] Iter 19245 | Loss 0.307109 | Grad 0.0375 \n","[2024-04-10 13:14:41,980::train::INFO] [Train] Iter 19246 | Loss 0.338653 | Grad 0.0465 \n","[2024-04-10 13:14:42,101::train::INFO] [Train] Iter 19247 | Loss 0.272663 | Grad 0.0398 \n","[2024-04-10 13:14:42,221::train::INFO] [Train] Iter 19248 | Loss 0.318880 | Grad 0.0375 \n","[2024-04-10 13:14:42,341::train::INFO] [Train] Iter 19249 | Loss 0.299104 | Grad 0.0478 \n","[2024-04-10 13:14:42,460::train::INFO] [Train] Iter 19250 | Loss 0.317955 | Grad 0.0362 \n","[2024-04-10 13:14:42,580::train::INFO] [Train] Iter 19251 | Loss 0.312841 | Grad 0.0288 \n","[2024-04-10 13:14:42,700::train::INFO] [Train] Iter 19252 | Loss 0.276458 | Grad 0.0294 \n","[2024-04-10 13:14:42,820::train::INFO] [Train] Iter 19253 | Loss 0.304436 | Grad 0.0378 \n","[2024-04-10 13:14:42,939::train::INFO] [Train] Iter 19254 | Loss 0.292039 | Grad 0.0268 \n","[2024-04-10 13:14:43,059::train::INFO] [Train] Iter 19255 | Loss 0.301483 | Grad 0.0378 \n","[2024-04-10 13:14:43,179::train::INFO] [Train] Iter 19256 | Loss 0.332413 | Grad 0.0543 \n","[2024-04-10 13:14:43,299::train::INFO] [Train] Iter 19257 | Loss 0.313654 | Grad 0.0768 \n","[2024-04-10 13:14:43,417::train::INFO] [Train] Iter 19258 | Loss 0.309860 | Grad 0.0545 \n","[2024-04-10 13:14:43,538::train::INFO] [Train] Iter 19259 | Loss 0.320559 | Grad 0.0418 \n","[2024-04-10 13:14:43,612::train::INFO] [Train] Iter 19260 | Loss 0.334535 | Grad 0.0419 \n","[2024-04-10 13:14:43,731::train::INFO] [Train] Iter 19261 | Loss 0.278488 | Grad 0.0441 \n","[2024-04-10 13:14:43,850::train::INFO] [Train] Iter 19262 | Loss 0.318177 | Grad 0.0436 \n","[2024-04-10 13:14:43,970::train::INFO] [Train] Iter 19263 | Loss 0.318037 | Grad 0.0541 \n","[2024-04-10 13:14:44,089::train::INFO] [Train] Iter 19264 | Loss 0.331615 | Grad 0.0804 \n","[2024-04-10 13:14:44,209::train::INFO] [Train] Iter 19265 | Loss 0.299643 | Grad 0.0429 \n","[2024-04-10 13:14:44,330::train::INFO] [Train] Iter 19266 | Loss 0.363164 | Grad 0.0367 \n","[2024-04-10 13:14:44,450::train::INFO] [Train] Iter 19267 | Loss 0.323569 | Grad 0.0430 \n","[2024-04-10 13:14:44,570::train::INFO] [Train] Iter 19268 | Loss 0.350139 | Grad 0.0691 \n","[2024-04-10 13:14:44,695::train::INFO] [Train] Iter 19269 | Loss 0.336945 | Grad 0.0384 \n","[2024-04-10 13:14:44,814::train::INFO] [Train] Iter 19270 | Loss 0.318296 | Grad 0.0422 \n","[2024-04-10 13:14:44,933::train::INFO] [Train] Iter 19271 | Loss 0.323009 | Grad 0.0540 \n","[2024-04-10 13:14:45,052::train::INFO] [Train] Iter 19272 | Loss 0.327006 | Grad 0.0521 \n","[2024-04-10 13:14:45,173::train::INFO] [Train] Iter 19273 | Loss 0.322246 | Grad 0.0577 \n","[2024-04-10 13:14:45,293::train::INFO] [Train] Iter 19274 | Loss 0.324345 | Grad 0.0526 \n","[2024-04-10 13:14:45,415::train::INFO] [Train] Iter 19275 | Loss 0.352456 | Grad 0.0507 \n","[2024-04-10 13:14:45,536::train::INFO] [Train] Iter 19276 | Loss 0.294968 | Grad 0.0367 \n","[2024-04-10 13:14:45,656::train::INFO] [Train] Iter 19277 | Loss 0.296516 | Grad 0.0459 \n","[2024-04-10 13:14:45,776::train::INFO] [Train] Iter 19278 | Loss 0.335165 | Grad 0.0566 \n","[2024-04-10 13:14:45,896::train::INFO] [Train] Iter 19279 | Loss 0.310107 | Grad 0.0795 \n","[2024-04-10 13:14:46,017::train::INFO] [Train] Iter 19280 | Loss 0.369701 | Grad 0.1007 \n","[2024-04-10 13:14:46,136::train::INFO] [Train] Iter 19281 | Loss 0.312168 | Grad 0.0449 \n","[2024-04-10 13:14:46,256::train::INFO] [Train] Iter 19282 | Loss 0.291728 | Grad 0.0372 \n","[2024-04-10 13:14:46,377::train::INFO] [Train] Iter 19283 | Loss 0.297948 | Grad 0.0349 \n","[2024-04-10 13:14:46,496::train::INFO] [Train] Iter 19284 | Loss 0.324055 | Grad 0.0491 \n","[2024-04-10 13:14:46,616::train::INFO] [Train] Iter 19285 | Loss 0.332455 | Grad 0.0582 \n","[2024-04-10 13:14:46,736::train::INFO] [Train] Iter 19286 | Loss 0.343876 | Grad 0.0716 \n","[2024-04-10 13:14:46,856::train::INFO] [Train] Iter 19287 | Loss 0.298042 | Grad 0.0507 \n","[2024-04-10 13:14:46,975::train::INFO] [Train] Iter 19288 | Loss 0.266909 | Grad 0.0317 \n","[2024-04-10 13:14:47,095::train::INFO] [Train] Iter 19289 | Loss 0.321198 | Grad 0.0370 \n","[2024-04-10 13:14:47,214::train::INFO] [Train] Iter 19290 | Loss 0.300856 | Grad 0.0365 \n","[2024-04-10 13:14:47,333::train::INFO] [Train] Iter 19291 | Loss 0.281142 | Grad 0.0433 \n","[2024-04-10 13:14:47,451::train::INFO] [Train] Iter 19292 | Loss 0.315544 | Grad 0.0516 \n","[2024-04-10 13:14:47,569::train::INFO] [Train] Iter 19293 | Loss 0.314079 | Grad 0.0451 \n","[2024-04-10 13:14:47,689::train::INFO] [Train] Iter 19294 | Loss 0.319293 | Grad 0.0286 \n","[2024-04-10 13:14:47,809::train::INFO] [Train] Iter 19295 | Loss 0.334078 | Grad 0.0569 \n","[2024-04-10 13:14:47,928::train::INFO] [Train] Iter 19296 | Loss 0.323381 | Grad 0.0626 \n","[2024-04-10 13:14:48,050::train::INFO] [Train] Iter 19297 | Loss 0.328121 | Grad 0.0526 \n","[2024-04-10 13:14:48,170::train::INFO] [Train] Iter 19298 | Loss 0.305721 | Grad 0.0481 \n","[2024-04-10 13:14:48,289::train::INFO] [Train] Iter 19299 | Loss 0.364101 | Grad 0.0493 \n","[2024-04-10 13:14:48,407::train::INFO] [Train] Iter 19300 | Loss 0.331869 | Grad 0.0398 \n","[2024-04-10 13:14:48,528::train::INFO] [Train] Iter 19301 | Loss 0.293090 | Grad 0.0441 \n","[2024-04-10 13:14:48,647::train::INFO] [Train] Iter 19302 | Loss 0.353451 | Grad 0.0897 \n","[2024-04-10 13:14:48,767::train::INFO] [Train] Iter 19303 | Loss 0.268680 | Grad 0.0388 \n","[2024-04-10 13:14:48,885::train::INFO] [Train] Iter 19304 | Loss 0.320190 | Grad 0.0817 \n","[2024-04-10 13:14:48,960::train::INFO] [Train] Iter 19305 | Loss 0.273890 | Grad 0.0435 \n","[2024-04-10 13:14:49,078::train::INFO] [Train] Iter 19306 | Loss 0.321208 | Grad 0.0376 \n","[2024-04-10 13:14:49,198::train::INFO] [Train] Iter 19307 | Loss 0.278468 | Grad 0.0307 \n","[2024-04-10 13:14:49,318::train::INFO] [Train] Iter 19308 | Loss 0.347696 | Grad 0.0798 \n","[2024-04-10 13:14:49,440::train::INFO] [Train] Iter 19309 | Loss 0.282250 | Grad 0.0360 \n","[2024-04-10 13:14:49,561::train::INFO] [Train] Iter 19310 | Loss 0.297813 | Grad 0.0315 \n","[2024-04-10 13:14:49,681::train::INFO] [Train] Iter 19311 | Loss 0.284190 | Grad 0.0430 \n","[2024-04-10 13:14:49,802::train::INFO] [Train] Iter 19312 | Loss 0.321284 | Grad 0.0342 \n","[2024-04-10 13:14:49,922::train::INFO] [Train] Iter 19313 | Loss 0.331128 | Grad 0.0428 \n","[2024-04-10 13:14:50,043::train::INFO] [Train] Iter 19314 | Loss 0.333566 | Grad 0.0390 \n","[2024-04-10 13:14:50,163::train::INFO] [Train] Iter 19315 | Loss 0.278880 | Grad 0.0393 \n","[2024-04-10 13:14:50,283::train::INFO] [Train] Iter 19316 | Loss 0.318629 | Grad 0.0480 \n","[2024-04-10 13:14:50,404::train::INFO] [Train] Iter 19317 | Loss 0.337442 | Grad 0.0474 \n","[2024-04-10 13:14:50,524::train::INFO] [Train] Iter 19318 | Loss 0.326267 | Grad 0.0557 \n","[2024-04-10 13:14:50,644::train::INFO] [Train] Iter 19319 | Loss 0.345332 | Grad 0.0433 \n","[2024-04-10 13:14:50,765::train::INFO] [Train] Iter 19320 | Loss 0.345635 | Grad 0.0451 \n","[2024-04-10 13:14:50,885::train::INFO] [Train] Iter 19321 | Loss 0.303246 | Grad 0.0440 \n","[2024-04-10 13:14:51,004::train::INFO] [Train] Iter 19322 | Loss 0.305499 | Grad 0.0516 \n","[2024-04-10 13:14:51,124::train::INFO] [Train] Iter 19323 | Loss 0.313428 | Grad 0.0547 \n","[2024-04-10 13:14:51,244::train::INFO] [Train] Iter 19324 | Loss 0.341172 | Grad 0.0647 \n","[2024-04-10 13:14:51,363::train::INFO] [Train] Iter 19325 | Loss 0.338319 | Grad 0.0890 \n","[2024-04-10 13:14:51,484::train::INFO] [Train] Iter 19326 | Loss 0.361178 | Grad 0.0605 \n","[2024-04-10 13:14:51,604::train::INFO] [Train] Iter 19327 | Loss 0.338300 | Grad 0.0462 \n","[2024-04-10 13:14:51,723::train::INFO] [Train] Iter 19328 | Loss 0.345358 | Grad 0.0333 \n","[2024-04-10 13:14:51,842::train::INFO] [Train] Iter 19329 | Loss 0.309749 | Grad 0.0359 \n","[2024-04-10 13:14:51,962::train::INFO] [Train] Iter 19330 | Loss 0.310813 | Grad 0.0429 \n","[2024-04-10 13:14:52,081::train::INFO] [Train] Iter 19331 | Loss 0.332536 | Grad 0.0504 \n","[2024-04-10 13:14:52,204::train::INFO] [Train] Iter 19332 | Loss 0.294461 | Grad 0.0455 \n","[2024-04-10 13:14:52,330::train::INFO] [Train] Iter 19333 | Loss 0.307100 | Grad 0.0423 \n","[2024-04-10 13:14:52,454::train::INFO] [Train] Iter 19334 | Loss 0.305465 | Grad 0.0550 \n","[2024-04-10 13:14:52,579::train::INFO] [Train] Iter 19335 | Loss 0.314870 | Grad 0.0481 \n","[2024-04-10 13:14:52,708::train::INFO] [Train] Iter 19336 | Loss 0.302580 | Grad 0.0461 \n","[2024-04-10 13:14:52,829::train::INFO] [Train] Iter 19337 | Loss 0.325831 | Grad 0.0534 \n","[2024-04-10 13:14:52,949::train::INFO] [Train] Iter 19338 | Loss 0.281655 | Grad 0.0363 \n","[2024-04-10 13:14:53,074::train::INFO] [Train] Iter 19339 | Loss 0.314628 | Grad 0.0343 \n","[2024-04-10 13:14:53,195::train::INFO] [Train] Iter 19340 | Loss 0.336318 | Grad 0.0379 \n","[2024-04-10 13:14:53,318::train::INFO] [Train] Iter 19341 | Loss 0.299304 | Grad 0.0283 \n","[2024-04-10 13:14:53,442::train::INFO] [Train] Iter 19342 | Loss 0.284383 | Grad 0.0411 \n","[2024-04-10 13:14:53,565::train::INFO] [Train] Iter 19343 | Loss 0.346833 | Grad 0.0524 \n","[2024-04-10 13:14:53,686::train::INFO] [Train] Iter 19344 | Loss 0.320405 | Grad 0.0342 \n","[2024-04-10 13:14:53,808::train::INFO] [Train] Iter 19345 | Loss 0.306779 | Grad 0.0318 \n","[2024-04-10 13:14:53,929::train::INFO] [Train] Iter 19346 | Loss 0.339575 | Grad 0.0424 \n","[2024-04-10 13:14:54,050::train::INFO] [Train] Iter 19347 | Loss 0.305516 | Grad 0.0410 \n","[2024-04-10 13:14:54,171::train::INFO] [Train] Iter 19348 | Loss 0.328782 | Grad 0.0466 \n","[2024-04-10 13:14:54,292::train::INFO] [Train] Iter 19349 | Loss 0.309651 | Grad 0.0568 \n","[2024-04-10 13:14:54,369::train::INFO] [Train] Iter 19350 | Loss 0.272830 | Grad 0.0296 \n","[2024-04-10 13:14:54,490::train::INFO] [Train] Iter 19351 | Loss 0.300825 | Grad 0.0567 \n","[2024-04-10 13:14:54,616::train::INFO] [Train] Iter 19352 | Loss 0.284192 | Grad 0.0271 \n","[2024-04-10 13:14:54,737::train::INFO] [Train] Iter 19353 | Loss 0.357106 | Grad 0.0513 \n","[2024-04-10 13:14:54,859::train::INFO] [Train] Iter 19354 | Loss 0.305757 | Grad 0.0462 \n","[2024-04-10 13:14:54,982::train::INFO] [Train] Iter 19355 | Loss 0.272430 | Grad 0.0320 \n","[2024-04-10 13:14:55,103::train::INFO] [Train] Iter 19356 | Loss 0.331649 | Grad 0.0841 \n","[2024-04-10 13:14:55,225::train::INFO] [Train] Iter 19357 | Loss 0.303425 | Grad 0.0492 \n","[2024-04-10 13:14:55,343::train::INFO] [Train] Iter 19358 | Loss 0.280820 | Grad 0.0342 \n","[2024-04-10 13:14:55,463::train::INFO] [Train] Iter 19359 | Loss 0.297599 | Grad 0.0416 \n","[2024-04-10 13:14:55,582::train::INFO] [Train] Iter 19360 | Loss 0.307315 | Grad 0.0795 \n","[2024-04-10 13:14:55,701::train::INFO] [Train] Iter 19361 | Loss 0.308843 | Grad 0.0393 \n","[2024-04-10 13:14:55,820::train::INFO] [Train] Iter 19362 | Loss 0.329207 | Grad 0.0365 \n","[2024-04-10 13:14:55,939::train::INFO] [Train] Iter 19363 | Loss 0.316367 | Grad 0.0306 \n","[2024-04-10 13:14:56,057::train::INFO] [Train] Iter 19364 | Loss 0.295300 | Grad 0.0389 \n","[2024-04-10 13:14:56,177::train::INFO] [Train] Iter 19365 | Loss 0.313647 | Grad 0.0291 \n","[2024-04-10 13:14:56,298::train::INFO] [Train] Iter 19366 | Loss 0.349790 | Grad 0.0462 \n","[2024-04-10 13:14:56,417::train::INFO] [Train] Iter 19367 | Loss 0.318826 | Grad 0.0527 \n","[2024-04-10 13:14:56,535::train::INFO] [Train] Iter 19368 | Loss 0.256073 | Grad 0.0285 \n","[2024-04-10 13:14:56,655::train::INFO] [Train] Iter 19369 | Loss 0.259960 | Grad 0.0376 \n","[2024-04-10 13:14:56,775::train::INFO] [Train] Iter 19370 | Loss 0.342238 | Grad 0.0523 \n","[2024-04-10 13:14:56,894::train::INFO] [Train] Iter 19371 | Loss 0.319140 | Grad 0.0446 \n","[2024-04-10 13:14:57,013::train::INFO] [Train] Iter 19372 | Loss 0.306414 | Grad 0.0556 \n","[2024-04-10 13:14:57,132::train::INFO] [Train] Iter 19373 | Loss 0.296174 | Grad 0.0585 \n","[2024-04-10 13:14:57,251::train::INFO] [Train] Iter 19374 | Loss 0.325214 | Grad 0.0688 \n","[2024-04-10 13:14:57,370::train::INFO] [Train] Iter 19375 | Loss 0.318029 | Grad 0.0327 \n","[2024-04-10 13:14:57,491::train::INFO] [Train] Iter 19376 | Loss 0.367926 | Grad 0.0803 \n","[2024-04-10 13:14:57,610::train::INFO] [Train] Iter 19377 | Loss 0.317711 | Grad 0.0767 \n","[2024-04-10 13:14:57,731::train::INFO] [Train] Iter 19378 | Loss 0.344757 | Grad 0.0621 \n","[2024-04-10 13:14:57,849::train::INFO] [Train] Iter 19379 | Loss 0.370842 | Grad 0.0715 \n","[2024-04-10 13:14:57,968::train::INFO] [Train] Iter 19380 | Loss 0.357935 | Grad 0.0557 \n","[2024-04-10 13:14:58,086::train::INFO] [Train] Iter 19381 | Loss 0.268142 | Grad 0.0318 \n","[2024-04-10 13:14:58,214::train::INFO] [Train] Iter 19382 | Loss 0.350506 | Grad 0.0607 \n","[2024-04-10 13:14:58,333::train::INFO] [Train] Iter 19383 | Loss 0.317532 | Grad 0.0428 \n","[2024-04-10 13:14:58,452::train::INFO] [Train] Iter 19384 | Loss 0.310254 | Grad 0.0365 \n","[2024-04-10 13:14:58,572::train::INFO] [Train] Iter 19385 | Loss 0.293725 | Grad 0.0370 \n","[2024-04-10 13:14:58,693::train::INFO] [Train] Iter 19386 | Loss 0.317254 | Grad 0.0402 \n","[2024-04-10 13:14:58,815::train::INFO] [Train] Iter 19387 | Loss 0.322161 | Grad 0.0346 \n","[2024-04-10 13:14:58,934::train::INFO] [Train] Iter 19388 | Loss 0.327074 | Grad 0.0424 \n","[2024-04-10 13:14:59,054::train::INFO] [Train] Iter 19389 | Loss 0.303238 | Grad 0.0453 \n","[2024-04-10 13:14:59,174::train::INFO] [Train] Iter 19390 | Loss 0.280726 | Grad 0.0355 \n","[2024-04-10 13:14:59,293::train::INFO] [Train] Iter 19391 | Loss 0.315727 | Grad 0.0483 \n","[2024-04-10 13:14:59,415::train::INFO] [Train] Iter 19392 | Loss 0.261868 | Grad 0.0321 \n","[2024-04-10 13:14:59,536::train::INFO] [Train] Iter 19393 | Loss 0.333083 | Grad 0.0764 \n","[2024-04-10 13:14:59,655::train::INFO] [Train] Iter 19394 | Loss 0.294565 | Grad 0.0487 \n","[2024-04-10 13:14:59,732::train::INFO] [Train] Iter 19395 | Loss 0.293529 | Grad 0.0656 \n","[2024-04-10 13:14:59,850::train::INFO] [Train] Iter 19396 | Loss 0.347684 | Grad 0.0496 \n","[2024-04-10 13:14:59,969::train::INFO] [Train] Iter 19397 | Loss 0.307283 | Grad 0.0382 \n","[2024-04-10 13:15:00,088::train::INFO] [Train] Iter 19398 | Loss 0.301834 | Grad 0.0429 \n","[2024-04-10 13:15:00,207::train::INFO] [Train] Iter 19399 | Loss 0.314544 | Grad 0.0400 \n","[2024-04-10 13:15:00,327::train::INFO] [Train] Iter 19400 | Loss 0.291188 | Grad 0.0576 \n","[2024-04-10 13:15:00,448::train::INFO] [Train] Iter 19401 | Loss 0.295704 | Grad 0.0374 \n","[2024-04-10 13:15:00,567::train::INFO] [Train] Iter 19402 | Loss 0.322930 | Grad 0.0319 \n","[2024-04-10 13:15:00,689::train::INFO] [Train] Iter 19403 | Loss 0.309979 | Grad 0.0509 \n","[2024-04-10 13:15:00,811::train::INFO] [Train] Iter 19404 | Loss 0.310671 | Grad 0.0463 \n","[2024-04-10 13:15:00,932::train::INFO] [Train] Iter 19405 | Loss 0.325967 | Grad 0.0297 \n","[2024-04-10 13:15:01,052::train::INFO] [Train] Iter 19406 | Loss 0.301558 | Grad 0.0438 \n","[2024-04-10 13:15:01,172::train::INFO] [Train] Iter 19407 | Loss 0.336334 | Grad 0.0375 \n","[2024-04-10 13:15:01,294::train::INFO] [Train] Iter 19408 | Loss 0.321865 | Grad 0.0538 \n","[2024-04-10 13:15:01,413::train::INFO] [Train] Iter 19409 | Loss 0.316838 | Grad 0.0579 \n","[2024-04-10 13:15:01,532::train::INFO] [Train] Iter 19410 | Loss 0.333058 | Grad 0.0283 \n","[2024-04-10 13:15:01,651::train::INFO] [Train] Iter 19411 | Loss 0.325569 | Grad 0.0420 \n","[2024-04-10 13:15:01,770::train::INFO] [Train] Iter 19412 | Loss 0.296031 | Grad 0.0585 \n","[2024-04-10 13:15:01,888::train::INFO] [Train] Iter 19413 | Loss 0.296462 | Grad 0.0536 \n","[2024-04-10 13:15:02,007::train::INFO] [Train] Iter 19414 | Loss 0.334189 | Grad 0.0441 \n","[2024-04-10 13:15:02,127::train::INFO] [Train] Iter 19415 | Loss 0.320529 | Grad 0.0419 \n","[2024-04-10 13:15:02,246::train::INFO] [Train] Iter 19416 | Loss 0.324423 | Grad 0.0615 \n","[2024-04-10 13:15:02,365::train::INFO] [Train] Iter 19417 | Loss 0.320951 | Grad 0.0473 \n","[2024-04-10 13:15:02,484::train::INFO] [Train] Iter 19418 | Loss 0.344903 | Grad 0.0477 \n","[2024-04-10 13:15:02,603::train::INFO] [Train] Iter 19419 | Loss 0.323883 | Grad 0.0462 \n","[2024-04-10 13:15:02,729::train::INFO] [Train] Iter 19420 | Loss 0.341453 | Grad 0.0527 \n","[2024-04-10 13:15:02,848::train::INFO] [Train] Iter 19421 | Loss 0.318869 | Grad 0.0350 \n","[2024-04-10 13:15:02,967::train::INFO] [Train] Iter 19422 | Loss 0.274053 | Grad 0.0524 \n","[2024-04-10 13:15:03,086::train::INFO] [Train] Iter 19423 | Loss 0.309067 | Grad 0.0426 \n","[2024-04-10 13:15:03,205::train::INFO] [Train] Iter 19424 | Loss 0.318203 | Grad 0.0578 \n","[2024-04-10 13:15:03,328::train::INFO] [Train] Iter 19425 | Loss 0.301660 | Grad 0.0587 \n","[2024-04-10 13:15:03,447::train::INFO] [Train] Iter 19426 | Loss 0.335488 | Grad 0.0572 \n","[2024-04-10 13:15:03,567::train::INFO] [Train] Iter 19427 | Loss 0.323864 | Grad 0.0531 \n","[2024-04-10 13:15:03,686::train::INFO] [Train] Iter 19428 | Loss 0.332514 | Grad 0.0419 \n","[2024-04-10 13:15:03,806::train::INFO] [Train] Iter 19429 | Loss 0.354327 | Grad 0.0398 \n","[2024-04-10 13:15:03,926::train::INFO] [Train] Iter 19430 | Loss 0.329416 | Grad 0.0449 \n","[2024-04-10 13:15:04,047::train::INFO] [Train] Iter 19431 | Loss 0.295112 | Grad 0.0435 \n","[2024-04-10 13:15:04,166::train::INFO] [Train] Iter 19432 | Loss 0.320732 | Grad 0.0382 \n","[2024-04-10 13:15:04,285::train::INFO] [Train] Iter 19433 | Loss 0.350832 | Grad 0.0535 \n","[2024-04-10 13:15:04,404::train::INFO] [Train] Iter 19434 | Loss 0.300765 | Grad 0.0392 \n","[2024-04-10 13:15:04,523::train::INFO] [Train] Iter 19435 | Loss 0.264388 | Grad 0.0517 \n","[2024-04-10 13:15:04,642::train::INFO] [Train] Iter 19436 | Loss 0.327071 | Grad 0.0384 \n","[2024-04-10 13:15:04,760::train::INFO] [Train] Iter 19437 | Loss 0.313137 | Grad 0.0409 \n","[2024-04-10 13:15:04,878::train::INFO] [Train] Iter 19438 | Loss 0.282277 | Grad 0.0391 \n","[2024-04-10 13:15:04,999::train::INFO] [Train] Iter 19439 | Loss 0.351025 | Grad 0.0557 \n","[2024-04-10 13:15:05,074::train::INFO] [Train] Iter 19440 | Loss 0.330378 | Grad 0.0403 \n","[2024-04-10 13:15:05,193::train::INFO] [Train] Iter 19441 | Loss 0.300767 | Grad 0.0757 \n","[2024-04-10 13:15:05,312::train::INFO] [Train] Iter 19442 | Loss 0.333829 | Grad 0.0516 \n","[2024-04-10 13:15:05,433::train::INFO] [Train] Iter 19443 | Loss 0.282100 | Grad 0.0370 \n","[2024-04-10 13:15:05,554::train::INFO] [Train] Iter 19444 | Loss 0.303229 | Grad 0.0459 \n","[2024-04-10 13:15:05,673::train::INFO] [Train] Iter 19445 | Loss 0.339302 | Grad 0.0600 \n","[2024-04-10 13:15:05,795::train::INFO] [Train] Iter 19446 | Loss 0.274481 | Grad 0.0303 \n","[2024-04-10 13:15:05,915::train::INFO] [Train] Iter 19447 | Loss 0.295963 | Grad 0.0358 \n","[2024-04-10 13:15:06,034::train::INFO] [Train] Iter 19448 | Loss 0.312286 | Grad 0.0503 \n","[2024-04-10 13:15:06,156::train::INFO] [Train] Iter 19449 | Loss 0.301350 | Grad 0.0473 \n","[2024-04-10 13:15:06,277::train::INFO] [Train] Iter 19450 | Loss 0.297898 | Grad 0.0352 \n","[2024-04-10 13:15:06,399::train::INFO] [Train] Iter 19451 | Loss 0.318380 | Grad 0.0497 \n","[2024-04-10 13:15:06,520::train::INFO] [Train] Iter 19452 | Loss 0.315989 | Grad 0.0556 \n","[2024-04-10 13:15:06,642::train::INFO] [Train] Iter 19453 | Loss 0.326848 | Grad 0.0522 \n","[2024-04-10 13:15:06,768::train::INFO] [Train] Iter 19454 | Loss 0.367310 | Grad 0.0722 \n","[2024-04-10 13:15:06,890::train::INFO] [Train] Iter 19455 | Loss 0.291975 | Grad 0.0291 \n","[2024-04-10 13:15:07,012::train::INFO] [Train] Iter 19456 | Loss 0.281906 | Grad 0.0317 \n","[2024-04-10 13:15:07,132::train::INFO] [Train] Iter 19457 | Loss 0.284895 | Grad 0.0385 \n","[2024-04-10 13:15:07,253::train::INFO] [Train] Iter 19458 | Loss 0.301337 | Grad 0.0605 \n","[2024-04-10 13:15:07,373::train::INFO] [Train] Iter 19459 | Loss 0.278244 | Grad 0.0539 \n","[2024-04-10 13:15:07,492::train::INFO] [Train] Iter 19460 | Loss 0.322921 | Grad 0.0633 \n","[2024-04-10 13:15:07,612::train::INFO] [Train] Iter 19461 | Loss 0.229986 | Grad 0.0242 \n","[2024-04-10 13:15:07,733::train::INFO] [Train] Iter 19462 | Loss 0.319421 | Grad 0.0585 \n","[2024-04-10 13:15:07,853::train::INFO] [Train] Iter 19463 | Loss 0.313877 | Grad 0.0544 \n","[2024-04-10 13:15:07,975::train::INFO] [Train] Iter 19464 | Loss 0.348251 | Grad 0.0592 \n","[2024-04-10 13:15:08,096::train::INFO] [Train] Iter 19465 | Loss 0.372727 | Grad 0.0839 \n","[2024-04-10 13:15:08,217::train::INFO] [Train] Iter 19466 | Loss 0.292771 | Grad 0.0505 \n","[2024-04-10 13:15:08,337::train::INFO] [Train] Iter 19467 | Loss 0.264900 | Grad 0.0412 \n","[2024-04-10 13:15:08,458::train::INFO] [Train] Iter 19468 | Loss 0.348141 | Grad 0.0748 \n","[2024-04-10 13:15:08,577::train::INFO] [Train] Iter 19469 | Loss 0.319429 | Grad 0.0821 \n","[2024-04-10 13:15:08,695::train::INFO] [Train] Iter 19470 | Loss 0.334008 | Grad 0.0495 \n","[2024-04-10 13:15:08,815::train::INFO] [Train] Iter 19471 | Loss 0.327545 | Grad 0.0618 \n","[2024-04-10 13:15:08,934::train::INFO] [Train] Iter 19472 | Loss 0.289050 | Grad 0.0360 \n","[2024-04-10 13:15:09,053::train::INFO] [Train] Iter 19473 | Loss 0.299498 | Grad 0.0497 \n","[2024-04-10 13:15:09,175::train::INFO] [Train] Iter 19474 | Loss 0.271712 | Grad 0.0358 \n","[2024-04-10 13:15:09,296::train::INFO] [Train] Iter 19475 | Loss 0.291369 | Grad 0.0389 \n","[2024-04-10 13:15:09,417::train::INFO] [Train] Iter 19476 | Loss 0.308349 | Grad 0.0313 \n","[2024-04-10 13:15:09,536::train::INFO] [Train] Iter 19477 | Loss 0.282734 | Grad 0.0465 \n","[2024-04-10 13:15:09,655::train::INFO] [Train] Iter 19478 | Loss 0.320469 | Grad 0.0666 \n","[2024-04-10 13:15:09,778::train::INFO] [Train] Iter 19479 | Loss 0.329761 | Grad 0.0455 \n","[2024-04-10 13:15:09,898::train::INFO] [Train] Iter 19480 | Loss 0.290875 | Grad 0.0440 \n","[2024-04-10 13:15:10,020::train::INFO] [Train] Iter 19481 | Loss 0.324974 | Grad 0.0426 \n","[2024-04-10 13:15:10,142::train::INFO] [Train] Iter 19482 | Loss 0.312462 | Grad 0.0442 \n","[2024-04-10 13:15:10,263::train::INFO] [Train] Iter 19483 | Loss 0.319635 | Grad 0.0613 \n","[2024-04-10 13:15:10,388::train::INFO] [Train] Iter 19484 | Loss 0.341874 | Grad 0.0671 \n","[2024-04-10 13:15:10,463::train::INFO] [Train] Iter 19485 | Loss 0.375358 | Grad 0.0529 \n","[2024-04-10 13:15:10,581::train::INFO] [Train] Iter 19486 | Loss 0.310686 | Grad 0.0287 \n","[2024-04-10 13:15:10,700::train::INFO] [Train] Iter 19487 | Loss 0.312683 | Grad 0.0325 \n","[2024-04-10 13:15:10,818::train::INFO] [Train] Iter 19488 | Loss 0.317563 | Grad 0.0546 \n","[2024-04-10 13:15:10,936::train::INFO] [Train] Iter 19489 | Loss 0.337073 | Grad 0.0610 \n","[2024-04-10 13:15:11,055::train::INFO] [Train] Iter 19490 | Loss 0.258109 | Grad 0.0246 \n","[2024-04-10 13:15:11,175::train::INFO] [Train] Iter 19491 | Loss 0.331154 | Grad 0.0439 \n","[2024-04-10 13:15:11,293::train::INFO] [Train] Iter 19492 | Loss 0.364973 | Grad 0.0556 \n","[2024-04-10 13:15:11,412::train::INFO] [Train] Iter 19493 | Loss 0.315639 | Grad 0.0384 \n","[2024-04-10 13:15:11,531::train::INFO] [Train] Iter 19494 | Loss 0.314463 | Grad 0.0420 \n","[2024-04-10 13:15:11,651::train::INFO] [Train] Iter 19495 | Loss 0.298892 | Grad 0.0335 \n","[2024-04-10 13:15:11,771::train::INFO] [Train] Iter 19496 | Loss 0.355588 | Grad 0.0731 \n","[2024-04-10 13:15:11,892::train::INFO] [Train] Iter 19497 | Loss 0.338238 | Grad 0.0505 \n","[2024-04-10 13:15:12,012::train::INFO] [Train] Iter 19498 | Loss 0.302286 | Grad 0.0396 \n","[2024-04-10 13:15:12,133::train::INFO] [Train] Iter 19499 | Loss 0.298992 | Grad 0.0348 \n","[2024-04-10 13:15:12,253::train::INFO] [Train] Iter 19500 | Loss 0.312115 | Grad 0.0306 \n","[2024-04-10 13:15:12,372::train::INFO] [Train] Iter 19501 | Loss 0.336775 | Grad 0.0432 \n","[2024-04-10 13:15:12,493::train::INFO] [Train] Iter 19502 | Loss 0.269928 | Grad 0.0352 \n","[2024-04-10 13:15:12,616::train::INFO] [Train] Iter 19503 | Loss 0.316641 | Grad 0.0329 \n","[2024-04-10 13:15:12,737::train::INFO] [Train] Iter 19504 | Loss 0.366179 | Grad 0.0789 \n","[2024-04-10 13:15:12,857::train::INFO] [Train] Iter 19505 | Loss 0.324861 | Grad 0.0635 \n","[2024-04-10 13:15:12,977::train::INFO] [Train] Iter 19506 | Loss 0.320143 | Grad 0.0403 \n","[2024-04-10 13:15:13,096::train::INFO] [Train] Iter 19507 | Loss 0.347382 | Grad 0.0385 \n","[2024-04-10 13:15:13,218::train::INFO] [Train] Iter 19508 | Loss 0.276553 | Grad 0.0383 \n","[2024-04-10 13:15:13,340::train::INFO] [Train] Iter 19509 | Loss 0.310682 | Grad 0.0502 \n","[2024-04-10 13:15:13,461::train::INFO] [Train] Iter 19510 | Loss 0.308319 | Grad 0.0425 \n","[2024-04-10 13:15:13,581::train::INFO] [Train] Iter 19511 | Loss 0.336545 | Grad 0.0442 \n","[2024-04-10 13:15:13,703::train::INFO] [Train] Iter 19512 | Loss 0.298967 | Grad 0.0309 \n","[2024-04-10 13:15:13,822::train::INFO] [Train] Iter 19513 | Loss 0.335405 | Grad 0.0426 \n","[2024-04-10 13:15:13,942::train::INFO] [Train] Iter 19514 | Loss 0.276093 | Grad 0.0335 \n","[2024-04-10 13:15:14,061::train::INFO] [Train] Iter 19515 | Loss 0.360323 | Grad 0.0691 \n","[2024-04-10 13:15:14,181::train::INFO] [Train] Iter 19516 | Loss 0.323624 | Grad 0.0705 \n","[2024-04-10 13:15:14,300::train::INFO] [Train] Iter 19517 | Loss 0.311602 | Grad 0.0631 \n","[2024-04-10 13:15:14,419::train::INFO] [Train] Iter 19518 | Loss 0.319552 | Grad 0.0529 \n","[2024-04-10 13:15:14,537::train::INFO] [Train] Iter 19519 | Loss 0.322364 | Grad 0.0678 \n","[2024-04-10 13:15:14,655::train::INFO] [Train] Iter 19520 | Loss 0.303083 | Grad 0.0455 \n","[2024-04-10 13:15:14,773::train::INFO] [Train] Iter 19521 | Loss 0.294864 | Grad 0.0348 \n","[2024-04-10 13:15:14,891::train::INFO] [Train] Iter 19522 | Loss 0.289980 | Grad 0.0335 \n","[2024-04-10 13:15:15,011::train::INFO] [Train] Iter 19523 | Loss 0.344790 | Grad 0.0905 \n","[2024-04-10 13:15:15,130::train::INFO] [Train] Iter 19524 | Loss 0.322376 | Grad 0.0469 \n","[2024-04-10 13:15:15,250::train::INFO] [Train] Iter 19525 | Loss 0.329039 | Grad 0.0467 \n","[2024-04-10 13:15:15,371::train::INFO] [Train] Iter 19526 | Loss 0.311366 | Grad 0.0557 \n","[2024-04-10 13:15:15,491::train::INFO] [Train] Iter 19527 | Loss 0.342369 | Grad 0.0745 \n","[2024-04-10 13:15:15,609::train::INFO] [Train] Iter 19528 | Loss 0.344298 | Grad 0.0477 \n","[2024-04-10 13:15:15,730::train::INFO] [Train] Iter 19529 | Loss 0.344690 | Grad 0.0513 \n","[2024-04-10 13:15:15,805::train::INFO] [Train] Iter 19530 | Loss 0.319558 | Grad 0.0603 \n","[2024-04-10 13:15:15,925::train::INFO] [Train] Iter 19531 | Loss 0.332413 | Grad 0.0670 \n","[2024-04-10 13:15:16,043::train::INFO] [Train] Iter 19532 | Loss 0.286670 | Grad 0.0463 \n","[2024-04-10 13:15:16,161::train::INFO] [Train] Iter 19533 | Loss 0.332714 | Grad 0.0425 \n","[2024-04-10 13:15:16,283::train::INFO] [Train] Iter 19534 | Loss 0.307422 | Grad 0.0402 \n","[2024-04-10 13:15:16,402::train::INFO] [Train] Iter 19535 | Loss 0.306726 | Grad 0.0364 \n","[2024-04-10 13:15:16,520::train::INFO] [Train] Iter 19536 | Loss 0.288858 | Grad 0.0335 \n","[2024-04-10 13:15:16,639::train::INFO] [Train] Iter 19537 | Loss 0.323469 | Grad 0.0525 \n","[2024-04-10 13:15:16,759::train::INFO] [Train] Iter 19538 | Loss 0.310678 | Grad 0.0500 \n","[2024-04-10 13:15:16,878::train::INFO] [Train] Iter 19539 | Loss 0.295949 | Grad 0.0322 \n","[2024-04-10 13:15:16,998::train::INFO] [Train] Iter 19540 | Loss 0.297088 | Grad 0.0426 \n","[2024-04-10 13:15:17,117::train::INFO] [Train] Iter 19541 | Loss 0.354709 | Grad 0.0515 \n","[2024-04-10 13:15:17,239::train::INFO] [Train] Iter 19542 | Loss 0.314198 | Grad 0.0355 \n","[2024-04-10 13:15:17,359::train::INFO] [Train] Iter 19543 | Loss 0.291180 | Grad 0.0355 \n","[2024-04-10 13:15:17,480::train::INFO] [Train] Iter 19544 | Loss 0.315985 | Grad 0.0450 \n","[2024-04-10 13:15:17,599::train::INFO] [Train] Iter 19545 | Loss 0.338824 | Grad 0.0389 \n","[2024-04-10 13:15:17,719::train::INFO] [Train] Iter 19546 | Loss 0.292001 | Grad 0.0292 \n","[2024-04-10 13:15:17,838::train::INFO] [Train] Iter 19547 | Loss 0.276526 | Grad 0.0432 \n","[2024-04-10 13:15:17,956::train::INFO] [Train] Iter 19548 | Loss 0.293173 | Grad 0.0485 \n","[2024-04-10 13:15:18,073::train::INFO] [Train] Iter 19549 | Loss 0.307929 | Grad 0.0451 \n","[2024-04-10 13:15:18,193::train::INFO] [Train] Iter 19550 | Loss 0.297923 | Grad 0.0591 \n","[2024-04-10 13:15:18,313::train::INFO] [Train] Iter 19551 | Loss 0.326187 | Grad 0.0674 \n","[2024-04-10 13:15:18,432::train::INFO] [Train] Iter 19552 | Loss 0.299869 | Grad 0.0550 \n","[2024-04-10 13:15:18,557::train::INFO] [Train] Iter 19553 | Loss 0.318344 | Grad 0.0602 \n","[2024-04-10 13:15:18,680::train::INFO] [Train] Iter 19554 | Loss 0.296826 | Grad 0.0339 \n","[2024-04-10 13:15:18,801::train::INFO] [Train] Iter 19555 | Loss 0.299509 | Grad 0.0436 \n","[2024-04-10 13:15:18,919::train::INFO] [Train] Iter 19556 | Loss 0.293682 | Grad 0.0362 \n","[2024-04-10 13:15:19,040::train::INFO] [Train] Iter 19557 | Loss 0.294770 | Grad 0.0424 \n","[2024-04-10 13:15:19,163::train::INFO] [Train] Iter 19558 | Loss 0.291009 | Grad 0.0416 \n","[2024-04-10 13:15:19,287::train::INFO] [Train] Iter 19559 | Loss 0.282032 | Grad 0.0372 \n","[2024-04-10 13:15:19,407::train::INFO] [Train] Iter 19560 | Loss 0.311560 | Grad 0.0499 \n","[2024-04-10 13:15:19,527::train::INFO] [Train] Iter 19561 | Loss 0.263393 | Grad 0.0262 \n","[2024-04-10 13:15:19,650::train::INFO] [Train] Iter 19562 | Loss 0.286887 | Grad 0.0346 \n","[2024-04-10 13:15:19,773::train::INFO] [Train] Iter 19563 | Loss 0.317323 | Grad 0.0501 \n","[2024-04-10 13:15:19,895::train::INFO] [Train] Iter 19564 | Loss 0.336610 | Grad 0.0466 \n","[2024-04-10 13:15:20,018::train::INFO] [Train] Iter 19565 | Loss 0.358732 | Grad 0.0493 \n","[2024-04-10 13:15:20,141::train::INFO] [Train] Iter 19566 | Loss 0.313043 | Grad 0.0560 \n","[2024-04-10 13:15:20,264::train::INFO] [Train] Iter 19567 | Loss 0.334675 | Grad 0.0506 \n","[2024-04-10 13:15:20,386::train::INFO] [Train] Iter 19568 | Loss 0.320365 | Grad 0.0357 \n","[2024-04-10 13:15:20,506::train::INFO] [Train] Iter 19569 | Loss 0.322414 | Grad 0.0564 \n","[2024-04-10 13:15:20,626::train::INFO] [Train] Iter 19570 | Loss 0.335548 | Grad 0.0741 \n","[2024-04-10 13:15:20,747::train::INFO] [Train] Iter 19571 | Loss 0.334309 | Grad 0.0658 \n","[2024-04-10 13:15:20,866::train::INFO] [Train] Iter 19572 | Loss 0.269088 | Grad 0.0575 \n","[2024-04-10 13:15:20,991::train::INFO] [Train] Iter 19573 | Loss 0.297549 | Grad 0.0607 \n","[2024-04-10 13:15:21,113::train::INFO] [Train] Iter 19574 | Loss 0.323073 | Grad 0.0550 \n","[2024-04-10 13:15:21,189::train::INFO] [Train] Iter 19575 | Loss 0.349106 | Grad 0.0462 \n","[2024-04-10 13:15:21,312::train::INFO] [Train] Iter 19576 | Loss 0.321922 | Grad 0.0579 \n","[2024-04-10 13:15:21,434::train::INFO] [Train] Iter 19577 | Loss 0.321306 | Grad 0.0323 \n","[2024-04-10 13:15:21,559::train::INFO] [Train] Iter 19578 | Loss 0.345750 | Grad 0.0476 \n","[2024-04-10 13:15:21,678::train::INFO] [Train] Iter 19579 | Loss 0.300610 | Grad 0.0471 \n","[2024-04-10 13:15:21,797::train::INFO] [Train] Iter 19580 | Loss 0.303036 | Grad 0.0472 \n","[2024-04-10 13:15:21,914::train::INFO] [Train] Iter 19581 | Loss 0.328949 | Grad 0.0425 \n","[2024-04-10 13:15:22,032::train::INFO] [Train] Iter 19582 | Loss 0.291162 | Grad 0.0422 \n","[2024-04-10 13:15:22,152::train::INFO] [Train] Iter 19583 | Loss 0.279973 | Grad 0.0465 \n","[2024-04-10 13:15:22,272::train::INFO] [Train] Iter 19584 | Loss 0.316453 | Grad 0.0420 \n","[2024-04-10 13:15:22,391::train::INFO] [Train] Iter 19585 | Loss 0.278003 | Grad 0.0566 \n","[2024-04-10 13:15:22,511::train::INFO] [Train] Iter 19586 | Loss 0.314574 | Grad 0.0428 \n","[2024-04-10 13:15:22,631::train::INFO] [Train] Iter 19587 | Loss 0.346798 | Grad 0.0500 \n","[2024-04-10 13:15:22,751::train::INFO] [Train] Iter 19588 | Loss 0.317689 | Grad 0.0415 \n","[2024-04-10 13:15:22,873::train::INFO] [Train] Iter 19589 | Loss 0.317568 | Grad 0.0496 \n","[2024-04-10 13:15:22,991::train::INFO] [Train] Iter 19590 | Loss 0.305652 | Grad 0.0347 \n","[2024-04-10 13:15:23,110::train::INFO] [Train] Iter 19591 | Loss 0.305466 | Grad 0.0419 \n","[2024-04-10 13:15:23,230::train::INFO] [Train] Iter 19592 | Loss 0.293756 | Grad 0.0512 \n","[2024-04-10 13:15:23,351::train::INFO] [Train] Iter 19593 | Loss 0.323415 | Grad 0.0439 \n","[2024-04-10 13:15:23,474::train::INFO] [Train] Iter 19594 | Loss 0.348478 | Grad 0.0765 \n","[2024-04-10 13:15:23,593::train::INFO] [Train] Iter 19595 | Loss 0.305949 | Grad 0.0342 \n","[2024-04-10 13:15:23,713::train::INFO] [Train] Iter 19596 | Loss 0.285954 | Grad 0.0318 \n","[2024-04-10 13:15:23,832::train::INFO] [Train] Iter 19597 | Loss 0.333872 | Grad 0.0604 \n","[2024-04-10 13:15:23,952::train::INFO] [Train] Iter 19598 | Loss 0.275062 | Grad 0.0352 \n","[2024-04-10 13:15:24,075::train::INFO] [Train] Iter 19599 | Loss 0.274092 | Grad 0.0332 \n","[2024-04-10 13:15:24,195::train::INFO] [Train] Iter 19600 | Loss 0.334196 | Grad 0.0419 \n","[2024-04-10 13:15:24,314::train::INFO] [Train] Iter 19601 | Loss 0.325035 | Grad 0.0460 \n","[2024-04-10 13:15:24,433::train::INFO] [Train] Iter 19602 | Loss 0.337599 | Grad 0.0542 \n","[2024-04-10 13:15:24,556::train::INFO] [Train] Iter 19603 | Loss 0.314555 | Grad 0.0458 \n","[2024-04-10 13:15:24,676::train::INFO] [Train] Iter 19604 | Loss 0.284061 | Grad 0.0377 \n","[2024-04-10 13:15:24,796::train::INFO] [Train] Iter 19605 | Loss 0.284650 | Grad 0.0442 \n","[2024-04-10 13:15:24,915::train::INFO] [Train] Iter 19606 | Loss 0.329697 | Grad 0.0822 \n","[2024-04-10 13:15:25,036::train::INFO] [Train] Iter 19607 | Loss 0.301599 | Grad 0.0485 \n","[2024-04-10 13:15:25,156::train::INFO] [Train] Iter 19608 | Loss 0.300641 | Grad 0.0368 \n","[2024-04-10 13:15:25,276::train::INFO] [Train] Iter 19609 | Loss 0.284811 | Grad 0.0297 \n","[2024-04-10 13:15:25,399::train::INFO] [Train] Iter 19610 | Loss 0.305690 | Grad 0.0528 \n","[2024-04-10 13:15:25,518::train::INFO] [Train] Iter 19611 | Loss 0.320847 | Grad 0.0473 \n","[2024-04-10 13:15:25,637::train::INFO] [Train] Iter 19612 | Loss 0.296917 | Grad 0.0350 \n","[2024-04-10 13:15:25,758::train::INFO] [Train] Iter 19613 | Loss 0.318513 | Grad 0.0481 \n","[2024-04-10 13:15:25,879::train::INFO] [Train] Iter 19614 | Loss 0.312498 | Grad 0.0541 \n","[2024-04-10 13:15:26,000::train::INFO] [Train] Iter 19615 | Loss 0.292048 | Grad 0.0471 \n","[2024-04-10 13:15:26,120::train::INFO] [Train] Iter 19616 | Loss 0.272496 | Grad 0.0427 \n","[2024-04-10 13:15:26,240::train::INFO] [Train] Iter 19617 | Loss 0.360504 | Grad 0.0881 \n","[2024-04-10 13:15:26,361::train::INFO] [Train] Iter 19618 | Loss 0.333450 | Grad 0.0613 \n","[2024-04-10 13:15:26,481::train::INFO] [Train] Iter 19619 | Loss 0.303767 | Grad 0.0387 \n","[2024-04-10 13:15:26,557::train::INFO] [Train] Iter 19620 | Loss 0.303723 | Grad 0.0394 \n","[2024-04-10 13:15:26,676::train::INFO] [Train] Iter 19621 | Loss 0.299044 | Grad 0.0405 \n","[2024-04-10 13:15:26,795::train::INFO] [Train] Iter 19622 | Loss 0.328541 | Grad 0.0412 \n","[2024-04-10 13:15:26,914::train::INFO] [Train] Iter 19623 | Loss 0.275609 | Grad 0.0372 \n","[2024-04-10 13:15:27,033::train::INFO] [Train] Iter 19624 | Loss 0.326023 | Grad 0.0628 \n","[2024-04-10 13:15:27,153::train::INFO] [Train] Iter 19625 | Loss 0.290880 | Grad 0.0460 \n","[2024-04-10 13:15:27,273::train::INFO] [Train] Iter 19626 | Loss 0.316137 | Grad 0.0411 \n","[2024-04-10 13:15:27,394::train::INFO] [Train] Iter 19627 | Loss 0.292729 | Grad 0.0385 \n","[2024-04-10 13:15:27,515::train::INFO] [Train] Iter 19628 | Loss 0.299682 | Grad 0.0436 \n","[2024-04-10 13:15:27,636::train::INFO] [Train] Iter 19629 | Loss 0.301240 | Grad 0.0335 \n","[2024-04-10 13:15:27,755::train::INFO] [Train] Iter 19630 | Loss 0.389520 | Grad 0.0615 \n","[2024-04-10 13:15:27,876::train::INFO] [Train] Iter 19631 | Loss 0.275017 | Grad 0.0470 \n","[2024-04-10 13:15:27,996::train::INFO] [Train] Iter 19632 | Loss 0.296319 | Grad 0.0440 \n","[2024-04-10 13:15:28,120::train::INFO] [Train] Iter 19633 | Loss 0.278674 | Grad 0.0345 \n","[2024-04-10 13:15:28,239::train::INFO] [Train] Iter 19634 | Loss 0.337701 | Grad 0.0388 \n","[2024-04-10 13:15:28,358::train::INFO] [Train] Iter 19635 | Loss 0.332229 | Grad 0.0581 \n","[2024-04-10 13:15:28,477::train::INFO] [Train] Iter 19636 | Loss 0.321157 | Grad 0.0484 \n","[2024-04-10 13:15:28,596::train::INFO] [Train] Iter 19637 | Loss 0.343921 | Grad 0.0538 \n","[2024-04-10 13:15:28,721::train::INFO] [Train] Iter 19638 | Loss 0.316165 | Grad 0.0453 \n","[2024-04-10 13:15:28,840::train::INFO] [Train] Iter 19639 | Loss 0.319105 | Grad 0.0631 \n","[2024-04-10 13:15:28,959::train::INFO] [Train] Iter 19640 | Loss 0.301744 | Grad 0.0588 \n","[2024-04-10 13:15:29,079::train::INFO] [Train] Iter 19641 | Loss 0.294850 | Grad 0.0511 \n","[2024-04-10 13:15:29,198::train::INFO] [Train] Iter 19642 | Loss 0.321594 | Grad 0.0556 \n","[2024-04-10 13:15:29,318::train::INFO] [Train] Iter 19643 | Loss 0.280844 | Grad 0.0456 \n","[2024-04-10 13:15:29,438::train::INFO] [Train] Iter 19644 | Loss 0.335905 | Grad 0.0593 \n","[2024-04-10 13:15:29,558::train::INFO] [Train] Iter 19645 | Loss 0.299342 | Grad 0.0338 \n","[2024-04-10 13:15:29,679::train::INFO] [Train] Iter 19646 | Loss 0.295721 | Grad 0.0589 \n","[2024-04-10 13:15:29,800::train::INFO] [Train] Iter 19647 | Loss 0.264058 | Grad 0.0339 \n","[2024-04-10 13:15:29,919::train::INFO] [Train] Iter 19648 | Loss 0.321171 | Grad 0.0346 \n","[2024-04-10 13:15:30,040::train::INFO] [Train] Iter 19649 | Loss 0.261242 | Grad 0.0275 \n","[2024-04-10 13:15:30,160::train::INFO] [Train] Iter 19650 | Loss 0.347430 | Grad 0.0579 \n","[2024-04-10 13:15:30,280::train::INFO] [Train] Iter 19651 | Loss 0.298677 | Grad 0.0500 \n","[2024-04-10 13:15:30,400::train::INFO] [Train] Iter 19652 | Loss 0.308575 | Grad 0.0475 \n","[2024-04-10 13:15:30,519::train::INFO] [Train] Iter 19653 | Loss 0.319836 | Grad 0.0548 \n","[2024-04-10 13:15:30,639::train::INFO] [Train] Iter 19654 | Loss 0.304071 | Grad 0.0327 \n","[2024-04-10 13:15:30,762::train::INFO] [Train] Iter 19655 | Loss 0.340546 | Grad 0.0458 \n","[2024-04-10 13:15:30,883::train::INFO] [Train] Iter 19656 | Loss 0.323539 | Grad 0.0709 \n","[2024-04-10 13:15:31,002::train::INFO] [Train] Iter 19657 | Loss 0.292470 | Grad 0.0338 \n","[2024-04-10 13:15:31,122::train::INFO] [Train] Iter 19658 | Loss 0.288957 | Grad 0.0392 \n","[2024-04-10 13:15:31,241::train::INFO] [Train] Iter 19659 | Loss 0.334727 | Grad 0.0413 \n","[2024-04-10 13:15:31,362::train::INFO] [Train] Iter 19660 | Loss 0.272230 | Grad 0.0404 \n","[2024-04-10 13:15:31,480::train::INFO] [Train] Iter 19661 | Loss 0.287345 | Grad 0.0565 \n","[2024-04-10 13:15:31,598::train::INFO] [Train] Iter 19662 | Loss 0.343914 | Grad 0.0874 \n","[2024-04-10 13:15:31,722::train::INFO] [Train] Iter 19663 | Loss 0.333641 | Grad 0.0766 \n","[2024-04-10 13:15:31,842::train::INFO] [Train] Iter 19664 | Loss 0.348991 | Grad 0.0638 \n","[2024-04-10 13:15:31,919::train::INFO] [Train] Iter 19665 | Loss 0.301195 | Grad 0.0335 \n","[2024-04-10 13:15:32,044::train::INFO] [Train] Iter 19666 | Loss 0.293281 | Grad 0.0310 \n","[2024-04-10 13:15:32,164::train::INFO] [Train] Iter 19667 | Loss 0.340661 | Grad 0.0415 \n","[2024-04-10 13:15:32,284::train::INFO] [Train] Iter 19668 | Loss 0.331206 | Grad 0.0536 \n","[2024-04-10 13:15:32,407::train::INFO] [Train] Iter 19669 | Loss 0.328413 | Grad 0.0460 \n","[2024-04-10 13:15:32,527::train::INFO] [Train] Iter 19670 | Loss 0.361854 | Grad 0.0453 \n","[2024-04-10 13:15:32,647::train::INFO] [Train] Iter 19671 | Loss 0.282181 | Grad 0.0391 \n","[2024-04-10 13:15:32,775::train::INFO] [Train] Iter 19672 | Loss 0.298298 | Grad 0.0397 \n","[2024-04-10 13:15:32,901::train::INFO] [Train] Iter 19673 | Loss 0.320739 | Grad 0.0429 \n","[2024-04-10 13:15:33,022::train::INFO] [Train] Iter 19674 | Loss 0.336272 | Grad 0.0448 \n","[2024-04-10 13:15:33,146::train::INFO] [Train] Iter 19675 | Loss 0.350411 | Grad 0.0386 \n","[2024-04-10 13:15:33,267::train::INFO] [Train] Iter 19676 | Loss 0.338635 | Grad 0.0484 \n","[2024-04-10 13:15:33,391::train::INFO] [Train] Iter 19677 | Loss 0.358484 | Grad 0.0490 \n","[2024-04-10 13:15:33,511::train::INFO] [Train] Iter 19678 | Loss 0.330513 | Grad 0.0297 \n","[2024-04-10 13:15:33,631::train::INFO] [Train] Iter 19679 | Loss 0.308615 | Grad 0.0343 \n","[2024-04-10 13:15:33,750::train::INFO] [Train] Iter 19680 | Loss 0.322164 | Grad 0.0539 \n","[2024-04-10 13:15:33,872::train::INFO] [Train] Iter 19681 | Loss 0.337205 | Grad 0.0360 \n","[2024-04-10 13:15:33,992::train::INFO] [Train] Iter 19682 | Loss 0.293122 | Grad 0.0488 \n","[2024-04-10 13:15:34,113::train::INFO] [Train] Iter 19683 | Loss 0.282862 | Grad 0.0466 \n","[2024-04-10 13:15:34,235::train::INFO] [Train] Iter 19684 | Loss 0.327340 | Grad 0.0637 \n","[2024-04-10 13:15:34,357::train::INFO] [Train] Iter 19685 | Loss 0.312048 | Grad 0.0502 \n","[2024-04-10 13:15:34,480::train::INFO] [Train] Iter 19686 | Loss 0.334988 | Grad 0.0661 \n","[2024-04-10 13:15:34,604::train::INFO] [Train] Iter 19687 | Loss 0.236797 | Grad 0.0484 \n","[2024-04-10 13:15:34,726::train::INFO] [Train] Iter 19688 | Loss 0.299148 | Grad 0.0509 \n","[2024-04-10 13:15:34,848::train::INFO] [Train] Iter 19689 | Loss 0.350362 | Grad 0.0391 \n","[2024-04-10 13:15:34,968::train::INFO] [Train] Iter 19690 | Loss 0.341392 | Grad 0.0410 \n","[2024-04-10 13:15:35,088::train::INFO] [Train] Iter 19691 | Loss 0.307859 | Grad 0.0404 \n","[2024-04-10 13:15:35,208::train::INFO] [Train] Iter 19692 | Loss 0.304120 | Grad 0.0470 \n","[2024-04-10 13:15:35,328::train::INFO] [Train] Iter 19693 | Loss 0.320950 | Grad 0.0385 \n","[2024-04-10 13:15:35,448::train::INFO] [Train] Iter 19694 | Loss 0.301320 | Grad 0.0372 \n","[2024-04-10 13:15:35,568::train::INFO] [Train] Iter 19695 | Loss 0.298052 | Grad 0.0406 \n","[2024-04-10 13:15:35,690::train::INFO] [Train] Iter 19696 | Loss 0.324383 | Grad 0.0334 \n","[2024-04-10 13:15:35,810::train::INFO] [Train] Iter 19697 | Loss 0.300956 | Grad 0.0385 \n","[2024-04-10 13:15:35,929::train::INFO] [Train] Iter 19698 | Loss 0.312047 | Grad 0.0303 \n","[2024-04-10 13:15:36,048::train::INFO] [Train] Iter 19699 | Loss 0.304923 | Grad 0.0354 \n","[2024-04-10 13:15:36,168::train::INFO] [Train] Iter 19700 | Loss 0.328629 | Grad 0.0548 \n","[2024-04-10 13:15:36,289::train::INFO] [Train] Iter 19701 | Loss 0.323278 | Grad 0.0430 \n","[2024-04-10 13:15:36,409::train::INFO] [Train] Iter 19702 | Loss 0.313996 | Grad 0.0295 \n","[2024-04-10 13:15:36,527::train::INFO] [Train] Iter 19703 | Loss 0.286419 | Grad 0.0388 \n","[2024-04-10 13:15:36,645::train::INFO] [Train] Iter 19704 | Loss 0.337346 | Grad 0.0352 \n","[2024-04-10 13:15:36,765::train::INFO] [Train] Iter 19705 | Loss 0.294134 | Grad 0.0450 \n","[2024-04-10 13:15:36,885::train::INFO] [Train] Iter 19706 | Loss 0.338416 | Grad 0.0684 \n","[2024-04-10 13:15:37,004::train::INFO] [Train] Iter 19707 | Loss 0.323745 | Grad 0.0838 \n","[2024-04-10 13:15:37,124::train::INFO] [Train] Iter 19708 | Loss 0.331977 | Grad 0.0588 \n","[2024-04-10 13:15:37,244::train::INFO] [Train] Iter 19709 | Loss 0.313736 | Grad 0.0571 \n","[2024-04-10 13:15:37,319::train::INFO] [Train] Iter 19710 | Loss 0.334551 | Grad 0.0606 \n","[2024-04-10 13:15:37,437::train::INFO] [Train] Iter 19711 | Loss 0.342866 | Grad 0.0518 \n","[2024-04-10 13:15:37,556::train::INFO] [Train] Iter 19712 | Loss 0.329461 | Grad 0.0318 \n","[2024-04-10 13:15:37,675::train::INFO] [Train] Iter 19713 | Loss 0.340882 | Grad 0.0395 \n","[2024-04-10 13:15:37,794::train::INFO] [Train] Iter 19714 | Loss 0.275043 | Grad 0.0324 \n","[2024-04-10 13:15:37,912::train::INFO] [Train] Iter 19715 | Loss 0.303825 | Grad 0.0433 \n","[2024-04-10 13:15:38,030::train::INFO] [Train] Iter 19716 | Loss 0.306632 | Grad 0.0445 \n","[2024-04-10 13:15:38,148::train::INFO] [Train] Iter 19717 | Loss 0.327450 | Grad 0.0654 \n","[2024-04-10 13:15:38,267::train::INFO] [Train] Iter 19718 | Loss 0.287978 | Grad 0.0344 \n","[2024-04-10 13:15:38,386::train::INFO] [Train] Iter 19719 | Loss 0.280893 | Grad 0.0313 \n","[2024-04-10 13:15:38,506::train::INFO] [Train] Iter 19720 | Loss 0.316132 | Grad 0.0338 \n","[2024-04-10 13:15:38,625::train::INFO] [Train] Iter 19721 | Loss 0.319001 | Grad 0.0507 \n","[2024-04-10 13:15:38,745::train::INFO] [Train] Iter 19722 | Loss 0.326652 | Grad 0.0684 \n","[2024-04-10 13:15:38,865::train::INFO] [Train] Iter 19723 | Loss 0.340618 | Grad 0.0466 \n","[2024-04-10 13:15:38,987::train::INFO] [Train] Iter 19724 | Loss 0.312218 | Grad 0.0457 \n","[2024-04-10 13:15:39,111::train::INFO] [Train] Iter 19725 | Loss 0.304709 | Grad 0.0421 \n","[2024-04-10 13:15:39,232::train::INFO] [Train] Iter 19726 | Loss 0.337953 | Grad 0.0474 \n","[2024-04-10 13:15:39,351::train::INFO] [Train] Iter 19727 | Loss 0.305429 | Grad 0.0554 \n","[2024-04-10 13:15:39,472::train::INFO] [Train] Iter 19728 | Loss 0.301449 | Grad 0.0537 \n","[2024-04-10 13:15:39,591::train::INFO] [Train] Iter 19729 | Loss 0.308420 | Grad 0.0318 \n","[2024-04-10 13:15:39,712::train::INFO] [Train] Iter 19730 | Loss 0.341790 | Grad 0.0412 \n","[2024-04-10 13:15:39,832::train::INFO] [Train] Iter 19731 | Loss 0.330449 | Grad 0.0596 \n","[2024-04-10 13:15:39,953::train::INFO] [Train] Iter 19732 | Loss 0.347534 | Grad 0.0961 \n","[2024-04-10 13:15:40,073::train::INFO] [Train] Iter 19733 | Loss 0.330509 | Grad 0.0705 \n","[2024-04-10 13:15:40,194::train::INFO] [Train] Iter 19734 | Loss 0.274324 | Grad 0.0441 \n","[2024-04-10 13:15:40,316::train::INFO] [Train] Iter 19735 | Loss 0.336565 | Grad 0.0455 \n","[2024-04-10 13:15:40,435::train::INFO] [Train] Iter 19736 | Loss 0.312247 | Grad 0.0718 \n","[2024-04-10 13:15:40,556::train::INFO] [Train] Iter 19737 | Loss 0.344888 | Grad 0.0539 \n","[2024-04-10 13:15:40,676::train::INFO] [Train] Iter 19738 | Loss 0.324640 | Grad 0.0462 \n","[2024-04-10 13:15:40,795::train::INFO] [Train] Iter 19739 | Loss 0.267308 | Grad 0.0354 \n","[2024-04-10 13:15:40,914::train::INFO] [Train] Iter 19740 | Loss 0.304969 | Grad 0.0636 \n","[2024-04-10 13:15:41,035::train::INFO] [Train] Iter 19741 | Loss 0.325787 | Grad 0.0415 \n","[2024-04-10 13:15:41,154::train::INFO] [Train] Iter 19742 | Loss 0.334556 | Grad 0.0551 \n","[2024-04-10 13:15:41,273::train::INFO] [Train] Iter 19743 | Loss 0.344979 | Grad 0.0831 \n","[2024-04-10 13:15:41,392::train::INFO] [Train] Iter 19744 | Loss 0.331972 | Grad 0.0350 \n","[2024-04-10 13:15:41,511::train::INFO] [Train] Iter 19745 | Loss 0.282484 | Grad 0.0509 \n","[2024-04-10 13:15:41,632::train::INFO] [Train] Iter 19746 | Loss 0.352028 | Grad 0.0651 \n","[2024-04-10 13:15:41,750::train::INFO] [Train] Iter 19747 | Loss 0.290762 | Grad 0.0276 \n","[2024-04-10 13:15:41,868::train::INFO] [Train] Iter 19748 | Loss 0.310490 | Grad 0.0465 \n","[2024-04-10 13:15:41,987::train::INFO] [Train] Iter 19749 | Loss 0.341260 | Grad 0.0583 \n","[2024-04-10 13:15:42,106::train::INFO] [Train] Iter 19750 | Loss 0.308487 | Grad 0.0405 \n","[2024-04-10 13:15:42,226::train::INFO] [Train] Iter 19751 | Loss 0.324329 | Grad 0.0416 \n","[2024-04-10 13:15:42,346::train::INFO] [Train] Iter 19752 | Loss 0.287663 | Grad 0.0296 \n","[2024-04-10 13:15:42,468::train::INFO] [Train] Iter 19753 | Loss 0.279178 | Grad 0.0771 \n","[2024-04-10 13:15:42,587::train::INFO] [Train] Iter 19754 | Loss 0.329004 | Grad 0.0739 \n","[2024-04-10 13:15:42,663::train::INFO] [Train] Iter 19755 | Loss 0.321629 | Grad 0.1024 \n","[2024-04-10 13:15:42,781::train::INFO] [Train] Iter 19756 | Loss 0.323343 | Grad 0.0417 \n","[2024-04-10 13:15:42,900::train::INFO] [Train] Iter 19757 | Loss 0.277518 | Grad 0.0320 \n","[2024-04-10 13:15:43,018::train::INFO] [Train] Iter 19758 | Loss 0.324910 | Grad 0.0809 \n","[2024-04-10 13:15:43,137::train::INFO] [Train] Iter 19759 | Loss 0.315983 | Grad 0.0640 \n","[2024-04-10 13:15:43,257::train::INFO] [Train] Iter 19760 | Loss 0.358629 | Grad 0.0695 \n","[2024-04-10 13:15:43,375::train::INFO] [Train] Iter 19761 | Loss 0.296825 | Grad 0.0378 \n","[2024-04-10 13:15:43,494::train::INFO] [Train] Iter 19762 | Loss 0.289208 | Grad 0.0321 \n","[2024-04-10 13:15:43,613::train::INFO] [Train] Iter 19763 | Loss 0.287485 | Grad 0.0410 \n","[2024-04-10 13:15:43,734::train::INFO] [Train] Iter 19764 | Loss 0.296623 | Grad 0.0471 \n","[2024-04-10 13:15:43,854::train::INFO] [Train] Iter 19765 | Loss 0.334941 | Grad 0.0613 \n","[2024-04-10 13:15:43,974::train::INFO] [Train] Iter 19766 | Loss 0.304435 | Grad 0.0550 \n","[2024-04-10 13:15:44,096::train::INFO] [Train] Iter 19767 | Loss 0.313414 | Grad 0.0455 \n","[2024-04-10 13:15:44,215::train::INFO] [Train] Iter 19768 | Loss 0.311905 | Grad 0.0488 \n","[2024-04-10 13:15:44,337::train::INFO] [Train] Iter 19769 | Loss 0.330930 | Grad 0.0693 \n","[2024-04-10 13:15:44,456::train::INFO] [Train] Iter 19770 | Loss 0.327929 | Grad 0.0572 \n","[2024-04-10 13:15:44,575::train::INFO] [Train] Iter 19771 | Loss 0.309131 | Grad 0.0486 \n","[2024-04-10 13:15:44,694::train::INFO] [Train] Iter 19772 | Loss 0.311357 | Grad 0.0803 \n","[2024-04-10 13:15:44,813::train::INFO] [Train] Iter 19773 | Loss 0.287191 | Grad 0.0529 \n","[2024-04-10 13:15:44,935::train::INFO] [Train] Iter 19774 | Loss 0.317890 | Grad 0.0496 \n","[2024-04-10 13:15:45,056::train::INFO] [Train] Iter 19775 | Loss 0.344217 | Grad 0.0667 \n","[2024-04-10 13:15:45,179::train::INFO] [Train] Iter 19776 | Loss 0.313365 | Grad 0.0522 \n","[2024-04-10 13:15:45,300::train::INFO] [Train] Iter 19777 | Loss 0.316650 | Grad 0.0551 \n","[2024-04-10 13:15:45,422::train::INFO] [Train] Iter 19778 | Loss 0.334994 | Grad 0.0670 \n","[2024-04-10 13:15:45,544::train::INFO] [Train] Iter 19779 | Loss 0.288209 | Grad 0.0558 \n","[2024-04-10 13:15:45,667::train::INFO] [Train] Iter 19780 | Loss 0.326289 | Grad 0.0681 \n","[2024-04-10 13:15:45,787::train::INFO] [Train] Iter 19781 | Loss 0.318299 | Grad 0.0523 \n","[2024-04-10 13:15:45,908::train::INFO] [Train] Iter 19782 | Loss 0.294562 | Grad 0.0351 \n","[2024-04-10 13:15:46,032::train::INFO] [Train] Iter 19783 | Loss 0.337619 | Grad 0.0487 \n","[2024-04-10 13:15:46,152::train::INFO] [Train] Iter 19784 | Loss 0.296241 | Grad 0.0616 \n","[2024-04-10 13:15:46,275::train::INFO] [Train] Iter 19785 | Loss 0.323995 | Grad 0.0547 \n","[2024-04-10 13:15:46,395::train::INFO] [Train] Iter 19786 | Loss 0.350897 | Grad 0.0736 \n","[2024-04-10 13:15:46,515::train::INFO] [Train] Iter 19787 | Loss 0.322267 | Grad 0.0500 \n","[2024-04-10 13:15:46,634::train::INFO] [Train] Iter 19788 | Loss 0.288056 | Grad 0.0303 \n","[2024-04-10 13:15:46,755::train::INFO] [Train] Iter 19789 | Loss 0.296044 | Grad 0.0362 \n","[2024-04-10 13:15:46,875::train::INFO] [Train] Iter 19790 | Loss 0.320126 | Grad 0.0467 \n","[2024-04-10 13:15:46,996::train::INFO] [Train] Iter 19791 | Loss 0.313969 | Grad 0.0392 \n","[2024-04-10 13:15:47,115::train::INFO] [Train] Iter 19792 | Loss 0.297246 | Grad 0.0402 \n","[2024-04-10 13:15:47,243::train::INFO] [Train] Iter 19793 | Loss 0.317210 | Grad 0.0581 \n","[2024-04-10 13:15:47,366::train::INFO] [Train] Iter 19794 | Loss 0.279508 | Grad 0.0418 \n","[2024-04-10 13:15:47,488::train::INFO] [Train] Iter 19795 | Loss 0.316620 | Grad 0.0421 \n","[2024-04-10 13:15:47,610::train::INFO] [Train] Iter 19796 | Loss 0.318314 | Grad 0.0442 \n","[2024-04-10 13:15:47,731::train::INFO] [Train] Iter 19797 | Loss 0.318094 | Grad 0.0429 \n","[2024-04-10 13:15:47,854::train::INFO] [Train] Iter 19798 | Loss 0.289239 | Grad 0.0436 \n","[2024-04-10 13:15:47,978::train::INFO] [Train] Iter 19799 | Loss 0.364566 | Grad 0.0792 \n","[2024-04-10 13:15:48,052::train::INFO] [Train] Iter 19800 | Loss 0.353067 | Grad 0.0480 \n","[2024-04-10 13:15:48,171::train::INFO] [Train] Iter 19801 | Loss 0.281706 | Grad 0.0462 \n","[2024-04-10 13:15:48,292::train::INFO] [Train] Iter 19802 | Loss 0.312889 | Grad 0.0541 \n","[2024-04-10 13:15:48,412::train::INFO] [Train] Iter 19803 | Loss 0.349960 | Grad 0.0434 \n","[2024-04-10 13:15:48,532::train::INFO] [Train] Iter 19804 | Loss 0.319148 | Grad 0.0329 \n","[2024-04-10 13:15:48,652::train::INFO] [Train] Iter 19805 | Loss 0.329629 | Grad 0.0491 \n","[2024-04-10 13:15:48,773::train::INFO] [Train] Iter 19806 | Loss 0.290810 | Grad 0.0563 \n","[2024-04-10 13:15:48,893::train::INFO] [Train] Iter 19807 | Loss 0.314827 | Grad 0.0550 \n","[2024-04-10 13:15:49,012::train::INFO] [Train] Iter 19808 | Loss 0.295187 | Grad 0.0755 \n","[2024-04-10 13:15:49,131::train::INFO] [Train] Iter 19809 | Loss 0.354997 | Grad 0.0426 \n","[2024-04-10 13:15:49,251::train::INFO] [Train] Iter 19810 | Loss 0.317774 | Grad 0.0370 \n","[2024-04-10 13:15:49,370::train::INFO] [Train] Iter 19811 | Loss 0.342714 | Grad 0.0447 \n","[2024-04-10 13:15:49,488::train::INFO] [Train] Iter 19812 | Loss 0.294700 | Grad 0.0586 \n","[2024-04-10 13:15:49,606::train::INFO] [Train] Iter 19813 | Loss 0.317173 | Grad 0.0346 \n","[2024-04-10 13:15:49,724::train::INFO] [Train] Iter 19814 | Loss 0.342158 | Grad 0.0446 \n","[2024-04-10 13:15:49,843::train::INFO] [Train] Iter 19815 | Loss 0.335278 | Grad 0.0442 \n","[2024-04-10 13:15:49,963::train::INFO] [Train] Iter 19816 | Loss 0.320147 | Grad 0.0487 \n","[2024-04-10 13:15:50,082::train::INFO] [Train] Iter 19817 | Loss 0.323911 | Grad 0.0362 \n","[2024-04-10 13:15:50,203::train::INFO] [Train] Iter 19818 | Loss 0.332375 | Grad 0.0707 \n","[2024-04-10 13:15:50,324::train::INFO] [Train] Iter 19819 | Loss 0.315898 | Grad 0.0517 \n","[2024-04-10 13:15:50,444::train::INFO] [Train] Iter 19820 | Loss 0.326028 | Grad 0.0422 \n","[2024-04-10 13:15:50,563::train::INFO] [Train] Iter 19821 | Loss 0.319570 | Grad 0.0570 \n","[2024-04-10 13:15:50,686::train::INFO] [Train] Iter 19822 | Loss 0.340455 | Grad 0.0587 \n","[2024-04-10 13:15:50,805::train::INFO] [Train] Iter 19823 | Loss 0.314946 | Grad 0.0713 \n","[2024-04-10 13:15:50,924::train::INFO] [Train] Iter 19824 | Loss 0.367332 | Grad 0.0445 \n","[2024-04-10 13:15:51,044::train::INFO] [Train] Iter 19825 | Loss 0.312686 | Grad 0.0309 \n","[2024-04-10 13:15:51,163::train::INFO] [Train] Iter 19826 | Loss 0.335575 | Grad 0.0438 \n","[2024-04-10 13:15:51,283::train::INFO] [Train] Iter 19827 | Loss 0.285045 | Grad 0.0277 \n","[2024-04-10 13:15:51,404::train::INFO] [Train] Iter 19828 | Loss 0.307658 | Grad 0.0268 \n","[2024-04-10 13:15:51,524::train::INFO] [Train] Iter 19829 | Loss 0.329764 | Grad 0.0406 \n","[2024-04-10 13:15:51,643::train::INFO] [Train] Iter 19830 | Loss 0.275818 | Grad 0.0462 \n","[2024-04-10 13:15:51,762::train::INFO] [Train] Iter 19831 | Loss 0.325491 | Grad 0.0454 \n","[2024-04-10 13:15:51,882::train::INFO] [Train] Iter 19832 | Loss 0.336146 | Grad 0.0688 \n","[2024-04-10 13:15:52,000::train::INFO] [Train] Iter 19833 | Loss 0.298272 | Grad 0.0370 \n","[2024-04-10 13:15:52,117::train::INFO] [Train] Iter 19834 | Loss 0.289072 | Grad 0.0278 \n","[2024-04-10 13:15:52,237::train::INFO] [Train] Iter 19835 | Loss 0.339792 | Grad 0.0418 \n","[2024-04-10 13:15:52,357::train::INFO] [Train] Iter 19836 | Loss 0.275814 | Grad 0.0413 \n","[2024-04-10 13:15:52,476::train::INFO] [Train] Iter 19837 | Loss 0.292531 | Grad 0.0320 \n","[2024-04-10 13:15:52,596::train::INFO] [Train] Iter 19838 | Loss 0.312711 | Grad 0.0385 \n","[2024-04-10 13:15:52,719::train::INFO] [Train] Iter 19839 | Loss 0.289926 | Grad 0.0510 \n","[2024-04-10 13:15:52,838::train::INFO] [Train] Iter 19840 | Loss 0.316358 | Grad 0.0535 \n","[2024-04-10 13:15:52,955::train::INFO] [Train] Iter 19841 | Loss 0.379844 | Grad 0.0685 \n","[2024-04-10 13:15:53,073::train::INFO] [Train] Iter 19842 | Loss 0.288115 | Grad 0.0311 \n","[2024-04-10 13:15:53,191::train::INFO] [Train] Iter 19843 | Loss 0.333379 | Grad 0.0633 \n","[2024-04-10 13:15:53,310::train::INFO] [Train] Iter 19844 | Loss 0.288176 | Grad 0.0713 \n","[2024-04-10 13:15:53,386::train::INFO] [Train] Iter 19845 | Loss 0.314332 | Grad 0.0541 \n","[2024-04-10 13:15:53,505::train::INFO] [Train] Iter 19846 | Loss 0.307184 | Grad 0.0493 \n","[2024-04-10 13:15:53,625::train::INFO] [Train] Iter 19847 | Loss 0.330448 | Grad 0.0558 \n","[2024-04-10 13:15:53,745::train::INFO] [Train] Iter 19848 | Loss 0.314702 | Grad 0.0395 \n","[2024-04-10 13:15:53,864::train::INFO] [Train] Iter 19849 | Loss 0.348753 | Grad 0.0514 \n","[2024-04-10 13:15:53,982::train::INFO] [Train] Iter 19850 | Loss 0.301862 | Grad 0.0337 \n","[2024-04-10 13:15:54,100::train::INFO] [Train] Iter 19851 | Loss 0.304835 | Grad 0.0359 \n","[2024-04-10 13:15:54,221::train::INFO] [Train] Iter 19852 | Loss 0.294529 | Grad 0.0390 \n","[2024-04-10 13:15:54,339::train::INFO] [Train] Iter 19853 | Loss 0.288662 | Grad 0.0407 \n","[2024-04-10 13:15:54,458::train::INFO] [Train] Iter 19854 | Loss 0.371691 | Grad 0.0788 \n","[2024-04-10 13:15:54,576::train::INFO] [Train] Iter 19855 | Loss 0.322303 | Grad 0.0436 \n","[2024-04-10 13:15:54,695::train::INFO] [Train] Iter 19856 | Loss 0.298470 | Grad 0.0501 \n","[2024-04-10 13:15:54,815::train::INFO] [Train] Iter 19857 | Loss 0.322222 | Grad 0.0440 \n","[2024-04-10 13:15:54,935::train::INFO] [Train] Iter 19858 | Loss 0.325521 | Grad 0.0378 \n","[2024-04-10 13:15:55,056::train::INFO] [Train] Iter 19859 | Loss 0.310017 | Grad 0.0524 \n","[2024-04-10 13:15:55,175::train::INFO] [Train] Iter 19860 | Loss 0.324703 | Grad 0.0425 \n","[2024-04-10 13:15:55,296::train::INFO] [Train] Iter 19861 | Loss 0.312336 | Grad 0.0461 \n","[2024-04-10 13:15:55,417::train::INFO] [Train] Iter 19862 | Loss 0.293675 | Grad 0.0381 \n","[2024-04-10 13:15:55,537::train::INFO] [Train] Iter 19863 | Loss 0.312558 | Grad 0.0452 \n","[2024-04-10 13:15:55,656::train::INFO] [Train] Iter 19864 | Loss 0.324942 | Grad 0.0473 \n","[2024-04-10 13:15:55,776::train::INFO] [Train] Iter 19865 | Loss 0.319214 | Grad 0.0352 \n","[2024-04-10 13:15:55,900::train::INFO] [Train] Iter 19866 | Loss 0.285483 | Grad 0.0595 \n","[2024-04-10 13:15:56,020::train::INFO] [Train] Iter 19867 | Loss 0.347520 | Grad 0.0466 \n","[2024-04-10 13:15:56,139::train::INFO] [Train] Iter 19868 | Loss 0.356601 | Grad 0.0647 \n","[2024-04-10 13:15:56,258::train::INFO] [Train] Iter 19869 | Loss 0.332775 | Grad 0.0527 \n","[2024-04-10 13:15:56,377::train::INFO] [Train] Iter 19870 | Loss 0.323517 | Grad 0.0561 \n","[2024-04-10 13:15:56,498::train::INFO] [Train] Iter 19871 | Loss 0.292875 | Grad 0.0448 \n","[2024-04-10 13:15:56,616::train::INFO] [Train] Iter 19872 | Loss 0.339709 | Grad 0.0416 \n","[2024-04-10 13:15:56,736::train::INFO] [Train] Iter 19873 | Loss 0.331657 | Grad 0.0574 \n","[2024-04-10 13:15:56,855::train::INFO] [Train] Iter 19874 | Loss 0.293607 | Grad 0.0517 \n","[2024-04-10 13:15:56,974::train::INFO] [Train] Iter 19875 | Loss 0.262119 | Grad 0.0628 \n","[2024-04-10 13:15:57,093::train::INFO] [Train] Iter 19876 | Loss 0.305046 | Grad 0.0654 \n","[2024-04-10 13:15:57,213::train::INFO] [Train] Iter 19877 | Loss 0.329483 | Grad 0.0971 \n","[2024-04-10 13:15:57,333::train::INFO] [Train] Iter 19878 | Loss 0.323485 | Grad 0.0500 \n","[2024-04-10 13:15:57,452::train::INFO] [Train] Iter 19879 | Loss 0.329561 | Grad 0.0419 \n","[2024-04-10 13:15:57,573::train::INFO] [Train] Iter 19880 | Loss 0.381793 | Grad 0.0515 \n","[2024-04-10 13:15:57,693::train::INFO] [Train] Iter 19881 | Loss 0.311553 | Grad 0.0343 \n","[2024-04-10 13:15:57,813::train::INFO] [Train] Iter 19882 | Loss 0.279686 | Grad 0.0261 \n","[2024-04-10 13:15:57,934::train::INFO] [Train] Iter 19883 | Loss 0.315318 | Grad 0.0639 \n","[2024-04-10 13:15:58,053::train::INFO] [Train] Iter 19884 | Loss 0.314661 | Grad 0.0425 \n","[2024-04-10 13:15:58,176::train::INFO] [Train] Iter 19885 | Loss 0.308371 | Grad 0.0450 \n","[2024-04-10 13:15:58,299::train::INFO] [Train] Iter 19886 | Loss 0.323572 | Grad 0.0493 \n","[2024-04-10 13:15:58,419::train::INFO] [Train] Iter 19887 | Loss 0.362369 | Grad 0.0572 \n","[2024-04-10 13:15:58,541::train::INFO] [Train] Iter 19888 | Loss 0.310050 | Grad 0.0625 \n","[2024-04-10 13:15:58,661::train::INFO] [Train] Iter 19889 | Loss 0.315916 | Grad 0.0515 \n","[2024-04-10 13:15:58,737::train::INFO] [Train] Iter 19890 | Loss 0.257690 | Grad 0.0360 \n","[2024-04-10 13:15:58,856::train::INFO] [Train] Iter 19891 | Loss 0.310693 | Grad 0.0343 \n","[2024-04-10 13:15:58,977::train::INFO] [Train] Iter 19892 | Loss 0.298947 | Grad 0.0398 \n","[2024-04-10 13:15:59,099::train::INFO] [Train] Iter 19893 | Loss 0.296841 | Grad 0.0312 \n","[2024-04-10 13:15:59,221::train::INFO] [Train] Iter 19894 | Loss 0.341081 | Grad 0.0400 \n","[2024-04-10 13:15:59,344::train::INFO] [Train] Iter 19895 | Loss 0.350221 | Grad 0.0412 \n","[2024-04-10 13:15:59,464::train::INFO] [Train] Iter 19896 | Loss 0.248071 | Grad 0.0359 \n","[2024-04-10 13:15:59,591::train::INFO] [Train] Iter 19897 | Loss 0.306493 | Grad 0.0498 \n","[2024-04-10 13:15:59,714::train::INFO] [Train] Iter 19898 | Loss 0.288420 | Grad 0.0458 \n","[2024-04-10 13:15:59,834::train::INFO] [Train] Iter 19899 | Loss 0.309541 | Grad 0.0363 \n","[2024-04-10 13:15:59,953::train::INFO] [Train] Iter 19900 | Loss 0.319495 | Grad 0.0463 \n","[2024-04-10 13:16:00,072::train::INFO] [Train] Iter 19901 | Loss 0.302385 | Grad 0.0481 \n","[2024-04-10 13:16:00,195::train::INFO] [Train] Iter 19902 | Loss 0.313054 | Grad 0.0389 \n","[2024-04-10 13:16:00,316::train::INFO] [Train] Iter 19903 | Loss 0.265409 | Grad 0.0457 \n","[2024-04-10 13:16:00,440::train::INFO] [Train] Iter 19904 | Loss 0.307827 | Grad 0.0349 \n","[2024-04-10 13:16:00,562::train::INFO] [Train] Iter 19905 | Loss 0.291579 | Grad 0.0385 \n","[2024-04-10 13:16:00,684::train::INFO] [Train] Iter 19906 | Loss 0.326628 | Grad 0.0478 \n","[2024-04-10 13:16:00,807::train::INFO] [Train] Iter 19907 | Loss 0.286755 | Grad 0.0443 \n","[2024-04-10 13:16:00,931::train::INFO] [Train] Iter 19908 | Loss 0.303255 | Grad 0.0352 \n","[2024-04-10 13:16:01,057::train::INFO] [Train] Iter 19909 | Loss 0.299533 | Grad 0.0513 \n","[2024-04-10 13:16:01,179::train::INFO] [Train] Iter 19910 | Loss 0.323562 | Grad 0.0539 \n","[2024-04-10 13:16:01,297::train::INFO] [Train] Iter 19911 | Loss 0.337628 | Grad 0.0463 \n","[2024-04-10 13:16:01,416::train::INFO] [Train] Iter 19912 | Loss 0.304588 | Grad 0.0506 \n","[2024-04-10 13:16:01,535::train::INFO] [Train] Iter 19913 | Loss 0.316532 | Grad 0.0477 \n","[2024-04-10 13:16:01,655::train::INFO] [Train] Iter 19914 | Loss 0.265738 | Grad 0.0497 \n","[2024-04-10 13:16:01,775::train::INFO] [Train] Iter 19915 | Loss 0.319874 | Grad 0.0360 \n","[2024-04-10 13:16:01,894::train::INFO] [Train] Iter 19916 | Loss 0.321910 | Grad 0.0378 \n","[2024-04-10 13:16:02,013::train::INFO] [Train] Iter 19917 | Loss 0.296356 | Grad 0.0490 \n","[2024-04-10 13:16:02,131::train::INFO] [Train] Iter 19918 | Loss 0.307867 | Grad 0.0376 \n","[2024-04-10 13:16:02,251::train::INFO] [Train] Iter 19919 | Loss 0.301001 | Grad 0.0353 \n","[2024-04-10 13:16:02,370::train::INFO] [Train] Iter 19920 | Loss 0.287437 | Grad 0.0367 \n","[2024-04-10 13:16:02,488::train::INFO] [Train] Iter 19921 | Loss 0.335892 | Grad 0.0482 \n","[2024-04-10 13:16:02,607::train::INFO] [Train] Iter 19922 | Loss 0.316364 | Grad 0.0639 \n","[2024-04-10 13:16:02,728::train::INFO] [Train] Iter 19923 | Loss 0.306566 | Grad 0.0519 \n","[2024-04-10 13:16:02,850::train::INFO] [Train] Iter 19924 | Loss 0.362227 | Grad 0.0896 \n","[2024-04-10 13:16:02,971::train::INFO] [Train] Iter 19925 | Loss 0.331906 | Grad 0.0463 \n","[2024-04-10 13:16:03,089::train::INFO] [Train] Iter 19926 | Loss 0.312612 | Grad 0.0470 \n","[2024-04-10 13:16:03,209::train::INFO] [Train] Iter 19927 | Loss 0.336837 | Grad 0.0446 \n","[2024-04-10 13:16:03,330::train::INFO] [Train] Iter 19928 | Loss 0.324963 | Grad 0.0463 \n","[2024-04-10 13:16:03,452::train::INFO] [Train] Iter 19929 | Loss 0.303667 | Grad 0.0588 \n","[2024-04-10 13:16:03,571::train::INFO] [Train] Iter 19930 | Loss 0.275104 | Grad 0.0446 \n","[2024-04-10 13:16:03,689::train::INFO] [Train] Iter 19931 | Loss 0.333160 | Grad 0.0682 \n","[2024-04-10 13:16:03,808::train::INFO] [Train] Iter 19932 | Loss 0.327009 | Grad 0.0368 \n","[2024-04-10 13:16:03,926::train::INFO] [Train] Iter 19933 | Loss 0.310496 | Grad 0.0588 \n","[2024-04-10 13:16:04,044::train::INFO] [Train] Iter 19934 | Loss 0.282643 | Grad 0.0551 \n","[2024-04-10 13:16:04,117::train::INFO] [Train] Iter 19935 | Loss 0.296977 | Grad 0.0636 \n","[2024-04-10 13:16:04,236::train::INFO] [Train] Iter 19936 | Loss 0.295448 | Grad 0.0363 \n","[2024-04-10 13:16:04,355::train::INFO] [Train] Iter 19937 | Loss 0.349602 | Grad 0.0761 \n","[2024-04-10 13:16:04,474::train::INFO] [Train] Iter 19938 | Loss 0.314553 | Grad 0.0472 \n","[2024-04-10 13:16:04,592::train::INFO] [Train] Iter 19939 | Loss 0.312273 | Grad 0.0423 \n","[2024-04-10 13:16:04,712::train::INFO] [Train] Iter 19940 | Loss 0.321308 | Grad 0.0461 \n","[2024-04-10 13:16:04,832::train::INFO] [Train] Iter 19941 | Loss 0.350218 | Grad 0.0560 \n","[2024-04-10 13:16:04,952::train::INFO] [Train] Iter 19942 | Loss 0.327662 | Grad 0.0409 \n","[2024-04-10 13:16:05,072::train::INFO] [Train] Iter 19943 | Loss 0.360336 | Grad 0.0957 \n","[2024-04-10 13:16:05,190::train::INFO] [Train] Iter 19944 | Loss 0.309820 | Grad 0.0364 \n","[2024-04-10 13:16:05,309::train::INFO] [Train] Iter 19945 | Loss 0.349674 | Grad 0.0461 \n","[2024-04-10 13:16:05,429::train::INFO] [Train] Iter 19946 | Loss 0.306894 | Grad 0.0448 \n","[2024-04-10 13:16:05,548::train::INFO] [Train] Iter 19947 | Loss 0.314196 | Grad 0.0560 \n","[2024-04-10 13:16:05,669::train::INFO] [Train] Iter 19948 | Loss 0.352114 | Grad 0.0530 \n","[2024-04-10 13:16:05,789::train::INFO] [Train] Iter 19949 | Loss 0.331257 | Grad 0.0373 \n","[2024-04-10 13:16:05,909::train::INFO] [Train] Iter 19950 | Loss 0.274392 | Grad 0.0272 \n","[2024-04-10 13:16:06,028::train::INFO] [Train] Iter 19951 | Loss 0.317539 | Grad 0.0359 \n","[2024-04-10 13:16:06,146::train::INFO] [Train] Iter 19952 | Loss 0.351723 | Grad 0.0433 \n","[2024-04-10 13:16:06,267::train::INFO] [Train] Iter 19953 | Loss 0.320686 | Grad 0.0399 \n","[2024-04-10 13:16:06,387::train::INFO] [Train] Iter 19954 | Loss 0.310809 | Grad 0.0874 \n","[2024-04-10 13:16:06,506::train::INFO] [Train] Iter 19955 | Loss 0.299132 | Grad 0.0556 \n","[2024-04-10 13:16:06,624::train::INFO] [Train] Iter 19956 | Loss 0.280128 | Grad 0.0349 \n","[2024-04-10 13:16:06,743::train::INFO] [Train] Iter 19957 | Loss 0.316883 | Grad 0.0364 \n","[2024-04-10 13:16:06,863::train::INFO] [Train] Iter 19958 | Loss 0.321937 | Grad 0.0333 \n","[2024-04-10 13:16:06,981::train::INFO] [Train] Iter 19959 | Loss 0.283302 | Grad 0.0547 \n","[2024-04-10 13:16:07,100::train::INFO] [Train] Iter 19960 | Loss 0.351354 | Grad 0.0477 \n","[2024-04-10 13:16:07,219::train::INFO] [Train] Iter 19961 | Loss 0.283119 | Grad 0.0316 \n","[2024-04-10 13:16:07,338::train::INFO] [Train] Iter 19962 | Loss 0.305263 | Grad 0.0519 \n","[2024-04-10 13:16:07,458::train::INFO] [Train] Iter 19963 | Loss 0.269716 | Grad 0.0237 \n","[2024-04-10 13:16:07,578::train::INFO] [Train] Iter 19964 | Loss 0.328026 | Grad 0.0539 \n","[2024-04-10 13:16:07,697::train::INFO] [Train] Iter 19965 | Loss 0.299838 | Grad 0.0664 \n","[2024-04-10 13:16:07,821::train::INFO] [Train] Iter 19966 | Loss 0.307537 | Grad 0.0295 \n","[2024-04-10 13:16:07,941::train::INFO] [Train] Iter 19967 | Loss 0.328285 | Grad 0.0524 \n","[2024-04-10 13:16:08,061::train::INFO] [Train] Iter 19968 | Loss 0.326241 | Grad 0.0347 \n","[2024-04-10 13:16:08,188::train::INFO] [Train] Iter 19969 | Loss 0.301189 | Grad 0.0382 \n","[2024-04-10 13:16:08,307::train::INFO] [Train] Iter 19970 | Loss 0.283466 | Grad 0.0448 \n","[2024-04-10 13:16:08,426::train::INFO] [Train] Iter 19971 | Loss 0.294188 | Grad 0.0615 \n","[2024-04-10 13:16:08,545::train::INFO] [Train] Iter 19972 | Loss 0.308823 | Grad 0.0469 \n","[2024-04-10 13:16:08,665::train::INFO] [Train] Iter 19973 | Loss 0.314880 | Grad 0.0249 \n","[2024-04-10 13:16:08,785::train::INFO] [Train] Iter 19974 | Loss 0.343232 | Grad 0.0427 \n","[2024-04-10 13:16:08,906::train::INFO] [Train] Iter 19975 | Loss 0.319365 | Grad 0.0502 \n","[2024-04-10 13:16:09,026::train::INFO] [Train] Iter 19976 | Loss 0.290628 | Grad 0.0455 \n","[2024-04-10 13:16:09,145::train::INFO] [Train] Iter 19977 | Loss 0.315183 | Grad 0.0737 \n","[2024-04-10 13:16:09,264::train::INFO] [Train] Iter 19978 | Loss 0.312723 | Grad 0.0707 \n","[2024-04-10 13:16:09,384::train::INFO] [Train] Iter 19979 | Loss 0.333769 | Grad 0.0396 \n","[2024-04-10 13:16:09,458::train::INFO] [Train] Iter 19980 | Loss 0.269714 | Grad 0.0297 \n","[2024-04-10 13:16:09,577::train::INFO] [Train] Iter 19981 | Loss 0.285181 | Grad 0.0308 \n","[2024-04-10 13:16:09,696::train::INFO] [Train] Iter 19982 | Loss 0.314131 | Grad 0.0452 \n","[2024-04-10 13:16:09,816::train::INFO] [Train] Iter 19983 | Loss 0.322152 | Grad 0.0429 \n","[2024-04-10 13:16:09,936::train::INFO] [Train] Iter 19984 | Loss 0.338962 | Grad 0.0440 \n","[2024-04-10 13:16:10,057::train::INFO] [Train] Iter 19985 | Loss 0.300907 | Grad 0.0419 \n","[2024-04-10 13:16:10,176::train::INFO] [Train] Iter 19986 | Loss 0.308961 | Grad 0.0211 \n","[2024-04-10 13:16:10,298::train::INFO] [Train] Iter 19987 | Loss 0.290080 | Grad 0.0378 \n","[2024-04-10 13:16:10,418::train::INFO] [Train] Iter 19988 | Loss 0.320396 | Grad 0.0604 \n","[2024-04-10 13:16:10,538::train::INFO] [Train] Iter 19989 | Loss 0.320467 | Grad 0.0332 \n","[2024-04-10 13:16:10,661::train::INFO] [Train] Iter 19990 | Loss 0.372401 | Grad 0.0516 \n","[2024-04-10 13:16:10,781::train::INFO] [Train] Iter 19991 | Loss 0.302498 | Grad 0.0545 \n","[2024-04-10 13:16:10,905::train::INFO] [Train] Iter 19992 | Loss 0.298493 | Grad 0.0480 \n","[2024-04-10 13:16:11,025::train::INFO] [Train] Iter 19993 | Loss 0.319571 | Grad 0.0529 \n","[2024-04-10 13:16:11,144::train::INFO] [Train] Iter 19994 | Loss 0.281201 | Grad 0.0329 \n","[2024-04-10 13:16:11,269::train::INFO] [Train] Iter 19995 | Loss 0.291259 | Grad 0.0331 \n","[2024-04-10 13:16:11,391::train::INFO] [Train] Iter 19996 | Loss 0.313823 | Grad 0.0406 \n","[2024-04-10 13:16:11,513::train::INFO] [Train] Iter 19997 | Loss 0.351240 | Grad 0.0528 \n","[2024-04-10 13:16:11,633::train::INFO] [Train] Iter 19998 | Loss 0.325001 | Grad 0.0470 \n","[2024-04-10 13:16:11,753::train::INFO] [Train] Iter 19999 | Loss 0.315733 | Grad 0.0535 \n","[2024-04-10 13:16:11,874::train::INFO] [Train] Iter 20000 | Loss 0.250725 | Grad 0.0419 \n","Validate: 100% 31/31 [00:48<00:00,  1.57s/it]\n","EMD-CD: 100% 31/31 [00:00<00:00, 136.60it/s]\n","[2024-04-10 13:17:00,726::train::INFO] [Val] Iter 20000 | CD 0.000769 | EMD 0.000000  \n","Inspect:   3% 1/31 [00:03<01:34,  3.15s/it]\n","[2024-04-10 13:17:04,160::train::INFO] [Train] Iter 20001 | Loss 0.320885 | Grad 0.0735 \n","[2024-04-10 13:17:04,286::train::INFO] [Train] Iter 20002 | Loss 0.336264 | Grad 0.0613 \n","[2024-04-10 13:17:04,408::train::INFO] [Train] Iter 20003 | Loss 0.334805 | Grad 0.0778 \n","[2024-04-10 13:17:04,530::train::INFO] [Train] Iter 20004 | Loss 0.328178 | Grad 0.0505 \n","[2024-04-10 13:17:04,650::train::INFO] [Train] Iter 20005 | Loss 0.354975 | Grad 0.0577 \n","[2024-04-10 13:17:04,770::train::INFO] [Train] Iter 20006 | Loss 0.346312 | Grad 0.0606 \n","[2024-04-10 13:17:04,889::train::INFO] [Train] Iter 20007 | Loss 0.295598 | Grad 0.0382 \n","[2024-04-10 13:17:05,013::train::INFO] [Train] Iter 20008 | Loss 0.312966 | Grad 0.0507 \n","[2024-04-10 13:17:05,138::train::INFO] [Train] Iter 20009 | Loss 0.327099 | Grad 0.0396 \n","[2024-04-10 13:17:05,258::train::INFO] [Train] Iter 20010 | Loss 0.336830 | Grad 0.0509 \n","[2024-04-10 13:17:05,377::train::INFO] [Train] Iter 20011 | Loss 0.311849 | Grad 0.0436 \n","[2024-04-10 13:17:05,497::train::INFO] [Train] Iter 20012 | Loss 0.316618 | Grad 0.0529 \n","[2024-04-10 13:17:05,619::train::INFO] [Train] Iter 20013 | Loss 0.369908 | Grad 0.0793 \n","[2024-04-10 13:17:05,740::train::INFO] [Train] Iter 20014 | Loss 0.323605 | Grad 0.0545 \n","[2024-04-10 13:17:05,861::train::INFO] [Train] Iter 20015 | Loss 0.366596 | Grad 0.0763 \n","[2024-04-10 13:17:05,983::train::INFO] [Train] Iter 20016 | Loss 0.274392 | Grad 0.0626 \n","[2024-04-10 13:17:06,106::train::INFO] [Train] Iter 20017 | Loss 0.337359 | Grad 0.0410 \n","[2024-04-10 13:17:06,227::train::INFO] [Train] Iter 20018 | Loss 0.294452 | Grad 0.0380 \n","[2024-04-10 13:17:06,346::train::INFO] [Train] Iter 20019 | Loss 0.285616 | Grad 0.0422 \n","[2024-04-10 13:17:06,465::train::INFO] [Train] Iter 20020 | Loss 0.323983 | Grad 0.0502 \n","[2024-04-10 13:17:06,584::train::INFO] [Train] Iter 20021 | Loss 0.282498 | Grad 0.0517 \n","[2024-04-10 13:17:06,704::train::INFO] [Train] Iter 20022 | Loss 0.313169 | Grad 0.0730 \n","[2024-04-10 13:17:06,825::train::INFO] [Train] Iter 20023 | Loss 0.312438 | Grad 0.0825 \n","[2024-04-10 13:17:06,946::train::INFO] [Train] Iter 20024 | Loss 0.350049 | Grad 0.0772 \n","[2024-04-10 13:17:07,021::train::INFO] [Train] Iter 20025 | Loss 0.313402 | Grad 0.0665 \n","[2024-04-10 13:17:07,139::train::INFO] [Train] Iter 20026 | Loss 0.309869 | Grad 0.0615 \n","[2024-04-10 13:17:07,259::train::INFO] [Train] Iter 20027 | Loss 0.336084 | Grad 0.0595 \n","[2024-04-10 13:17:07,377::train::INFO] [Train] Iter 20028 | Loss 0.338097 | Grad 0.0592 \n","[2024-04-10 13:17:07,501::train::INFO] [Train] Iter 20029 | Loss 0.328112 | Grad 0.0532 \n","[2024-04-10 13:17:07,619::train::INFO] [Train] Iter 20030 | Loss 0.303624 | Grad 0.0429 \n","[2024-04-10 13:17:07,738::train::INFO] [Train] Iter 20031 | Loss 0.326058 | Grad 0.0441 \n","[2024-04-10 13:17:07,857::train::INFO] [Train] Iter 20032 | Loss 0.350486 | Grad 0.0454 \n","[2024-04-10 13:17:07,976::train::INFO] [Train] Iter 20033 | Loss 0.283837 | Grad 0.0436 \n","[2024-04-10 13:17:08,095::train::INFO] [Train] Iter 20034 | Loss 0.329319 | Grad 0.0561 \n","[2024-04-10 13:17:08,214::train::INFO] [Train] Iter 20035 | Loss 0.317302 | Grad 0.0493 \n","[2024-04-10 13:17:08,333::train::INFO] [Train] Iter 20036 | Loss 0.331610 | Grad 0.0609 \n","[2024-04-10 13:17:08,453::train::INFO] [Train] Iter 20037 | Loss 0.269675 | Grad 0.0345 \n","[2024-04-10 13:17:08,572::train::INFO] [Train] Iter 20038 | Loss 0.313641 | Grad 0.0551 \n","[2024-04-10 13:17:08,693::train::INFO] [Train] Iter 20039 | Loss 0.300452 | Grad 0.0490 \n","[2024-04-10 13:17:08,813::train::INFO] [Train] Iter 20040 | Loss 0.342768 | Grad 0.0503 \n","[2024-04-10 13:17:08,933::train::INFO] [Train] Iter 20041 | Loss 0.340320 | Grad 0.0514 \n","[2024-04-10 13:17:09,054::train::INFO] [Train] Iter 20042 | Loss 0.306950 | Grad 0.0580 \n","[2024-04-10 13:17:09,172::train::INFO] [Train] Iter 20043 | Loss 0.282635 | Grad 0.0457 \n","[2024-04-10 13:17:09,290::train::INFO] [Train] Iter 20044 | Loss 0.339037 | Grad 0.0847 \n","[2024-04-10 13:17:09,409::train::INFO] [Train] Iter 20045 | Loss 0.351984 | Grad 0.0613 \n","[2024-04-10 13:17:09,528::train::INFO] [Train] Iter 20046 | Loss 0.303063 | Grad 0.0345 \n","[2024-04-10 13:17:09,650::train::INFO] [Train] Iter 20047 | Loss 0.321997 | Grad 0.0420 \n","[2024-04-10 13:17:09,770::train::INFO] [Train] Iter 20048 | Loss 0.324606 | Grad 0.0492 \n","[2024-04-10 13:17:09,889::train::INFO] [Train] Iter 20049 | Loss 0.307157 | Grad 0.0385 \n","[2024-04-10 13:17:10,008::train::INFO] [Train] Iter 20050 | Loss 0.303893 | Grad 0.0458 \n","[2024-04-10 13:17:10,128::train::INFO] [Train] Iter 20051 | Loss 0.290098 | Grad 0.0419 \n","[2024-04-10 13:17:10,248::train::INFO] [Train] Iter 20052 | Loss 0.314467 | Grad 0.0548 \n","[2024-04-10 13:17:10,368::train::INFO] [Train] Iter 20053 | Loss 0.320264 | Grad 0.0432 \n","[2024-04-10 13:17:10,489::train::INFO] [Train] Iter 20054 | Loss 0.300847 | Grad 0.0418 \n","[2024-04-10 13:17:10,609::train::INFO] [Train] Iter 20055 | Loss 0.304347 | Grad 0.0550 \n","[2024-04-10 13:17:10,730::train::INFO] [Train] Iter 20056 | Loss 0.310966 | Grad 0.0527 \n","[2024-04-10 13:17:10,766::train::INFO] Terminating...\n","^C\n"]}]},{"cell_type":"code","source":["%mv './logs_ae_chair/AE_2024_04_10__12_19_39/ckpt_0.001665_1000.pt' './pretrained/AE_augmented_chair.pt'"],"metadata":{"id":"ZVRsKH0eB6LH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test model\n","!python3 test_ae.py --ckpt './pretrained/AE_augmented_chair.pt' --categories chair\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"calCGTxmPBgo","executionInfo":{"status":"ok","timestamp":1712334455621,"user_tz":240,"elapsed":247904,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"1dbaec3e-78b5-4ab1-9fa3-bcc402b1514e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-04-05 16:23:30,588::test::INFO] [ARGS::ckpt] './pretrained/AE_augmented_cosine_similarity.pt'\n","[2024-04-05 16:23:30,589::test::INFO] [ARGS::categories] ['chair', 'table', 'car', 'sofa', 'pot', 'microwave', 'bathtub', 'can', 'speaker', 'lamp', 'printer', 'file', 'bed', 'cabinet', 'vessel', 'bus', 'washer', 'dishwasher', 'clock', 'bowl', 'tin_can', 'jar', 'camera']\n","[2024-04-05 16:23:30,590::test::INFO] [ARGS::save_dir] './results'\n","[2024-04-05 16:23:30,590::test::INFO] [ARGS::device] 'cuda'\n","[2024-04-05 16:23:30,590::test::INFO] [ARGS::dataset_path] './data/shapenet.hdf5'\n","[2024-04-05 16:23:30,590::test::INFO] [ARGS::batch_size] 128\n","[2024-04-05 16:23:30,771::test::INFO] Loading datasets...\n","[2024-04-05 16:23:31,838::test::INFO] Loading model...\n","100% 41/41 [03:55<00:00,  5.73s/it]\n","[2024-04-05 16:27:27,120::test::INFO] Saving point clouds...\n","[2024-04-05 16:27:32,617::test::INFO] Start computing metrics...\n","EMD-CD:   0% 0/41 [00:00<?, ?it/s]\n","\n","[WARNING]\n","  * EMD is not implemented due to GPU compatability issue.\n","  * We will set all EMD to zero by default.\n","  * You may implement your own EMD in the function `emd_approx` in ./evaluation/evaluation_metrics.py\n","\n","\n","EMD-CD: 100% 41/41 [00:01<00:00, 33.45it/s]\n","[2024-04-05 16:27:33,907::test::INFO] CD:  0.002200302668\n","[2024-04-05 16:27:33,907::test::INFO] EMD: 0.000000000000\n"]}]},{"cell_type":"code","source":["!python3 test_ae.py --ckpt \"./pretrained/AE_augmented_chair.pt\" --categories chair\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXjm8c_wwI03","executionInfo":{"status":"ok","timestamp":1712755482051,"user_tz":240,"elapsed":48248,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"13a89e45-77a8-4982-bc41-1c0863918ab4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-04-10 13:23:55,725::test::INFO] [ARGS::ckpt] './pretrained/AE_augmented_chair.pt'\n","[2024-04-10 13:23:55,726::test::INFO] [ARGS::categories] ['chair']\n","[2024-04-10 13:23:55,727::test::INFO] [ARGS::save_dir] './results'\n","[2024-04-10 13:23:55,728::test::INFO] [ARGS::device] 'cuda'\n","[2024-04-10 13:23:55,728::test::INFO] [ARGS::dataset_path] './data/shapenet.hdf5'\n","[2024-04-10 13:23:55,728::test::INFO] [ARGS::batch_size] 128\n","[2024-04-10 13:23:55,920::test::INFO] Loading datasets...\n","[2024-04-10 13:23:56,116::test::INFO] Loading model...\n","100% 8/8 [00:44<00:00,  5.54s/it]\n","[2024-04-10 13:24:40,486::test::INFO] Saving point clouds...\n","[2024-04-10 13:24:40,582::test::INFO] Start computing metrics...\n","EMD-CD:   0% 0/8 [00:00<?, ?it/s]\n","\n","[WARNING]\n","  * EMD is not implemented due to GPU compatability issue.\n","  * We will set all EMD to zero by default.\n","  * You may implement your own EMD in the function `emd_approx` in ./evaluation/evaluation_metrics.py\n","\n","\n","EMD-CD: 100% 8/8 [00:00<00:00, 35.30it/s]\n","[2024-04-10 13:24:40,821::test::INFO] CD:  0.001658126363\n","[2024-04-10 13:24:40,821::test::INFO] EMD: 0.000000000000\n"]}]},{"cell_type":"code","source":["# test using original model\n","# !python3 test_ae.py --ckpt ./pretrained/AE_all.pt --categories \"chair,table,car,sofa,pot,microwave,bathtub,can,speaker,lamp,printer,file,bed,cabinet,vessel,bus,washer,dishwasher,clock,bowl,tin_can,jar,camera\"\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEdJUeL4ONMr","executionInfo":{"status":"ok","timestamp":1712334747819,"user_tz":240,"elapsed":242824,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"a98131cd-eaad-4245-f08f-3c2e93bf19f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-04-05 16:28:26,279::test::INFO] [ARGS::ckpt] './pretrained/AE_all.pt'\n","[2024-04-05 16:28:26,280::test::INFO] [ARGS::categories] ['chair', 'table', 'car', 'sofa', 'pot', 'microwave', 'bathtub', 'can', 'speaker', 'lamp', 'printer', 'file', 'bed', 'cabinet', 'vessel', 'bus', 'washer', 'dishwasher', 'clock', 'bowl', 'tin_can', 'jar', 'camera']\n","[2024-04-05 16:28:26,282::test::INFO] [ARGS::save_dir] './results'\n","[2024-04-05 16:28:26,283::test::INFO] [ARGS::device] 'cuda'\n","[2024-04-05 16:28:26,283::test::INFO] [ARGS::dataset_path] './data/shapenet.hdf5'\n","[2024-04-05 16:28:26,283::test::INFO] [ARGS::batch_size] 128\n","[2024-04-05 16:28:26,447::test::INFO] Loading datasets...\n","[2024-04-05 16:28:27,556::test::INFO] Loading model...\n","100% 41/41 [03:55<00:00,  5.75s/it]\n","[2024-04-05 16:32:23,704::test::INFO] Saving point clouds...\n","[2024-04-05 16:32:25,183::test::INFO] Start computing metrics...\n","EMD-CD:   0% 0/41 [00:00<?, ?it/s]\n","\n","[WARNING]\n","  * EMD is not implemented due to GPU compatability issue.\n","  * We will set all EMD to zero by default.\n","  * You may implement your own EMD in the function `emd_approx` in ./evaluation/evaluation_metrics.py\n","\n","\n","EMD-CD: 100% 41/41 [00:01<00:00, 32.89it/s]\n","[2024-04-05 16:32:26,498::test::INFO] CD:  0.000597913866\n","[2024-04-05 16:32:26,498::test::INFO] EMD: 0.000000000000\n"]}]},{"cell_type":"code","source":["# test using original model\n","!python3 test_ae.py --ckpt ./pretrained/AE_all.pt --categories chair\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yf6wKR-wXF3","executionInfo":{"status":"ok","timestamp":1712755529692,"user_tz":240,"elapsed":47643,"user":{"displayName":"Benjamin Wu","userId":"14111912722786393655"}},"outputId":"1a8bc126-d893-4a77-a03c-ef02de090cd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2024-04-10 13:24:43,357::test::INFO] [ARGS::ckpt] './pretrained/AE_all.pt'\n","[2024-04-10 13:24:43,359::test::INFO] [ARGS::categories] ['chair']\n","[2024-04-10 13:24:43,359::test::INFO] [ARGS::save_dir] './results'\n","[2024-04-10 13:24:43,359::test::INFO] [ARGS::device] 'cuda'\n","[2024-04-10 13:24:43,359::test::INFO] [ARGS::dataset_path] './data/shapenet.hdf5'\n","[2024-04-10 13:24:43,359::test::INFO] [ARGS::batch_size] 128\n","[2024-04-10 13:24:43,508::test::INFO] Loading datasets...\n","[2024-04-10 13:24:43,705::test::INFO] Loading model...\n","100% 8/8 [00:44<00:00,  5.55s/it]\n","[2024-04-10 13:25:28,167::test::INFO] Saving point clouds...\n","[2024-04-10 13:25:28,290::test::INFO] Start computing metrics...\n","EMD-CD:   0% 0/8 [00:00<?, ?it/s]\n","\n","[WARNING]\n","  * EMD is not implemented due to GPU compatability issue.\n","  * We will set all EMD to zero by default.\n","  * You may implement your own EMD in the function `emd_approx` in ./evaluation/evaluation_metrics.py\n","\n","\n","EMD-CD: 100% 8/8 [00:00<00:00, 34.05it/s]\n","[2024-04-10 13:25:28,539::test::INFO] CD:  0.000552224112\n","[2024-04-10 13:25:28,540::test::INFO] EMD: 0.000000000000\n"]}]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()\n"],"metadata":{"id":"nt6Fl73oOZ7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bBp2z6qcEgUK"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aecc63c50ea6439da22b01a4a67d6ed6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_780ca1d039ff445b9c48f82c6eda36fd","IPY_MODEL_62fcc30b297e4831b31b71bc04188931","IPY_MODEL_64d85ca2ad6847e0a127ce4ff0d4c668"],"layout":"IPY_MODEL_a3854fcfb9814b298a222bbbba41dcc2"}},"780ca1d039ff445b9c48f82c6eda36fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_627c1d3756214b178a75cf4e724bca19","placeholder":"â€‹","style":"IPY_MODEL_599a55d788554aeca7dba0e40bb98d40","value":"â€‡59%"}},"62fcc30b297e4831b31b71bc04188931":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_13c296911d1c443e976b758fb179f430","max":22,"min":0,"orientation":"horizontal","style":"IPY_MODEL_616e99b84e534659a850c1893e1fdea7","value":13}},"64d85ca2ad6847e0a127ce4ff0d4c668":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be8082c1872d4832944e0d9452f22fba","placeholder":"â€‹","style":"IPY_MODEL_a3c1d8b3e138460ca3a990d02832df5c","value":"â€‡13/22â€‡[00:06&lt;00:04,â€‡â€‡1.85it/s]"}},"a3854fcfb9814b298a222bbbba41dcc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"627c1d3756214b178a75cf4e724bca19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"599a55d788554aeca7dba0e40bb98d40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13c296911d1c443e976b758fb179f430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"616e99b84e534659a850c1893e1fdea7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be8082c1872d4832944e0d9452f22fba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3c1d8b3e138460ca3a990d02832df5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}